<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta name="description" content="Course materials for An Introduction to Machine Learning">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="figures/cover_image.png" />
  <meta property="og:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="github-repo" content="bioinformatics-training/intro-machine-learning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="twitter:image" content="figures/cover_image.png" />

<meta name="author" content="Sudhakaran Prabakaran, Matt Wayland and Chris Penfold">


<meta name="date" content="2017-10-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="linear-models.html">
<link rel="next" href="nearest-neighbours.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Introduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About the course</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#registration"><i class="fa fa-check"></i><b>1.2</b> Registration</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4</b> Github</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.5</b> License</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6</b> Contact</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.7</b> Colophon</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a><ul>
<li class="chapter" data-level="2.1" data-path="intro.html"><a href="intro.html#what-is-machine-learning"><i class="fa fa-check"></i><b>2.1</b> What is machine learning?</a></li>
<li class="chapter" data-level="2.2" data-path="intro.html"><a href="intro.html#aspects-of-ml"><i class="fa fa-check"></i><b>2.2</b> Aspects of ML</a></li>
<li class="chapter" data-level="2.3" data-path="intro.html"><a href="intro.html#what-actually-happend-under-the-hood"><i class="fa fa-check"></i><b>2.3</b> What actually happend under the hood</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>3</b> Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-models.html"><a href="linear-models.html#linear-models"><i class="fa fa-check"></i><b>3.1</b> Linear models</a></li>
<li class="chapter" data-level="3.2" data-path="linear-models.html"><a href="linear-models.html#matrix-algebra"><i class="fa fa-check"></i><b>3.2</b> Matrix algebra</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Logistic regression</a><ul>
<li class="chapter" data-level="4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#regression"><i class="fa fa-check"></i><b>4.1</b> Regression</a><ul>
<li class="chapter" data-level="4.1.1" data-path="logistic-regression.html"><a href="logistic-regression.html#linear-regression"><i class="fa fa-check"></i><b>4.1.1</b> Linear regression</a></li>
<li class="chapter" data-level="4.1.2" data-path="logistic-regression.html"><a href="logistic-regression.html#gaussian-process-regression"><i class="fa fa-check"></i><b>4.1.2</b> Gaussian process regression</a></li>
<li class="chapter" data-level="4.1.3" data-path="logistic-regression.html"><a href="logistic-regression.html#inference-using-gaussian-process-regression"><i class="fa fa-check"></i><b>4.1.3</b> Inference using Gaussian process regression</a></li>
<li class="chapter" data-level="4.1.4" data-path="logistic-regression.html"><a href="logistic-regression.html#model-selection"><i class="fa fa-check"></i><b>4.1.4</b> Model Selection</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="logistic-regression.html"><a href="logistic-regression.html#classification"><i class="fa fa-check"></i><b>4.2</b> Classification</a><ul>
<li class="chapter" data-level="4.2.1" data-path="logistic-regression.html"><a href="logistic-regression.html#logistic-regression"><i class="fa fa-check"></i><b>4.2.1</b> Logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="logistic-regression.html"><a href="logistic-regression.html#gp-classification"><i class="fa fa-check"></i><b>4.3</b> GP classification</a><ul>
<li class="chapter" data-level="4.3.1" data-path="logistic-regression.html"><a href="logistic-regression.html#other-classification-approaches."><i class="fa fa-check"></i><b>4.3.1</b> Other classification approaches.</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>4.4</b> Resources</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html"><i class="fa fa-check"></i><b>5</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#classification-simulated-data"><i class="fa fa-check"></i><b>5.2</b> Classification: simulated data</a><ul>
<li class="chapter" data-level="5.2.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-function"><i class="fa fa-check"></i><b>5.2.1</b> knn function</a></li>
<li class="chapter" data-level="5.2.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#plotting-decision-boundaries"><i class="fa fa-check"></i><b>5.2.2</b> Plotting decision boundaries</a></li>
<li class="chapter" data-level="5.2.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.2.3</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="5.2.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#choosing-k"><i class="fa fa-check"></i><b>5.2.4</b> Choosing <em>k</em></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-cell-segmentation"><i class="fa fa-check"></i><b>5.3</b> Classification: cell segmentation</a><ul>
<li class="chapter" data-level="5.3.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#cell-segmentation-data-set"><i class="fa fa-check"></i><b>5.3.1</b> Cell segmentation data set</a></li>
<li class="chapter" data-level="5.3.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-splitting"><i class="fa fa-check"></i><b>5.3.2</b> Data splitting</a></li>
<li class="chapter" data-level="5.3.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#identification-of-data-quality-issues"><i class="fa fa-check"></i><b>5.3.3</b> Identification of data quality issues</a></li>
<li class="chapter" data-level="5.3.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#fit-model-without-feature-selection"><i class="fa fa-check"></i><b>5.3.4</b> Fit model without feature selection</a></li>
<li class="chapter" data-level="5.3.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#feature-selection-using-filter"><i class="fa fa-check"></i><b>5.3.5</b> Feature selection using filter</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-regression"><i class="fa fa-check"></i><b>5.4</b> Regression</a><ul>
<li class="chapter" data-level="5.4.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#partition-data"><i class="fa fa-check"></i><b>5.4.1</b> Partition data</a></li>
<li class="chapter" data-level="5.4.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-pre-processing"><i class="fa fa-check"></i><b>5.4.2</b> Data pre-processing</a></li>
<li class="chapter" data-level="5.4.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#search-for-optimum-k"><i class="fa fa-check"></i><b>5.4.3</b> Search for optimum <em>k</em></a></li>
<li class="chapter" data-level="5.4.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#use-model-to-make-predictions"><i class="fa fa-check"></i><b>5.4.4</b> Use model to make predictions</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#exercises"><i class="fa fa-check"></i><b>5.5</b> Exercises</a><ul>
<li class="chapter" data-level="5.5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx1"><i class="fa fa-check"></i><b>5.5.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>6</b> Decision trees and random forests</a><ul>
<li class="chapter" data-level="6.1" data-path="decision-trees.html"><a href="decision-trees.html#decision-trees"><i class="fa fa-check"></i><b>6.1</b> Decision Trees</a></li>
<li class="chapter" data-level="6.2" data-path="decision-trees.html"><a href="decision-trees.html#random-forest"><i class="fa fa-check"></i><b>6.2</b> Random Forest</a></li>
<li class="chapter" data-level="6.3" data-path="decision-trees.html"><a href="decision-trees.html#exercises-1"><i class="fa fa-check"></i><b>6.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>7</b> Support vector machines</a><ul>
<li class="chapter" data-level="7.1" data-path="svm.html"><a href="svm.html#introduction-1"><i class="fa fa-check"></i><b>7.1</b> Introduction</a><ul>
<li class="chapter" data-level="7.1.1" data-path="svm.html"><a href="svm.html#maximum-margin-classifier"><i class="fa fa-check"></i><b>7.1.1</b> Maximum margin classifier</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="svm.html"><a href="svm.html#support-vector-classifier"><i class="fa fa-check"></i><b>7.2</b> Support vector classifier</a></li>
<li class="chapter" data-level="7.3" data-path="svm.html"><a href="svm.html#support-vector-machine"><i class="fa fa-check"></i><b>7.3</b> Support Vector Machine</a></li>
<li class="chapter" data-level="7.4" data-path="svm.html"><a href="svm.html#example---training-a-classifier"><i class="fa fa-check"></i><b>7.4</b> Example - training a classifier</a><ul>
<li class="chapter" data-level="7.4.1" data-path="svm.html"><a href="svm.html#setup-environment"><i class="fa fa-check"></i><b>7.4.1</b> Setup environment</a></li>
<li class="chapter" data-level="7.4.2" data-path="svm.html"><a href="svm.html#partition-data-1"><i class="fa fa-check"></i><b>7.4.2</b> Partition data</a></li>
<li class="chapter" data-level="7.4.3" data-path="svm.html"><a href="svm.html#visualize-training-data"><i class="fa fa-check"></i><b>7.4.3</b> Visualize training data</a></li>
<li class="chapter" data-level="7.4.4" data-path="svm.html"><a href="svm.html#model-cross-validation-and-tuning"><i class="fa fa-check"></i><b>7.4.4</b> Model cross-validation and tuning</a></li>
<li class="chapter" data-level="7.4.5" data-path="svm.html"><a href="svm.html#prediction-performance-measures"><i class="fa fa-check"></i><b>7.4.5</b> Prediction performance measures</a></li>
<li class="chapter" data-level="7.4.6" data-path="svm.html"><a href="svm.html#plot-decision-boundary"><i class="fa fa-check"></i><b>7.4.6</b> Plot decision boundary</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="svm.html"><a href="svm.html#example---regression"><i class="fa fa-check"></i><b>7.5</b> Example - regression</a></li>
<li class="chapter" data-level="7.6" data-path="svm.html"><a href="svm.html#further-reading"><i class="fa fa-check"></i><b>7.6</b> Further reading</a></li>
<li class="chapter" data-level="7.7" data-path="svm.html"><a href="svm.html#exercises-2"><i class="fa fa-check"></i><b>7.7</b> Exercises</a><ul>
<li class="chapter" data-level="7.7.1" data-path="svm.html"><a href="svm.html#exercise-1"><i class="fa fa-check"></i><b>7.7.1</b> Exercise 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8</b> Artificial neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html#neural-networks"><i class="fa fa-check"></i><b>8.1</b> Neural Networks</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>9</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="9.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-dimensionality-reduction"><i class="fa fa-check"></i><b>9.1</b> Linear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#interpreting-the-principle-component-axes"><i class="fa fa-check"></i><b>9.1.1</b> Interpreting the Principle Component Axes</a></li>
<li class="chapter" data-level="9.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#horseshoe-effect"><i class="fa fa-check"></i><b>9.1.2</b> Horseshoe effect</a></li>
<li class="chapter" data-level="9.1.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#pca-analysis-of-mammalian-development"><i class="fa fa-check"></i><b>9.1.3</b> PCA analysis of mammalian development</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-dimensionality-reduction"><i class="fa fa-check"></i><b>9.2</b> Nonlinear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-warping"><i class="fa fa-check"></i><b>9.2.1</b> Nonlinear warping</a></li>
<li class="chapter" data-level="9.2.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#stochasticity"><i class="fa fa-check"></i><b>9.2.2</b> Stochasticity</a></li>
<li class="chapter" data-level="9.2.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#analysis-of-mammalian-development"><i class="fa fa-check"></i><b>9.2.3</b> Analysis of mammalian development</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#other-dimensionality-reduction-techniques"><i class="fa fa-check"></i><b>9.3</b> Other dimensionality reduction techniques</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#introduction-2"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#distance-metrics"><i class="fa fa-check"></i><b>10.2</b> Distance metrics</a></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative"><i class="fa fa-check"></i><b>10.3</b> Hierarchic agglomerative</a><ul>
<li class="chapter" data-level="10.3.1" data-path="clustering.html"><a href="clustering.html#linkage-algorithms"><i class="fa fa-check"></i><b>10.3.1</b> Linkage algorithms</a></li>
<li class="chapter" data-level="10.3.2" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets"><i class="fa fa-check"></i><b>10.3.2</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="10.3.3" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues"><i class="fa fa-check"></i><b>10.3.3</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.4</b> K-means</a><ul>
<li class="chapter" data-level="10.4.1" data-path="clustering.html"><a href="clustering.html#algorithm"><i class="fa fa-check"></i><b>10.4.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.4.2" data-path="clustering.html"><a href="clustering.html#choosing-initial-cluster-centres"><i class="fa fa-check"></i><b>10.4.2</b> Choosing initial cluster centres</a></li>
<li class="chapter" data-level="10.4.3" data-path="clustering.html"><a href="clustering.html#choosingK"><i class="fa fa-check"></i><b>10.4.3</b> Choosing k</a></li>
<li class="chapter" data-level="10.4.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-1"><i class="fa fa-check"></i><b>10.4.4</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="10.4.5" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-1"><i class="fa fa-check"></i><b>10.4.5</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>10.5</b> DBSCAN</a><ul>
<li class="chapter" data-level="10.5.1" data-path="clustering.html"><a href="clustering.html#algorithm-1"><i class="fa fa-check"></i><b>10.5.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.5.2" data-path="clustering.html"><a href="clustering.html#implementation-in-r"><i class="fa fa-check"></i><b>10.5.2</b> Implementation in R</a></li>
<li class="chapter" data-level="10.5.3" data-path="clustering.html"><a href="clustering.html#choosing-parameters"><i class="fa fa-check"></i><b>10.5.3</b> Choosing parameters</a></li>
<li class="chapter" data-level="10.5.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-2"><i class="fa fa-check"></i><b>10.5.4</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="10.5.5" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-2"><i class="fa fa-check"></i><b>10.5.5</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#evaluating-cluster-quality"><i class="fa fa-check"></i><b>10.6</b> Evaluating cluster quality</a><ul>
<li class="chapter" data-level="10.6.1" data-path="clustering.html"><a href="clustering.html#silhouetteMethod"><i class="fa fa-check"></i><b>10.6.1</b> Silhouette method</a></li>
<li class="chapter" data-level="10.6.2" data-path="clustering.html"><a href="clustering.html#example---k-means-clustering-of-blobs-data-set"><i class="fa fa-check"></i><b>10.6.2</b> Example - k-means clustering of blobs data set</a></li>
<li class="chapter" data-level="10.6.3" data-path="clustering.html"><a href="clustering.html#example---dbscan-clustering-of-noisy-moons"><i class="fa fa-check"></i><b>10.6.3</b> Example - DBSCAN clustering of noisy moons</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="clustering.html"><a href="clustering.html#exercises-3"><i class="fa fa-check"></i><b>10.7</b> Exercises</a><ul>
<li class="chapter" data-level="10.7.1" data-path="clustering.html"><a href="clustering.html#clusteringEx1"><i class="fa fa-check"></i><b>10.7.1</b> Exercise 1</a></li>
<li class="chapter" data-level="10.7.2" data-path="clustering.html"><a href="clustering.html#clusteringEx2"><i class="fa fa-check"></i><b>10.7.2</b> Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="logistic-regression.html"><a href="logistic-regression.html#resources"><i class="fa fa-check"></i><b>A</b> Resources</a><ul>
<li class="chapter" data-level="A.1" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i><b>A.1</b> Python</a></li>
<li class="chapter" data-level="A.2" data-path="resources.html"><a href="resources.html#machine-learning-data-set-repositories"><i class="fa fa-check"></i><b>A.2</b> Machine learning data set repositories</a><ul>
<li class="chapter" data-level="A.2.1" data-path="resources.html"><a href="resources.html#mldata"><i class="fa fa-check"></i><b>A.2.1</b> MLDATA</a></li>
<li class="chapter" data-level="A.2.2" data-path="resources.html"><a href="resources.html#uci-machine-learning-repository"><i class="fa fa-check"></i><b>A.2.2</b> UCI Machine Learning Repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html"><i class="fa fa-check"></i><b>B</b> Solutions ch. 3 - Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="B.1" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2"><i class="fa fa-check"></i><b>B.1</b> Example 2</a></li>
<li class="chapter" data-level="B.2" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#example-2-1"><i class="fa fa-check"></i><b>B.2</b> Example 2</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html"><i class="fa fa-check"></i><b>C</b> Solutions ch. 4 - Linear and non-linear (logistic) regression</a></li>
<li class="chapter" data-level="D" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html"><i class="fa fa-check"></i><b>D</b> Solutions ch. 5 - Nearest neighbours</a><ul>
<li class="chapter" data-level="D.1" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-1-1"><i class="fa fa-check"></i><b>D.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html"><i class="fa fa-check"></i><b>E</b> Solutions ch. 6 - Decision trees and random forests</a><ul>
<li class="chapter" data-level="E.1" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-1-2"><i class="fa fa-check"></i><b>E.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="solutions-svm.html"><a href="solutions-svm.html"><i class="fa fa-check"></i><b>F</b> Solutions ch. 7 - Support vector machines</a><ul>
<li class="chapter" data-level="F.1" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-1-3"><i class="fa fa-check"></i><b>F.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="solutions-ann.html"><a href="solutions-ann.html"><i class="fa fa-check"></i><b>G</b> Solutions ch. 8 - Artificial neural networks</a><ul>
<li class="chapter" data-level="G.1" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-1-4"><i class="fa fa-check"></i><b>G.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="H" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html"><i class="fa fa-check"></i><b>H</b> Solutions ch. 9 - Dimensionality reduction</a><ul>
<li class="chapter" data-level="H.1" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.1"><i class="fa fa-check"></i><b>H.1</b> Exercise 8.1</a></li>
<li class="chapter" data-level="H.2" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.2"><i class="fa fa-check"></i><b>H.2</b> Exercise 8.2</a></li>
<li class="chapter" data-level="H.3" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.3."><i class="fa fa-check"></i><b>H.3</b> Exercise 8.3.</a></li>
<li class="chapter" data-level="H.4" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.4."><i class="fa fa-check"></i><b>H.4</b> Exercise 8.4.</a></li>
<li class="chapter" data-level="H.5" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.4"><i class="fa fa-check"></i><b>H.5</b> Exercise 8.4</a></li>
<li class="chapter" data-level="H.6" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.6."><i class="fa fa-check"></i><b>H.6</b> Exercise 8.6.</a></li>
<li class="chapter" data-level="H.7" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.7."><i class="fa fa-check"></i><b>H.7</b> Exercise 8.7.</a></li>
<li class="chapter" data-level="H.8" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-8.8."><i class="fa fa-check"></i><b>H.8</b> Exercise 8.8.</a></li>
</ul></li>
<li class="chapter" data-level="I" data-path="solutions-clustering.html"><a href="solutions-clustering.html"><i class="fa fa-check"></i><b>I</b> Solutions ch. 10 - Clustering</a><ul>
<li class="chapter" data-level="I.1" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-1-5"><i class="fa fa-check"></i><b>I.1</b> Exercise 1</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="logistic-regression" class="section level1">
<h1><span class="header-section-number">4</span> Logistic regression</h1>
<p>Supervised learning refers to the task of inferring functions from a labelled set of data. Typically, the labelled dataset contains a matched pair of observations, with <span class="math inline">\(\mathbf{y} = (y_1,\ldots,y_n)^\top\)</span> denoting a vector containing observations of the variable of interest (output variable), and <span class="math inline">\(\mathbf{X}\)</span> denoting a matrix containing the observed values for the explanatory variables (input variables). Depending on the natre of the data, supervised learning involves either regression or classification.</p>
<p>Within a regression setting, we aim to identify how a continuous-valued output variable is predicted from the input variables. That is, we aim to learning a function that maps from the input to output variable, which should reveal something about the nature of the system itself. By learning an appropriate function, we can also predict the values of the output variable at new set of test locations i.e., positions where no observations have been made.</p>
<p>Classification algorithms, on the other hand, deal with discrete-valued outputs, such as where <span class="math inline">\(y\)</span> can be only a <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, or else where it falls into a number of classes e.g., “group 1”, “group 2” or “group 3”. The aim here is to learn how the potentially continuous input values map to the discrete class label, and ultimately, assign class labels for a new set of data.</p>
<p>Within this chapter we will briefly recap linear regression strategies, and introduce nonlinear approaches to regression based on Gaussian processes <a href="logistic-regression.html#regression">4.1</a>. In section <a href="logistic-regression.html#classification">4.2</a> we introduce a variety of classification algorithms, starting with simple logistic regression and Gaussian process classification, before highlighting a variety of other techniques.</p>
<div id="regression" class="section level2">
<h2><span class="header-section-number">4.1</span> Regression</h2>
<p>Within this section, we will make use of the dataset from <span class="citation">(Windram et al. <a href="#ref-windram2012arabidopsis">2012</a>, GSE39597)</span>, which captures the gene expression levels in the model plant <em>Arabidopsis thaliana</em> following infection with the necrotrophic pathogen <em>Botrytis cinerea</em>, considered to be the second most important fungal plant pathogen dues to its ability to cause disease in a range of plants. The dataset represents a time series measuring the gene expression in <em>Arabidopsis</em> leaves following inoculation with <em>Botrytis cinerea</em>. Our variable of interest will therefore typically be the expression level of a particular gene, with the explanatory variable usually being time, or the expression level of a putative regulator.</p>
<p>A pre-processed version of the data is available in the file is available here: . The processed data is a tab delimited file with the fist row containing gene IDs for 163 marker genes. Column 2 contains the time points of observations, with column 3 indicating which time series the datapoint belongs to (control versus infected time series). All subsequent columns indicate (log_2) normalised gene expression values from microarrays (V4 TAIR V9 spotted cDNA array). The expression dataset itself can therefore be split into two: the first <span class="math inline">\(24\)</span> observations represent measurements of <em>Arabidopsis</em> gene expression in a control experiment (uninfected), from <span class="math inline">\(2h\)</span> through <span class="math inline">\(48h\)</span> at <span class="math inline">\(2\)</span> hourly intervals; the second set of <span class="math inline">\(24\)</span> observations represents an infection datasets, from <span class="math inline">\(2h\)</span> after inoculation with <em>Botyris cinerea</em> through <span class="math inline">\(48h\)</span> at <span class="math inline">\(2\)</span> hourly intervals. We can read the dataset in as follows:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">geneindex &lt;-<span class="st"> </span><span class="dv">36</span>
D &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="dt">file =</span> <span class="st">&quot;data/Arabidopsis/Arabidopsis_Botrytis_transpose_3.csv&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="dt">row.names=</span><span class="dv">1</span>)
genenames &lt;-<span class="st"> </span><span class="kw">colnames</span>(D)
Xs &lt;-<span class="st"> </span>D$Time[<span class="dv">1</span>:<span class="dv">24</span>]</code></pre></div>
<p>Exercise 3.1. Plot the gene expression profiles to familiarise yourself with the data. Hint, you may need to add some noise in to the GP.</p>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">4.1.1</span> Linear regression</h3>
<p>One of the simplest regression models is linear regression, which assumes that the variable of interest, <span class="math inline">\(y\)</span>, depends on an explanatory variable, <span class="math inline">\(x\)</span>, via:</p>
<p><span class="math inline">\(y = mx + c.\)</span></p>
<p>For a typical set of data, we have a vector of observations, <span class="math inline">\(\mathbf{y} = \{y_1,y_2,\ldots,y_n\}\)</span> with a corresponding set of explanatory variables. For now we can assume that the explanatory variable is scalar, for example time in hours, such that we have a set of observations, <span class="math inline">\(\mathbf{x} = \{t_1,t_2,\ldots,t_n\}\)</span>. Using linear regression we aim to infer the parameters <span class="math inline">\(m\)</span> and <span class="math inline">\(c\)</span>, which will tell us something about the relationship between the two variables, and allow us to make predictions at a new set of locations, <span class="math inline">\(\mathbf{x}*\)</span>.</p>
<p>Recall that within R, linear regression can be implemented via the  function. In the example below, we perform linear regression for the gene expression of AT2G28890 as a function of time, using the infection time series only:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lm</span>(AT2G28890~Time, <span class="dt">data =</span> D[<span class="dv">25</span>:<span class="kw">nrow</span>(D),])</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = AT2G28890 ~ Time, data = D[25:nrow(D), ])
## 
## Coefficients:
## (Intercept)         Time  
##    10.14010     -0.04997</code></pre>
<p>Linear regression is also implemented within the  package, allowing us to make use of its other utilities. In fact, within , linear regression is performed by calling the function :</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)</code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<pre><code>## Loading required package: ggplot2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(mlbench)
<span class="kw">set.seed</span>(<span class="dv">1</span>)

lrfit &lt;-<span class="st"> </span><span class="kw">train</span>(y~., <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>Xs,<span class="dt">y=</span>D[<span class="dv">25</span>:<span class="kw">nrow</span>(D),geneindex]), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)
predictedValues&lt;-<span class="kw">predict</span>(lrfit)
<span class="kw">summary</span>(lrfit)</code></pre></div>
<pre><code>## 
## Call:
## lm(formula = .outcome ~ ., data = dat)
## 
## Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.77349 -0.17045 -0.01839  0.15795  0.63098 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 10.14010    0.13975   72.56  &lt; 2e-16 ***
## x           -0.04997    0.00489  -10.22 8.14e-10 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 0.3317 on 22 degrees of freedom
## Multiple R-squared:  0.826,  Adjusted R-squared:  0.8181 
## F-statistic: 104.4 on 1 and 22 DF,  p-value: 8.136e-10</code></pre>
<p>Finally, we can also fit to the control dataset and plot the inferred results alongside the observation data:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrfit2 &lt;-<span class="st"> </span><span class="kw">train</span>(y~., <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>Xs,<span class="dt">y=</span>D[<span class="dv">1</span>:<span class="dv">24</span>,geneindex]), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)
predictedValues2&lt;-<span class="kw">predict</span>(lrfit2)

<span class="kw">plot</span>(Xs,D[<span class="dv">25</span>:<span class="kw">nrow</span>(D),geneindex],<span class="dt">type=</span><span class="st">&quot;p&quot;</span>,<span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="kw">min</span>(D[,geneindex])-<span class="fl">0.2</span>, <span class="kw">max</span>(D[,geneindex]+<span class="fl">0.2</span>)),<span class="dt">main=</span>genenames[geneindex])
<span class="kw">points</span>(Xs,D[<span class="dv">1</span>:<span class="dv">24</span>,geneindex],<span class="dt">type=</span><span class="st">&quot;p&quot;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">points</span>(Xs,predictedValues,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;black&quot;</span>)
<span class="kw">points</span>(Xs,predictedValues2,<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-4-1.png" width="672" /></p>
<p>Excercise 3.2. In our example, above, we fitted a linear model to a gene to identify parameters and make predictions using all the data and default settings. We can make use of the  functionality to split our data into training and test sets, which should allow us to guage uncertainty in our parameters and the strength of the model.</p>
<p>Simple extensions to our linear model allow for more than one explanatory variable. For example, we could have two explanatory variables:</p>
<p><span class="math inline">\(y = m_1 x_1 + m_2 x_2 + c,\)</span></p>
<p>which requires inference of <span class="math inline">\([m_1,m_2,c]\)</span>, or <span class="math inline">\(n\)</span> explanatory variables:</p>
<p><span class="math inline">\(y = \sum_{i=1}^n m_i x_i + c.\)</span></p>
<p>requiring inference of <span class="math inline">\(n+1\)</span> parameters, <span class="math inline">\([m_1,\ldots,m_n,c]\)</span>. Such models can be useful when looking to identify statistical relationships that may exist within large datasets. For example, we could linearly regress the expression of AT2G28890 against the expression of a number of putative regulators. The values of the inferred parameters, <span class="math inline">\(m_i\)</span>, should tell us how strongly the expression of gene <span class="math inline">\(i\)</span> influences AT2G28890.</p>
<p>Exercise 3.3. Regress the expression pattern of a gene against putative regulators to identify potential regulatory relationships.</p>
<div id="polynomial-regression" class="section level4">
<h4><span class="header-section-number">4.1.1.1</span> Polynomial regression</h4>
<p>In general, linear models will not be appropriate for a large variety of datasets, particularly when the variable of interest is nonlinear. We can instead try to fit more complex models to the dataset, such as a quadratic function of the form:</p>
<p><span class="math inline">\(y = m_1 x + m_2 x^2 + c,\)</span></p>
<p>where <span class="math inline">\(m = [m_1,m_2,c]\)</span> represent the parameters we’re interested in inferring. Higher order polynomials can be fitted:</p>
<p><span class="math inline">\(y = \sum_{i=1}^{n} m_i x^i + c.\)</span></p>
<p>where <span class="math inline">\(m = [m_1,\ldots,m_n,c]\)</span> are the free parameters. Within R we can infer more complex polynomials to the data using the  package. In the example below we fit a 3rd order polynomial:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrfit3 &lt;-<span class="st"> </span><span class="kw">lm</span>(y~<span class="kw">poly</span>(x,<span class="dt">degree=</span><span class="dv">3</span>), <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>D[<span class="dv">1</span>:<span class="dv">24</span>,<span class="dv">1</span>],<span class="dt">y=</span>D[<span class="dv">1</span>:<span class="dv">24</span>,geneindex]))</code></pre></div>
<p>We can do this within  using the following snippet of code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lrfit3 &lt;-<span class="st"> </span><span class="kw">train</span>(y~<span class="kw">poly</span>(x,<span class="dt">degree=</span><span class="dv">3</span>), <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>D[<span class="dv">1</span>:<span class="dv">24</span>,<span class="dv">1</span>],<span class="dt">y=</span>D[<span class="dv">1</span>:<span class="dv">24</span>,geneindex]), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)
lrfit4 &lt;-<span class="st"> </span><span class="kw">train</span>(y~<span class="kw">poly</span>(x,<span class="dt">degree=</span><span class="dv">3</span>), <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>D[<span class="dv">25</span>:<span class="kw">nrow</span>(D),<span class="dv">1</span>],<span class="dt">y=</span>D[<span class="dv">25</span>:<span class="kw">nrow</span>(D),geneindex]), <span class="dt">method =</span> <span class="st">&quot;lm&quot;</span>)

<span class="kw">plot</span>(Xs,D[<span class="dv">25</span>:<span class="kw">nrow</span>(D),geneindex],<span class="dt">type=</span><span class="st">&quot;p&quot;</span>,<span class="dt">col=</span><span class="st">&quot;black&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="kw">min</span>(D[,geneindex])-<span class="fl">0.2</span>, <span class="kw">max</span>(D[,geneindex]+<span class="fl">0.2</span>)),<span class="dt">main=</span>genenames[geneindex])
<span class="kw">points</span>(Xs,D[<span class="dv">1</span>:<span class="dv">24</span>,geneindex],<span class="dt">type=</span><span class="st">&quot;p&quot;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">lines</span>(Xs,<span class="kw">fitted</span>(lrfit3),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>)
<span class="kw">lines</span>(Xs,<span class="kw">fitted</span>(lrfit4),<span class="dt">type=</span><span class="st">&quot;l&quot;</span>,<span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-6-1.png" width="672" /></p>
<p>Note that the fit appears to be better than for the linear regression model. We can quantify this by looking at the RMSE.</p>
<p>Exercise 3.4. What happens if we fit a much higher order polynomial? Try fitting a polynomial with degree = 20 and plotting the result. The fit appears match much more closely to the observed data. However, intuitively we feel this is wrong. Whilst it may be possible that the data was generated by a polynomial, it’s far more likely that we are overfitting the data. We can evaluate how good the model <em>really</em> is by looking at the RMSE from bootstrapped sampling. How does the RMSE compare? Which model seems to be best?</p>
</div>
</div>
<div id="gaussian-process-regression" class="section level3">
<h3><span class="header-section-number">4.1.2</span> Gaussian process regression</h3>
<p>Gaussian processes regression <span class="citation">(GPR, Williams and Rasmussen <a href="#ref-Williams2006">2006</a>)</span> represents a Bayesian nonparametric approach to regression capable of inferring potentially nonlinear functions a set of observations. Due to the Bayesian nature of these approaches, they tend to be much less prone to overfitting. Within GPR we assume the following:</p>
<p><span class="math inline">\(y = f(x)\)</span></p>
<p>where <span class="math inline">\(f(\cdot)\)</span> is some unknown nonlinear function. Gaussian processes are completely defined by their mean function, <span class="math inline">\(m(x)\)</span>, and covariance function, <span class="math inline">\((x,x^\prime)\)</span>, and we use the notation <span class="math inline">\(f(x) \sim \mathcal{GP}(m(x), k(x,x^\prime))\)</span> to denote a Gaussian process prior. For inference, we are typically have a set of observations, <span class="math inline">\(\mathbf{X}\)</span> and outputs <span class="math inline">\(\mathbf{y}\)</span>, and are interested in inferring the values, <span class="math inline">\(\mathbf{y}^*\)</span>, at new locations, <span class="math inline">\(\mathbf{x}^*\)</span>. We can infer a posterior distribution for <span class="math inline">\(f^*\)</span> using Bayes’ rule. A key advantage of GPs is the preditive distribution is analytically tractible and has the following form:</p>
<p><span class="math inline">\(y* | \mathbf{x}, \mathbf{y}, \mathbf{x}* \sim \mathcal{N}(f^*,K^*)\)</span></p>
<p>where,</p>
<p><span class="math inline">\(f^* = k_*^\top(K)^{-1} y\)</span>, <span class="math inline">\(K^* = k(x_*,x_*)^{-1} - k_*^\top (K)^{-1} k_*\)</span>.</p>
<p>In many cases, we assume our observations are corrupted by independent noise:</p>
<p><span class="math inline">\(y = f(x) + \varepsilon\)</span></p>
<p>where <span class="math inline">\(\varepsilon\)</span> represents observational noise. In this case, we have:</p>
<p><span class="math inline">\(f^* = k_*^\top(K+\sigma_n^2 \mathbb{I})^{-1} y\)</span>, <span class="math inline">\(K^* = k(x_*,x_*)^{-1} - k_*^\top (K+\sigma_n^2 \mathbb{I})^{-1} k_*\)</span>.</p>
<p>Although the most comprehensive GP toolboxes can be found in MATLAB or Python, such as the <a href="http://www.gaussianprocess.org/gpml/code/matlab/doc/">GPML</a> and <a href="https://github.com/SheffieldML/GPy">GPy</a>, toolboxes. some GP resources exist in R, including this <a href="https://www.r-bloggers.com/gaussian-process-regression-with-r/">blog</a>, which has begun to implemented a variety of applications used within the Matlab  package.</p>
<p>In the example, below, we implement some basic GPs to better understand what it is they’re doing. We first require a number of packages.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(MASS)</code></pre></div>
<pre><code>## Loading required package: MASS</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(plyr)</code></pre></div>
<pre><code>## Loading required package: plyr</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(reshape2)</code></pre></div>
<pre><code>## Loading required package: reshape2</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">require</span>(ggplot2)</code></pre></div>
<p>Recall that the GP is completely defined by its mean function and covariance function. We can assume a zero-mean function without loss of generality, but we must define a covariance function ahead of inference. The most commonly used covariance function is the squared exponential covariance function, defined as:</p>
<p><span class="math inline">\(C(x,x^\prime) = \sigma^2 \exp\biggl{(}\frac{(x-x^\prime)^2}{2l^2}\biggr{)}\)</span>,</p>
<p>The SE covariance function encodes for smooth functions, and has two hyperparameters: a length-scale hyperparameter <span class="math inline">\(l\)</span>, which defines roughly how fast the functions change over the input space, and a process variance hyperparameter, <span class="math inline">\(\sigma\)</span>, which encodes the amplitude of the function. In the snippet of code, below, we implement a squared exponential covariance function</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">covSE &lt;-<span class="st"> </span>function(X1,X2,<span class="dt">l=</span><span class="dv">1</span>,<span class="dt">sig=</span><span class="dv">1</span>) {
  K &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(X1)*<span class="kw">length</span>(X2)), <span class="dt">nrow=</span><span class="kw">length</span>(X1))
  for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(K)) {
    for (j in <span class="dv">1</span>:<span class="kw">ncol</span>(K)) {
      K[i,j] &lt;-<span class="st"> </span>sig^<span class="dv">2</span>*<span class="kw">exp</span>(-<span class="fl">0.5</span>*(<span class="kw">abs</span>(X1[i]-X2[j]))^<span class="dv">2</span> /l^<span class="dv">2</span>)
    }
  }
  <span class="kw">return</span>(K)
}</code></pre></div>
<p>To get an idea of what this means, we can generate samples from the GP prior at a set of defined positions along <span class="math inline">\(x\)</span>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x.star &lt;-<span class="st"> </span><span class="kw">seq</span>(-<span class="dv">5</span>,<span class="dv">5</span>,<span class="dt">len=</span><span class="dv">500</span>) ####Define a set of points at which to evaluate the functions
sigma  &lt;-<span class="st"> </span><span class="kw">covSE</span>(x.star,x.star) ###Evaluate the covariance function at those locations, to give the covariance matrix.
y1 &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(x.star)), sigma)
y2 &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(x.star)), sigma)
y3 &lt;-<span class="st"> </span><span class="kw">mvrnorm</span>(<span class="dv">1</span>, <span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(x.star)), sigma)
<span class="kw">plot</span>(y1,<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(<span class="kw">min</span>(y1,y2,y3),<span class="kw">max</span>(y1,y2,y3)))
<span class="kw">lines</span>(y2)
<span class="kw">lines</span>(y3)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-9-1.png" width="672" /></p>
<p>When we specify a GP, we are essentially encoding a distribution over a whole range of functions. Exactly how those functions behave depends upon the choice of covariance function and the hyperparameters. To get a feel for this, try changing the hyperparameters in the above code. A variety of other covariance functions exist, and can be found, with examples in the <a href="http://www.cs.toronto.edu/~duvenaud/cookbook/">Kernel Cookbook</a>.</p>
<p>Exercise 3.4 (optional): Try implementing another covariance function from the <a href="http://www.cs.toronto.edu/~duvenaud/cookbook/">Kernel Cookbook</a> and generating samples from the GP prior.</p>
</div>
<div id="inference-using-gaussian-process-regression" class="section level3">
<h3><span class="header-section-number">4.1.3</span> Inference using Gaussian process regression</h3>
<p>We can generate samples from the GP prior, but what about inference? In linear regression we aimed to infer the parameters, <span class="math inline">\(m\)</span> and <span class="math inline">\(a\)</span>. What is the GP doing? Essentially, it’s representing the (unknown) function in terms of the observed data (and the hyperparameters). Another way to look at it, is that we’ve specified our prior distribution (encoding for all functions of a particular kind), and during the inference procedure we’re giving a greater weight to a subset of those functions that pass close to our observed datapoints.</p>
<p>To demonstrate this, let’s assume we have an unknown function. In our example, for data generation, we will assume this to be <span class="math inline">\(y = sin(x)\)</span> as an illustrative example of a nonlinear function. We might have some observations from this function at a set of input positions <span class="math inline">\(x\)</span> e.g., we have one observation at <span class="math inline">\(x=-2\)</span>:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">c</span>(-<span class="dv">2</span>),
                <span class="dt">y=</span><span class="kw">sin</span>(<span class="kw">c</span>(-<span class="dv">2</span>)))</code></pre></div>
<p>We can infer a posterior GP (and plot this against the true underlying function in red):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">x &lt;-<span class="st"> </span>f$x
k.xx &lt;-<span class="st"> </span><span class="kw">covSE</span>(x,x)
k.xxs &lt;-<span class="st"> </span><span class="kw">covSE</span>(x,x.star)
k.xsx &lt;-<span class="st"> </span><span class="kw">covSE</span>(x.star,x)
k.xsxs &lt;-<span class="st"> </span><span class="kw">covSE</span>(x.star,x.star)

f.star.bar &lt;-<span class="st"> </span>k.xsx%*%<span class="kw">solve</span>(k.xx)%*%f$y  ###Mean
cov.f.star &lt;-<span class="st"> </span>k.xsxs -<span class="st"> </span>k.xsx%*%<span class="kw">solve</span>(k.xx)%*%k.xxs ###Var

<span class="kw">plot</span>(x.star,<span class="kw">sin</span>(x.star),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(-<span class="fl">2.2</span>, <span class="fl">2.2</span>))
<span class="kw">points</span>(f,<span class="dt">type=</span><span class="st">&#39;o&#39;</span>)
<span class="kw">lines</span>(x.star,f.star.bar,<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)
<span class="kw">lines</span>(x.star,f.star.bar<span class="dv">+2</span>*<span class="kw">sqrt</span>(<span class="kw">diag</span>(cov.f.star)),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">pch=</span><span class="dv">22</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)
<span class="kw">lines</span>(x.star,f.star.bar<span class="dv">-2</span>*<span class="kw">sqrt</span>(<span class="kw">diag</span>(cov.f.star)),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">pch=</span><span class="dv">22</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-11-1.png" width="672" /></p>
<p>Of course the fit is not particularly good, but we only had one observation, so this is not surprising. Crucially, we can see that the GP encodes the idea of uncertainty. Although the model fit is not particularly good, we can see exactly <em>where</em> it is no good.</p>
<p>Exercise 3.5 (optional): Try plotting some sample function from the posterior GP. Hint: these will be Gaussian distributed with mean  and covariance .</p>
<p>Let’s start by adding more observations. Here’s what the posterior fit looks like if we include 4 observations (at <span class="math inline">\(x \in [-4,-2,0,1]\)</span>):</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">c</span>(-<span class="dv">4</span>,-<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>),
                <span class="dt">y=</span><span class="kw">sin</span>(<span class="kw">c</span>(-<span class="dv">4</span>,-<span class="dv">2</span>,<span class="dv">0</span>,<span class="dv">1</span>)))
x &lt;-<span class="st"> </span>f$x
k.xx &lt;-<span class="st"> </span><span class="kw">covSE</span>(x,x)
k.xxs &lt;-<span class="st"> </span><span class="kw">covSE</span>(x,x.star)
k.xsx &lt;-<span class="st"> </span><span class="kw">covSE</span>(x.star,x)
k.xsxs &lt;-<span class="st"> </span><span class="kw">covSE</span>(x.star,x.star)

f.star.bar &lt;-<span class="st"> </span>k.xsx%*%<span class="kw">solve</span>(k.xx)%*%f$y  ###Mean
cov.f.star &lt;-<span class="st"> </span>k.xsxs -<span class="st"> </span>k.xsx%*%<span class="kw">solve</span>(k.xx)%*%k.xxs ###Var

<span class="kw">plot</span>(x.star,<span class="kw">sin</span>(x.star),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(-<span class="fl">2.2</span>, <span class="fl">2.2</span>))
<span class="kw">points</span>(f,<span class="dt">type=</span><span class="st">&#39;o&#39;</span>)
<span class="kw">lines</span>(x.star,f.star.bar,<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)
<span class="kw">lines</span>(x.star,f.star.bar<span class="dv">+2</span>*<span class="kw">sqrt</span>(<span class="kw">diag</span>(cov.f.star)),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">pch=</span><span class="dv">22</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)
<span class="kw">lines</span>(x.star,f.star.bar<span class="dv">-2</span>*<span class="kw">sqrt</span>(<span class="kw">diag</span>(cov.f.star)),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">pch=</span><span class="dv">22</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-12-1.png" width="672" /></p>
<p>Definitely getting better, and we still have a good grasp of where the model is uncertain. With <span class="math inline">\(7\)</span> observations:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">f &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x=</span><span class="kw">c</span>(-<span class="dv">4</span>,-<span class="dv">3</span>,-<span class="dv">2</span>,-<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>),
                <span class="dt">y=</span><span class="kw">sin</span>(<span class="kw">c</span>(-<span class="dv">4</span>,-<span class="dv">3</span>,-<span class="dv">2</span>,-<span class="dv">1</span>,<span class="dv">0</span>,<span class="dv">1</span>,<span class="dv">2</span>)))
x &lt;-<span class="st"> </span>f$x
k.xx &lt;-<span class="st"> </span><span class="kw">covSE</span>(x,x)
k.xxs &lt;-<span class="st"> </span><span class="kw">covSE</span>(x,x.star)
k.xsx &lt;-<span class="st"> </span><span class="kw">covSE</span>(x.star,x)
k.xsxs &lt;-<span class="st"> </span><span class="kw">covSE</span>(x.star,x.star)

f.star.bar &lt;-<span class="st"> </span>k.xsx%*%<span class="kw">solve</span>(k.xx)%*%f$y  ###Mean
cov.f.star &lt;-<span class="st"> </span>k.xsxs -<span class="st"> </span>k.xsx%*%<span class="kw">solve</span>(k.xx)%*%k.xxs ###Var

<span class="kw">plot</span>(x.star,<span class="kw">sin</span>(x.star),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">ylim=</span><span class="kw">c</span>(-<span class="fl">2.2</span>, <span class="fl">2.2</span>))
<span class="kw">points</span>(f,<span class="dt">type=</span><span class="st">&#39;o&#39;</span>)
<span class="kw">lines</span>(x.star,f.star.bar,<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>)
<span class="kw">lines</span>(x.star,f.star.bar<span class="dv">+2</span>*<span class="kw">sqrt</span>(<span class="kw">diag</span>(cov.f.star)),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">pch=</span><span class="dv">22</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)
<span class="kw">lines</span>(x.star,f.star.bar<span class="dv">-2</span>*<span class="kw">sqrt</span>(<span class="kw">diag</span>(cov.f.star)),<span class="dt">type =</span> <span class="st">&#39;l&#39;</span>,<span class="dt">pch=</span><span class="dv">22</span>, <span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">col=</span><span class="st">&quot;black&quot;</span>)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>We can see that even with only <span class="math inline">\(7\)</span> observations the posterior GP has begun to resemble the true (nonlinear) function very well: over most of the <span class="math inline">\(x\)</span>-axis the mean of the GP lies very close to the true function and, perhaps more importantly, we continue to have an treatment for the uncertainty.</p>
<div id="marginal-likelihood-and-hyperparameters" class="section level4">
<h4><span class="header-section-number">4.1.3.1</span> Marginal Likelihood and Hyperparameters</h4>
<p>Another key aspect of GP regression is the ability to analytically evaluate the marginal likelihood, otherwise referred to as the “model evidence”. For a GP this is:</p>
<p><span class="math inline">\(\ln p(\mathbf{y}|\mathbf{x}) = -\frac{1}{2}\mathbf{y}^\top (K)^{-1} \mathbf{y} -\frac{1}{2} \ln |K| - \frac{n}{2}\ln 2\pi\)</span></p>
<p>We calculate this in the snippet of code, below:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">calcML &lt;-<span class="st"> </span>function(f,<span class="dt">l=</span><span class="dv">1</span>,<span class="dt">sig=</span><span class="dv">1</span>) {
  f2 &lt;-<span class="st"> </span><span class="kw">t</span>(f)
  yt &lt;-<span class="st"> </span>f2[<span class="dv">2</span>,]
  y  &lt;-<span class="st"> </span>f[,<span class="dv">2</span>]
  K &lt;-<span class="st"> </span><span class="kw">covSE</span>(f[,<span class="dv">1</span>],f[,<span class="dv">1</span>],l,sig)
  ML &lt;-<span class="st"> </span>-<span class="fl">0.5</span>*yt%*%<span class="kw">ginv</span>(K<span class="fl">+0.1</span>^<span class="dv">2</span>*<span class="kw">diag</span>(<span class="kw">length</span>(y)))%*%y -<span class="fl">0.5</span>*<span class="kw">log</span>(<span class="kw">det</span>(K)) -(<span class="kw">length</span>(f[,<span class="dv">1</span>])/<span class="dv">2</span>)*<span class="kw">log</span>(<span class="dv">2</span>*pi);
  <span class="kw">return</span>(ML)
}</code></pre></div>
<p>The ability to calculate the marginal likelihood gives us a way to automatically select the hyperparameters. We can increment hyperparameters over a range of values, and choose the values that yield the greatest marginal likelihood. In the example, below, we increment both the length-scale and process variance hyperparameter:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">###install.packages(&quot;plot3D&quot;)
<span class="kw">library</span>(plot3D)

par &lt;-<span class="st"> </span><span class="kw">seq</span>(.<span class="dv">1</span>,<span class="dv">10</span>,<span class="dt">by=</span><span class="fl">0.1</span>)
ML &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(par)^<span class="dv">2</span>), <span class="dt">nrow=</span><span class="kw">length</span>(par), <span class="dt">ncol=</span><span class="kw">length</span>(par))
for(i in <span class="dv">1</span>:<span class="kw">length</span>(par)) {
  for(j in <span class="dv">1</span>:<span class="kw">length</span>(par)) {
    ML[i,j] &lt;-<span class="st"> </span><span class="kw">calcML</span>(f,par[i],par[j])
  }
}
<span class="kw">persp3D</span>(<span class="dt">z =</span> ML,<span class="dt">theta =</span> <span class="dv">120</span>)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-15-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">ind&lt;-<span class="kw">which</span>(ML==<span class="kw">max</span>(ML), <span class="dt">arr.ind=</span><span class="ot">TRUE</span>)
<span class="kw">print</span>(<span class="kw">c</span>(<span class="st">&quot;length-scale&quot;</span>, par[ind[<span class="dv">1</span>]]))</code></pre></div>
<pre><code>## [1] &quot;length-scale&quot; &quot;2.4&quot;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">c</span>(<span class="st">&quot;process variance&quot;</span>, par[ind[<span class="dv">2</span>]]))</code></pre></div>
<pre><code>## [1] &quot;process variance&quot; &quot;1.3&quot;</code></pre>
<p>Here we have performed a grid search to identify the optimal hyperparameters. In actuality, the derivative of the marginal likelihood with respect to the hyperparameters is analytically tractable, allowing us to optimise using gradient search algorithms.</p>
<p>Exercise 3.6: Try fitting plotting the GP using the optimised hyperparameter values.</p>
<p>Exercise 3.7: Now try fitting a Gaussian process to one of the gene expression profiles in the Botrytis dataset. Hint: You may need to normalise the time axis. Since this data also contains a high level of noise you will also need to use covariance function/ML calculation that incorporates this (the code below does this, with the noise representing a <span class="math inline">\(3\)</span>rd hyperparameter).</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">covSEn &lt;-<span class="st"> </span>function(X1,X2,<span class="dt">l=</span><span class="dv">1</span>,<span class="dt">sig=</span><span class="dv">1</span>,<span class="dt">sigman=</span><span class="fl">0.1</span>) {
  K &lt;-<span class="st"> </span><span class="kw">matrix</span>(<span class="kw">rep</span>(<span class="dv">0</span>, <span class="kw">length</span>(X1)*<span class="kw">length</span>(X2)), <span class="dt">nrow=</span><span class="kw">length</span>(X1))
  for (i in <span class="dv">1</span>:<span class="kw">nrow</span>(K)) {
    for (j in <span class="dv">1</span>:<span class="kw">ncol</span>(K)) {
      
      K[i,j] &lt;-<span class="st"> </span>sig^<span class="dv">2</span>*<span class="kw">exp</span>(-<span class="fl">0.5</span>*(<span class="kw">abs</span>(X1[i]-X2[j]))^<span class="dv">2</span> /l^<span class="dv">2</span>)
      
      if (i==j){
      K[i,j] &lt;-<span class="st"> </span>K[i,j] +<span class="st"> </span>sigman^<span class="dv">2</span>
      }
      
    }
  }
  <span class="kw">return</span>(K)
}</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">calcMLn &lt;-<span class="st"> </span>function(f,<span class="dt">l=</span><span class="dv">1</span>,<span class="dt">sig=</span><span class="dv">1</span>,<span class="dt">sigman=</span><span class="fl">0.1</span>) {
  f2 &lt;-<span class="st"> </span><span class="kw">t</span>(f)
  yt &lt;-<span class="st"> </span>f2[<span class="dv">2</span>,]
  y  &lt;-<span class="st"> </span>f[,<span class="dv">2</span>]
  K &lt;-<span class="st"> </span><span class="kw">covSE</span>(f[,<span class="dv">1</span>],f[,<span class="dv">1</span>],l,sig)
  ML &lt;-<span class="st"> </span>-<span class="fl">0.5</span>*yt%*%<span class="kw">ginv</span>(K+<span class="kw">diag</span>(<span class="kw">length</span>(y))*sigman^<span class="dv">2</span>)%*%y -<span class="fl">0.5</span>*<span class="kw">log</span>(<span class="kw">det</span>(K+<span class="kw">diag</span>(<span class="kw">length</span>(y))*sigman^<span class="dv">2</span>)) -(<span class="kw">length</span>(f[,<span class="dv">1</span>])/<span class="dv">2</span>)*<span class="kw">log</span>(<span class="dv">2</span>*pi);
  <span class="kw">return</span>(ML)
}</code></pre></div>
</div>
</div>
<div id="model-selection" class="section level3">
<h3><span class="header-section-number">4.1.4</span> Model Selection</h3>
<p>As well as being a useful criterion for selecting hyperparameters, the marginal likelihood can be used as a basis for selecting models. For example, we might be interested in comparing how well we fit the data using two different covariance functions: for example a squared exponential covariance function (model 1, <span class="math inline">\(M_1\)</span>) versus a periodic covariance function (model 2, <span class="math inline">\(M_2\)</span>). By taking the ratio of the marginal likelihoods we calculate the <a href="https://en.wikipedia.org/wiki/Bayes_factor">Bayes’ Factor</a> (BF) which allows us to determine which model is the best:</p>
<p><span class="math inline">\(\mbox{BF} = \frac{ML(M_1)}{ML(M_2)}\)</span>.</p>
<p>Very high values for the BF indicate strong evidence for <span class="math inline">\(M_1\)</span> over <span class="math inline">\(M_2\)</span>, whilst low values would indicate the contrary.</p>
<div id="advanced-application-1-differential-expression-of-time-series" class="section level4">
<h4><span class="header-section-number">4.1.4.1</span> Advanced application 1: differential expression of time series</h4>
<p>Differential expression analysis is concerned with identifying if two sets of data are significantly different. For example, if one measured the expression level of a gene in two different conditions, you could use an appropriate statistical test to determine whether the expression of those genes had changed significantly in the two conditions. These test commonly include t-tests, which are not appropriate when dealing with time series data (illustrated in Figure <a href="dimensionality-reduction.html#fig:dimreduc">9.1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:timeser"></span>
<img src="images/TimeSeries.jpg" alt="Differential expression analysis for time series. Here we have two time series with very different behaviour (right). However, as a whole the mean and variance of the time series is identical (left) and the datasets are not differentially expressed using a t-test (p&lt;0.9901)" width="55%" />
<p class="caption">
Figure 4.1: Differential expression analysis for time series. Here we have two time series with very different behaviour (right). However, as a whole the mean and variance of the time series is identical (left) and the datasets are not differentially expressed using a t-test (p&lt;0.9901)
</p>
</div>
<p>Gaussian processes regression represents a useful way of modelling time series, and can therefore be used as a basis for detecting differential expression. To do so we can write down two competing modes: (i) the two time series are differentially expressed, and are therefore best described by two independent GPs; (ii) the two time series are noisy observations from an identical underlying process, and are therefore best described by a single joint GP.</p>
<p>Exercise 3.8 (optional): Write a function for determining differential expression for two genes. Hint: you will need to fit <span class="math inline">\(3\)</span> GPs: one to the mock, one to the control, and one to the union of mock and control. You can use the ML to determine if the gene is differentially expressed.</p>
</div>
<div id="advanced-application-2-timing-of-differential-expression" class="section level4">
<h4><span class="header-section-number">4.1.4.2</span> Advanced Application 2: Timing of differential expression</h4>
<p>Nonlinear Bayesian regression represent a powerful tool for modelling. In the previous section we have shown how GPs can be used to model if two time series are differentially expressed. More advanced models using GPs aim to identify <strong>when</strong> two (or more) time series diverge <span class="citation">(Stegle et al. <a href="#ref-Stegle2010robust">2010</a>,<span class="citation">J. Yang et al. (<a href="#ref-yang2016inferring">2016</a>)</span>,<span class="citation">C. A. Penfold et al. (<a href="#ref-penfold2017nonparametric">2017</a>)</span>)</span>. The <a href="https://github.com/ManchesterBioinference/DEtime">DEtime</a> algorithm <span class="citation">(J. Yang et al. <a href="#ref-yang2016inferring">2016</a>)</span> is an R-based codebase for doing so.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">###install.packages(&quot;devtools&quot;)
<span class="kw">library</span>(devtools)
###install_github(&quot;ManchesterBioinference/DEtime&quot;)
###import(DEtime)
<span class="kw">library</span>(DEtime)</code></pre></div>
<p>Within , we can call the function  to calculate marginal likelihoods for two time series, similar to our application in the previous section. Note that here, unlike our analysis, the hyperparameters are optimised by gradient search rather than grid searches.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res_rank &lt;-<span class="st"> </span><span class="kw">DEtime_rank</span>(<span class="dt">ControlTimes =</span> Xs, <span class="dt">ControlData =</span> D[<span class="dv">1</span>:<span class="dv">24</span>,<span class="dv">3</span>], <span class="dt">PerturbedTimes =</span> Xs, <span class="dt">PerturbedData =</span> D[<span class="dv">25</span>:<span class="dv">48</span>,<span class="dv">3</span>], <span class="dt">savefile=</span><span class="ot">TRUE</span>)</code></pre></div>
<pre><code>## [1] &quot;gene IDs are not provided. Numbers are used instead&quot;
## rank list saved in DEtime_rank.txt</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co">#idx &lt;- which(res_rank[,2]&gt;1)</span></code></pre></div>
<p>For genes that are DE, we identify the timing of divergence between two time series using the function .</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res &lt;-<span class="st"> </span><span class="kw">DEtime_infer</span>(<span class="dt">ControlTimes =</span> Xs, <span class="dt">ControlData =</span> D[<span class="dv">1</span>:<span class="dv">24</span>,<span class="dv">3</span>], <span class="dt">PerturbedTimes =</span> Xs, <span class="dt">PerturbedData =</span> D[<span class="dv">25</span>:<span class="dv">48</span>,<span class="dv">3</span>])</code></pre></div>
<pre><code>## gene IDs are not provided. Numbers are used instead.
## Testing perturbation time points are not provided. Default one is used.
## gene 1 is done
## DEtime inference is done.
## Please use print_DEtime or plot_DEtime to view the results.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">print_DEtime</code></pre></div>
<pre><code>## function (DEtimeOutput) 
## {
##     cat(&quot;Perturbation point inference results from DEtime package: \n&quot;)
##     cat(&quot;==========================================================\n&quot;)
##     print(DEtimeOutput$result, sep = &quot;\t&quot;, zero.print = &quot;.&quot;)
##     cat(&quot;==========================================================\n&quot;)
## }
## &lt;environment: namespace:DEtime&gt;</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot_DEtime</span>(res)</code></pre></div>
<pre><code>## All genes will be plotted 
## 1 is plotted</code></pre>
<p>We can do it for all genes using the example below. By doing so, we can plot a histogram of the times of divergence, shedding light on the temporal progression of the infection process.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">res &lt;-<span class="st"> </span><span class="kw">DEtime_infer</span>(<span class="dt">ControlTimes =</span> Xs, <span class="dt">ControlData =</span> <span class="kw">t</span>(D[<span class="dv">1</span>:<span class="dv">24</span>,]), <span class="dt">PerturbedTimes =</span> Xs, <span class="dt">PerturbedData =</span> <span class="kw">t</span>(D[<span class="dv">25</span>:<span class="dv">48</span>,]))</code></pre></div>
<pre><code>## ControlData is accepted
## PerturbedData is accepted
## gene IDs are not provided. Numbers are used instead.
## Testing perturbation time points are not provided. Default one is used.
## gene 1 is done
## gene 2 is done
## gene 3 is done
## gene 4 is done
## gene 5 is done
## gene 6 is done
## gene 7 is done
## gene 8 is done
## gene 9 is done
## gene 10 is done
## gene 11 is done
## gene 12 is done
## gene 13 is done
## gene 14 is done
## gene 15 is done
## gene 16 is done
## gene 17 is done
## gene 18 is done
## gene 19 is done
## gene 20 is done
## gene 21 is done
## gene 22 is done
## gene 23 is done
## gene 24 is done
## gene 25 is done
## gene 26 is done
## gene 27 is done
## gene 28 is done
## gene 29 is done
## gene 30 is done
## gene 31 is done
## gene 32 is done
## gene 33 is done
## gene 34 is done
## gene 35 is done
## gene 36 is done
## gene 37 is done
## gene 38 is done
## gene 39 is done
## gene 40 is done
## gene 41 is done
## gene 42 is done
## gene 43 is done
## gene 44 is done
## gene 45 is done
## gene 46 is done
## gene 47 is done
## gene 48 is done
## gene 49 is done
## gene 50 is done
## gene 51 is done
## gene 52 is done
## gene 53 is done
## gene 54 is done
## gene 55 is done
## gene 56 is done
## gene 57 is done
## gene 58 is done
## gene 59 is done
## gene 60 is done
## gene 61 is done
## gene 62 is done
## gene 63 is done
## gene 64 is done
## gene 65 is done
## gene 66 is done
## gene 67 is done
## gene 68 is done
## gene 69 is done
## gene 70 is done
## gene 71 is done
## gene 72 is done
## gene 73 is done
## gene 74 is done
## gene 75 is done
## gene 76 is done
## gene 77 is done
## gene 78 is done
## gene 79 is done
## gene 80 is done
## gene 81 is done
## gene 82 is done
## gene 83 is done
## gene 84 is done
## gene 85 is done
## gene 86 is done
## gene 87 is done
## gene 88 is done
## gene 89 is done
## gene 90 is done
## gene 91 is done
## gene 92 is done
## gene 93 is done
## gene 94 is done
## gene 95 is done
## gene 96 is done
## gene 97 is done
## gene 98 is done
## gene 99 is done
## gene 100 is done
## gene 101 is done
## gene 102 is done
## gene 103 is done
## gene 104 is done
## gene 105 is done
## gene 106 is done
## gene 107 is done
## gene 108 is done
## gene 109 is done
## gene 110 is done
## gene 111 is done
## gene 112 is done
## gene 113 is done
## gene 114 is done
## gene 115 is done
## gene 116 is done
## gene 117 is done
## gene 118 is done
## gene 119 is done
## gene 120 is done
## gene 121 is done
## gene 122 is done
## gene 123 is done
## gene 124 is done
## gene 125 is done
## gene 126 is done
## gene 127 is done
## gene 128 is done
## gene 129 is done
## gene 130 is done
## gene 131 is done
## gene 132 is done
## gene 133 is done
## gene 134 is done
## gene 135 is done
## gene 136 is done
## gene 137 is done
## gene 138 is done
## gene 139 is done
## gene 140 is done
## gene 141 is done
## gene 142 is done
## gene 143 is done
## gene 144 is done
## gene 145 is done
## gene 146 is done
## gene 147 is done
## gene 148 is done
## gene 149 is done
## gene 150 is done
## gene 151 is done
## gene 152 is done
## gene 153 is done
## gene 154 is done
## gene 155 is done
## gene 156 is done
## gene 157 is done
## gene 158 is done
## gene 159 is done
## gene 160 is done
## gene 161 is done
## gene 162 is done
## gene 163 is done
## gene 164 is done
## DEtime inference is done.
## Please use print_DEtime or plot_DEtime to view the results.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(<span class="kw">as.numeric</span>(res$result[,<span class="dv">2</span>]))</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">hist</span>(<span class="kw">as.numeric</span>(res$result[,<span class="dv">2</span>]),<span class="dt">breaks=</span><span class="dv">20</span>)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-21-2.png" width="672" /></p>
</div>
<div id="scalability" class="section level4">
<h4><span class="header-section-number">4.1.4.3</span> Scalability</h4>
<p>Whilst GPs represent a powerful approach to nonlinear regression, they do have some limitations. Computationally, GPs do not scale well with the number of observations, and standard GP approaches are not suitable when we have a large number of observations. In your own time, you can try generating increasingly long time series, and attempting to fit a GP to the data. To overcome these limitations, approximate approaches to inference with GPs have been developed.</p>
</div>
</div>
</div>
<div id="classification" class="section level2">
<h2><span class="header-section-number">4.2</span> Classification</h2>
<p>Classification algorithms are a supervised learning techniques that assign data to categorical outputs. For example we may have a continuous input variable, <span class="math inline">\(x\)</span>, and want to learn how that variable maps to a discrete valued output, <span class="math inline">\(y\in [0,1]\)</span>, which might represent a phenotype (infected versus uninfected).</p>
<div id="logistic-regression" class="section level3">
<h3><span class="header-section-number">4.2.1</span> Logistic regression</h3>
<p>Linear regression models attempted to fit to real valued data, and are therefore not appropriate for classification. The simplest form of classification is logistic regression, which attempt to fit to the log odds:</p>
<p><span class="math inline">\(\ln \biggl{(}\frac{\mathbb{P}(y=1|x)}{1-\mathbb{P}(y=1|x)}\biggr{)} = c + m_1 x_1 + \ldots m_n x_n\)</span></p>
<p>Unlike linear regression, these models do not admit closed form solutions, but are typically solved iteratively. Within , logistic regression can applied using the  function. To illustate this we will again make use of our plant dataset. Within this dataset, the second column represents a binary variable indicating the infection status of the plant e.g., whether the plant has been infected with <em>Botrytis cinerea</em> or not. We will aim to learn a set of markers capable of predicting infection status. We first run a logistic regression:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(pROC)</code></pre></div>
<pre><code>## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation.</code></pre>
<pre><code>## 
## Attaching package: &#39;pROC&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:stats&#39;:
## 
##     cov, smooth, var</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ROCR)</code></pre></div>
<pre><code>## Loading required package: gplots</code></pre>
<pre><code>## 
## Attaching package: &#39;gplots&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:stats&#39;:
## 
##     lowess</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">warn=</span>-<span class="dv">1</span>)
mod_fit &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>., <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D[,<span class="dv">3</span>:<span class="kw">ncol</span>(D)], <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;glm&quot;</span>, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)</code></pre></div>
<p>We will now load in a second dataset, containnig a new set of observatoins not seen by the model, and attempt to predict the infection status.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">Dpred &lt;-<span class="st"> </span><span class="kw">read.csv</span>(<span class="dt">file =</span> <span class="st">&quot;data/Arabidopsis/Arabidopsis_Botrytis_pred_transpose_3.csv&quot;</span>, <span class="dt">header =</span> <span class="ot">TRUE</span>, <span class="dt">sep =</span> <span class="st">&quot;,&quot;</span>, <span class="dt">row.names=</span><span class="dv">1</span>)

prob &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_fit, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> Dpred[,<span class="dv">3</span>:<span class="kw">ncol</span>(Dpred)], <span class="dt">y =</span> <span class="kw">as.factor</span>(Dpred$Class)), <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)
pred &lt;-<span class="st"> </span><span class="kw">prediction</span>(prob$<span class="st">`</span><span class="dt">1</span><span class="st">`</span>, <span class="kw">as.factor</span>(Dpred$Class))</code></pre></div>
<p>To evaluate how well the algorithm has done, we can calculate a variety of summary statistics. For example the number of true positives, true negatives, false positive and false negatives. A useful summary is to plot the ROC curve (false positive rate versus true positive rate) and calculate the area under that curve. For a perfect algorithm, the area under this curve (AUC) will be equal to <span class="math inline">\(1\)</span>, whereas random assignment would give an area of <span class="math inline">\(0.5\)</span>. In the example below, we will calculate the AUC for a logistic regression model:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
<span class="kw">plot</span>(perf)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-24-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)
auc &lt;-<span class="st"> </span>auc@y.values[[<span class="dv">1</span>]]
auc</code></pre></div>
<pre><code>## [1] 0.5677083</code></pre>
</div>
</div>
<div id="gp-classification" class="section level2">
<h2><span class="header-section-number">4.3</span> GP classification</h2>
<p>Classification approaches using Gaussian processes are also possible. Unlike Gaussian process regression, Gaussian process classification is not analytically tractable, and we must instead use approximations. A GP classified has been implemented in  using a polynomial kernel, and can be called using the following code:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">mod_fit2 &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>., <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D[,<span class="dv">3</span>:<span class="kw">ncol</span>(D)], <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;gaussprPoly&quot;</span>)</code></pre></div>
<pre><code>## 
## Attaching package: &#39;kernlab&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     alpha</code></pre>
<p>We can again evaluate how well the algorithm has done:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">prob &lt;-<span class="st"> </span><span class="kw">predict</span>(mod_fit2, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> Dpred[,<span class="dv">3</span>:<span class="kw">ncol</span>(Dpred)], <span class="dt">y =</span> <span class="kw">as.factor</span>(Dpred$Class)), <span class="dt">type=</span><span class="st">&quot;prob&quot;</span>)
pred &lt;-<span class="st"> </span><span class="kw">prediction</span>(prob$<span class="st">`</span><span class="dt">1</span><span class="st">`</span>, <span class="kw">as.factor</span>(Dpred$Class))
perf &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;tpr&quot;</span>, <span class="dt">x.measure =</span> <span class="st">&quot;fpr&quot;</span>)
<span class="kw">plot</span>(perf)</code></pre></div>
<p><img src="03-logistic-regression_files/figure-html/unnamed-chunk-26-1.png" width="672" /></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">auc &lt;-<span class="st"> </span><span class="kw">performance</span>(pred, <span class="dt">measure =</span> <span class="st">&quot;auc&quot;</span>)
auc &lt;-<span class="st"> </span>auc@y.values[[<span class="dv">1</span>]]
auc</code></pre></div>
<pre><code>## [1] 0.9577908</code></pre>
<div id="other-classification-approaches." class="section level3">
<h3><span class="header-section-number">4.3.1</span> Other classification approaches.</h3>
<p>In some instances, ensemble learning, which combines predictions from multiple algorithms, can increase the predictive accuracy in classification tasks. A large variety of classification algorithms are availble in , including random forests, support vector machines, Bayesian generalised linear models. Some of these approaches have been implemented below.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;stepPlr&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;caTools&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;arm&quot;</span>)</code></pre></div>
<pre><code>## Loading required package: Matrix</code></pre>
<pre><code>## Loading required package: lme4</code></pre>
<pre><code>## 
## arm (Version 1.9-3, built: 2016-11-21)</code></pre>
<pre><code>## Working directory is /Users/matt/git_repositories/intro-machine-learning</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;party&quot;</span>)</code></pre></div>
<pre><code>## Loading required package: grid</code></pre>
<pre><code>## Loading required package: mvtnorm</code></pre>
<pre><code>## Loading required package: modeltools</code></pre>
<pre><code>## Loading required package: stats4</code></pre>
<pre><code>## 
## Attaching package: &#39;modeltools&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:lme4&#39;:
## 
##     refit</code></pre>
<pre><code>## The following object is masked from &#39;package:kernlab&#39;:
## 
##     prior</code></pre>
<pre><code>## The following object is masked from &#39;package:plyr&#39;:
## 
##     empty</code></pre>
<pre><code>## Loading required package: strucchange</code></pre>
<pre><code>## Loading required package: zoo</code></pre>
<pre><code>## 
## Attaching package: &#39;zoo&#39;</code></pre>
<pre><code>## The following objects are masked from &#39;package:base&#39;:
## 
##     as.Date, as.Date.numeric</code></pre>
<pre><code>## Loading required package: sandwich</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(<span class="st">&quot;kernlab&quot;</span>)
<span class="kw">library</span>(<span class="st">&quot;nnet&quot;</span>)
mod_fit1 &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D$AT1G03960, <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;plr&quot;</span>)

mod_fit2 &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D$AT1G03960, <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;LogitBoost&quot;</span>, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)

mod_fit3 &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D$AT1G03960, <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;bayesglm&quot;</span>, <span class="dt">family=</span><span class="st">&quot;binomial&quot;</span>)

mod_fit4 &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D$AT1G03960, <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;cforest&quot;</span>)

mod_fit5 &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D$AT1G03960, <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;svmRadialWeights&quot;</span>)

mod_fit6 &lt;-<span class="st"> </span><span class="kw">train</span>(y ~<span class="st"> </span>x, <span class="dt">data=</span><span class="kw">data.frame</span>(<span class="dt">x =</span> D$AT1G03960, <span class="dt">y =</span> <span class="kw">as.factor</span>(D$Class)), <span class="dt">method=</span><span class="st">&quot;multinom&quot;</span>)</code></pre></div>
<pre><code>## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 30.519099
## final  value 30.519095 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.947216 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 30.549852
## final  value 30.549849 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 25.636358
## final  value 25.636172 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.267782 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 25.713037
## final  value 25.712865 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 29.055866
## final  value 29.055660 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.570213 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 29.098966
## final  value 29.098804 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.167482 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.999983 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.178039 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.948048 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.252251 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.953327 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 30.493155 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.661797 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 30.527775 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 29.425208
## iter  20 value 29.425121
## final  value 29.425115 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.110804 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 29.464259
## iter  20 value 29.464192
## final  value 29.464187 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 30.805327
## iter  10 value 30.805326
## iter  10 value 30.805326
## final  value 30.805326 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.036134 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 30.836670
## iter  10 value 30.836670
## iter  10 value 30.836670
## final  value 30.836670 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 31.270571 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.687211 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 31.287343 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 23.982050
## iter  20 value 23.973564
## iter  30 value 23.970664
## iter  40 value 23.969663
## iter  50 value 23.969316
## iter  60 value 23.969197
## iter  70 value 23.969156
## final  value 23.969148 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.160340 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 24.161702
## iter  20 value 24.155332
## iter  30 value 24.153316
## iter  40 value 24.152661
## iter  50 value 24.152448
## iter  60 value 24.152378
## final  value 24.152357 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 30.692224 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.841096 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 30.718870 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 31.265330 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.032727 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 31.287627 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.036951 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.139283 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.048115 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 29.350332 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.502353 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 29.394715 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.270383 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.010581 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.280061 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 30.395874 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.556082 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 30.419405 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 31.565403 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.009702 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 31.583442 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 23.540015
## iter  20 value 23.530522
## final  value 23.530454 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.442863 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 23.706005
## iter  20 value 23.699247
## final  value 23.699226 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 29.830235 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.429878 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 29.860272 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 27.681555 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.764666 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 27.752198 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 29.950934
## final  value 29.950910 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.928940 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 30.002486
## final  value 30.002468 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.492517 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.132159 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.498412 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 28.798141 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.898210 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 28.861514 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.513424 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 33.177424 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 32.521012 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 27.532922
## iter  20 value 27.532659
## final  value 27.532640 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 31.956413 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## iter  10 value 27.590425
## iter  20 value 27.590214
## final  value 27.590199 
## converged
## # weights:  3 (2 variable)
## initial  value 33.271065 
## final  value 30.783532 
## converged</code></pre>
<p>Excercise 3.7. Take a look at the predictive accuracy of some of the other algorithms. How would you begin to go about combining predictions from multiple algorithms?</p>
</div>
</div>
<div id="resources" class="section level2">
<h2><span class="header-section-number">4.4</span> Resources</h2>
<p>A variety of examples using  to perform regression and classification have been implemented <a href="https://github.com/tobigithub/caret-machine-learning">here</a>.</p>
<p>More comprehensive Gaussian process packages are available in Matlab including <a href="http://www.gaussianprocess.org/gpml/code/matlab/doc/">GPML</a>, and Python, <a href="https://github.com/SheffieldML/GPy">GPy</a>. GP code based on <a href="https://www.tensorflow.org">Tensorflow</a> is also available in the form of <a href="http://gpflow.readthedocs.io/en/latest/intro.html">GPflow</a> and <a href="https://github.com/GPflow/GPflowOpt">GPflowopt</a>.</p>
<p>======= ## Exercises</p>
<p>Solutions to exercises can be found in appendix <a href="solutions-logistic-regression.html#solutions-logistic-regression">C</a>.</p>

</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-windram2012arabidopsis">
<p>Windram, Oliver, Priyadharshini Madhou, Stuart McHattie, Claire Hill, Richard Hickman, Emma Cooke, Dafyd J Jenkins, et al. 2012. “Arabidopsis Defense Against Botrytis Cinerea: Chronology and Regulation Deciphered by High-Resolution Temporal Transcriptomic Analysis.” <em>The Plant Cell</em> 24 (9). Am Soc Plant Biol: 3530–57.</p>
</div>
<div id="ref-Williams2006">
<p>Williams, Christopher KI, and Carl Edward Rasmussen. 2006. <em>Gaussian Processes for Machine Learning</em>. Vol. 2. the MIT Press.</p>
</div>
<div id="ref-Stegle2010robust">
<p>Stegle, Oliver, Katherine J Denby, Emma J Cooke, David L Wild, Zoubin Ghahramani, and Karsten M Borgwardt. 2010. “A Robust Bayesian Two-Sample Test for Detecting Intervals of Differential Gene Expression in Microarray Time Series.” <em>Journal of Computational Biology</em> 17 (3). Mary Ann Liebert, Inc. 140 Huguenot Street, 3rd Floor New Rochelle, NY 10801 USA: 355–67. <a href="http://online.liebertpub.com/doi/abs/10.1089/cmb.2009.0175" class="uri">http://online.liebertpub.com/doi/abs/10.1089/cmb.2009.0175</a>.</p>
</div>
<div id="ref-yang2016inferring">
<p>Yang, Jing, Christopher A Penfold, Murray R Grant, and Magnus Rattray. 2016. “Inferring the Perturbation Time from Biological Time Course Data.” <em>Bioinformatics</em>. Oxford Univ Press, btw329.</p>
</div>
<div id="ref-penfold2017nonparametric">
<p>Penfold, Christopher Andrew, Anastasiya Sybirna, John Reid, Yun Huang, Lorenz Wernisch, Zoubin Ghahramani, Murray Grant, and M Azim Surani. 2017. “Nonparametric Bayesian Inference of Transcriptional Branching and Recombination Identifies Regulators of Early Human Germ Cell Development.” <em>BioRxiv</em>. Cold Spring Harbor Labs Journals, 167684.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="linear-models.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="nearest-neighbours.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bioinformatics-training/intro-machine-learning/edit/master/03-logistic-regression.Rmd",
"text": "Edit"
},
"download": ["intro-machine-learning.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
