<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta name="description" content="Course materials for An Introduction to Machine Learning">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="figures/cover_image.png" />
  <meta property="og:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="github-repo" content="bioinformatics-training/intro-machine-learning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="twitter:image" content="figures/cover_image.png" />

<meta name="author" content="Sudhakaran Prabakaran, Matt Wayland and Chris Penfold">


<meta name="date" content="2017-09-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="logistic-regression.html">
<link rel="next" href="decision-trees.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Introduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About the course</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#registration"><i class="fa fa-check"></i><b>1.2</b> Registration</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4</b> Github</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.5</b> License</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6</b> Contact</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.7</b> Colophon</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>3</b> Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-models.html"><a href="linear-models.html#exercises"><i class="fa fa-check"></i><b>3.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Linear and non linear logistic regression</a><ul>
<li class="chapter" data-level="4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#exercises-1"><i class="fa fa-check"></i><b>4.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html"><i class="fa fa-check"></i><b>5</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#measuring-distance-between-objects"><i class="fa fa-check"></i><b>5.1.1</b> Measuring distance between objects</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#classification"><i class="fa fa-check"></i><b>5.2</b> Classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#algorithm"><i class="fa fa-check"></i><b>5.2.1</b> Algorithm</a></li>
<li class="chapter" data-level="5.2.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#simulated-data"><i class="fa fa-check"></i><b>5.2.2</b> Simulated data</a></li>
<li class="chapter" data-level="5.2.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-function"><i class="fa fa-check"></i><b>5.2.3</b> knn function</a></li>
<li class="chapter" data-level="5.2.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#plotting-decision-boundaries"><i class="fa fa-check"></i><b>5.2.4</b> Plotting decision boundaries</a></li>
<li class="chapter" data-level="5.2.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.2.5</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="5.2.6" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#choosing-k"><i class="fa fa-check"></i><b>5.2.6</b> Choosing <em>k</em></a></li>
<li class="chapter" data-level="5.2.7" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#feature-selection"><i class="fa fa-check"></i><b>5.2.7</b> Feature selection</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#regression"><i class="fa fa-check"></i><b>5.3</b> Regression</a></li>
<li class="chapter" data-level="5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#exercises-2"><i class="fa fa-check"></i><b>5.4</b> Exercises</a><ul>
<li class="chapter" data-level="5.4.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx1"><i class="fa fa-check"></i><b>5.4.1</b> Exercise 1</a></li>
<li class="chapter" data-level="5.4.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx2"><i class="fa fa-check"></i><b>5.4.2</b> Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>6</b> Decision trees and random forests</a><ul>
<li class="chapter" data-level="6.1" data-path="decision-trees.html"><a href="decision-trees.html#exercises-3"><i class="fa fa-check"></i><b>6.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>7</b> Support vector machines</a><ul>
<li class="chapter" data-level="7.1" data-path="svm.html"><a href="svm.html#exercises-4"><i class="fa fa-check"></i><b>7.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8</b> Artificial neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html#exercises-5"><i class="fa fa-check"></i><b>8.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>9</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="9.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-dimensionality-reduction"><i class="fa fa-check"></i><b>9.1</b> Linear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principle-component-analysis"><i class="fa fa-check"></i><b>9.1.1</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="9.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#horeshoe-effect"><i class="fa fa-check"></i><b>9.1.2</b> Horeshoe effect</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-dimensionality-reduction"><i class="fa fa-check"></i><b>9.2</b> Nonlinear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>9.2.1</b> t-SNE</a></li>
<li class="chapter" data-level="9.2.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#gaussian-process-latent-variable-models"><i class="fa fa-check"></i><b>9.2.2</b> Gaussian Process Latent Variable Models</a></li>
<li class="chapter" data-level="9.2.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#gplvms-with-informative-priors"><i class="fa fa-check"></i><b>9.2.3</b> GPLVMs with informative priors</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#exercises-6"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#distance-metrics"><i class="fa fa-check"></i><b>10.2</b> Distance metrics</a><ul>
<li class="chapter" data-level="10.2.1" data-path="clustering.html"><a href="clustering.html#image-segmentation"><i class="fa fa-check"></i><b>10.2.1</b> Image segmentation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative"><i class="fa fa-check"></i><b>10.3</b> Hierarchic agglomerative</a><ul>
<li class="chapter" data-level="10.3.1" data-path="clustering.html"><a href="clustering.html#linkage-algorithms"><i class="fa fa-check"></i><b>10.3.1</b> Linkage algorithms</a></li>
<li class="chapter" data-level="10.3.2" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets"><i class="fa fa-check"></i><b>10.3.2</b> Example: clustering synthetic data sets</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#section"><i class="fa fa-check"></i><b>10.4</b> </a></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#section-1"><i class="fa fa-check"></i><b>10.5</b> ———————</a></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#welcome-to-dendextend-version-1.5.2"><i class="fa fa-check"></i><b>10.6</b> Welcome to dendextend version 1.5.2</a></li>
<li class="chapter" data-level="10.7" data-path="clustering.html"><a href="clustering.html#type-citationdendextend-for-how-to-cite-the-package."><i class="fa fa-check"></i><b>10.7</b> Type citation(‘dendextend’) for how to cite the package.</a></li>
<li class="chapter" data-level="10.8" data-path="clustering.html"><a href="clustering.html#section-2"><i class="fa fa-check"></i><b>10.8</b> </a></li>
<li class="chapter" data-level="10.9" data-path="clustering.html"><a href="clustering.html#type-browsevignettespackage-dendextend-for-the-package-vignette."><i class="fa fa-check"></i><b>10.9</b> Type browseVignettes(package = ‘dendextend’) for the package vignette.</a></li>
<li class="chapter" data-level="10.10" data-path="clustering.html"><a href="clustering.html#the-github-page-is-httpsgithub.comtalgalilidendextend"><i class="fa fa-check"></i><b>10.10</b> The github page is: <a href="https://github.com/talgalili/dendextend/" class="uri">https://github.com/talgalili/dendextend/</a></a></li>
<li class="chapter" data-level="10.11" data-path="clustering.html"><a href="clustering.html#section-3"><i class="fa fa-check"></i><b>10.11</b> </a></li>
<li class="chapter" data-level="10.12" data-path="clustering.html"><a href="clustering.html#suggestions-and-bug-reports-can-be-submitted-at-httpsgithub.comtalgalilidendextendissues"><i class="fa fa-check"></i><b>10.12</b> Suggestions and bug-reports can be submitted at: <a href="https://github.com/talgalili/dendextend/issues" class="uri">https://github.com/talgalili/dendextend/issues</a></a></li>
<li class="chapter" data-level="10.13" data-path="clustering.html"><a href="clustering.html#or-contact-tal.galiligmail.com"><i class="fa fa-check"></i><b>10.13</b> Or contact: <a href="mailto:tal.galili@gmail.com">tal.galili@gmail.com</a></a></li>
<li class="chapter" data-level="10.14" data-path="clustering.html"><a href="clustering.html#section-4"><i class="fa fa-check"></i><b>10.14</b> </a></li>
<li class="chapter" data-level="10.15" data-path="clustering.html"><a href="clustering.html#to-suppress-this-message-use-suppresspackagestartupmessageslibrarydendextend"><i class="fa fa-check"></i><b>10.15</b> To suppress this message use: suppressPackageStartupMessages(library(dendextend))</a></li>
<li class="chapter" data-level="10.16" data-path="clustering.html"><a href="clustering.html#section-5"><i class="fa fa-check"></i><b>10.16</b> ———————</a></li>
<li class="chapter" data-level="10.17" data-path="clustering.html"><a href="clustering.html#section-6"><i class="fa fa-check"></i><b>10.17</b> </a></li>
<li class="chapter" data-level="10.18" data-path="clustering.html"><a href="clustering.html#attaching-package-dendextend"><i class="fa fa-check"></i><b>10.18</b> Attaching package: ‘dendextend’</a></li>
<li class="chapter" data-level="10.19" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packageggdendro"><i class="fa fa-check"></i><b>10.19</b> The following object is masked from ‘package:ggdendro’:</a></li>
<li class="chapter" data-level="10.20" data-path="clustering.html"><a href="clustering.html#section-7"><i class="fa fa-check"></i><b>10.20</b> </a></li>
<li class="chapter" data-level="10.21" data-path="clustering.html"><a href="clustering.html#theme_dendro"><i class="fa fa-check"></i><b>10.21</b> theme_dendro</a></li>
<li class="chapter" data-level="10.22" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packagestats"><i class="fa fa-check"></i><b>10.22</b> The following object is masked from ‘package:stats’:</a></li>
<li class="chapter" data-level="10.23" data-path="clustering.html"><a href="clustering.html#section-8"><i class="fa fa-check"></i><b>10.23</b> </a></li>
<li class="chapter" data-level="10.24" data-path="clustering.html"><a href="clustering.html#cutree"><i class="fa fa-check"></i><b>10.24</b> cutree</a><ul>
<li class="chapter" data-level="10.24.1" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues"><i class="fa fa-check"></i><b>10.24.1</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.25" data-path="clustering.html"><a href="clustering.html#tissue"><i class="fa fa-check"></i><b>10.25</b> tissue</a></li>
<li class="chapter" data-level="10.26" data-path="clustering.html"><a href="clustering.html#cerebellum-colon-endometrium-hippocampus-kidney-liver"><i class="fa fa-check"></i><b>10.26</b> cerebellum colon endometrium hippocampus kidney liver</a></li>
<li class="chapter" data-level="10.27" data-path="clustering.html"><a href="clustering.html#section-9"><i class="fa fa-check"></i><b>10.27</b> 38 34 15 31 39 26</a></li>
<li class="chapter" data-level="10.28" data-path="clustering.html"><a href="clustering.html#placenta"><i class="fa fa-check"></i><b>10.28</b> placenta</a></li>
<li class="chapter" data-level="10.29" data-path="clustering.html"><a href="clustering.html#section-10"><i class="fa fa-check"></i><b>10.29</b> 6</a></li>
<li class="chapter" data-level="10.30" data-path="clustering.html"><a href="clustering.html#section-11"><i class="fa fa-check"></i><b>10.30</b> <a href="clustering.html#section-39">1</a> 22215 189</a></li>
<li class="chapter" data-level="10.31" data-path="clustering.html"><a href="clustering.html#cluster"><i class="fa fa-check"></i><b>10.31</b> cluster</a></li>
<li class="chapter" data-level="10.32" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6"><i class="fa fa-check"></i><b>10.32</b> tissue 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.33" data-path="clustering.html"><a href="clustering.html#cerebellum-0-36-0-0-2-0"><i class="fa fa-check"></i><b>10.33</b> cerebellum 0 36 0 0 2 0</a></li>
<li class="chapter" data-level="10.34" data-path="clustering.html"><a href="clustering.html#colon-0-0-34-0-0-0"><i class="fa fa-check"></i><b>10.34</b> colon 0 0 34 0 0 0</a></li>
<li class="chapter" data-level="10.35" data-path="clustering.html"><a href="clustering.html#endometrium-15-0-0-0-0-0"><i class="fa fa-check"></i><b>10.35</b> endometrium 15 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.36" data-path="clustering.html"><a href="clustering.html#hippocampus-0-31-0-0-0-0"><i class="fa fa-check"></i><b>10.36</b> hippocampus 0 31 0 0 0 0</a></li>
<li class="chapter" data-level="10.37" data-path="clustering.html"><a href="clustering.html#kidney-37-0-0-0-2-0"><i class="fa fa-check"></i><b>10.37</b> kidney 37 0 0 0 2 0</a></li>
<li class="chapter" data-level="10.38" data-path="clustering.html"><a href="clustering.html#liver-0-0-0-24-2-0"><i class="fa fa-check"></i><b>10.38</b> liver 0 0 0 24 2 0</a></li>
<li class="chapter" data-level="10.39" data-path="clustering.html"><a href="clustering.html#placenta-0-0-0-0-0-6"><i class="fa fa-check"></i><b>10.39</b> placenta 0 0 0 0 0 6</a></li>
<li class="chapter" data-level="10.40" data-path="clustering.html"><a href="clustering.html#cluster-1"><i class="fa fa-check"></i><b>10.40</b> cluster</a></li>
<li class="chapter" data-level="10.41" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6-7-8"><i class="fa fa-check"></i><b>10.41</b> tissue 1 2 3 4 5 6 7 8</a></li>
<li class="chapter" data-level="10.42" data-path="clustering.html"><a href="clustering.html#cerebellum-0-31-0-0-2-0-5-0"><i class="fa fa-check"></i><b>10.42</b> cerebellum 0 31 0 0 2 0 5 0</a></li>
<li class="chapter" data-level="10.43" data-path="clustering.html"><a href="clustering.html#colon-0-0-34-0-0-0-0-0"><i class="fa fa-check"></i><b>10.43</b> colon 0 0 34 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.44" data-path="clustering.html"><a href="clustering.html#endometrium-0-0-0-0-0-15-0-0"><i class="fa fa-check"></i><b>10.44</b> endometrium 0 0 0 0 0 15 0 0</a></li>
<li class="chapter" data-level="10.45" data-path="clustering.html"><a href="clustering.html#hippocampus-0-31-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.45</b> hippocampus 0 31 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.46" data-path="clustering.html"><a href="clustering.html#kidney-37-0-0-0-2-0-0-0"><i class="fa fa-check"></i><b>10.46</b> kidney 37 0 0 0 2 0 0 0</a></li>
<li class="chapter" data-level="10.47" data-path="clustering.html"><a href="clustering.html#liver-0-0-0-24-2-0-0-0"><i class="fa fa-check"></i><b>10.47</b> liver 0 0 0 24 2 0 0 0</a></li>
<li class="chapter" data-level="10.48" data-path="clustering.html"><a href="clustering.html#placenta-0-0-0-0-0-0-0-6"><i class="fa fa-check"></i><b>10.48</b> placenta 0 0 0 0 0 0 0 6</a></li>
<li class="chapter" data-level="10.49" data-path="clustering.html"><a href="clustering.html#section-12"><i class="fa fa-check"></i><b>10.49</b> </a></li>
<li class="chapter" data-level="10.50" data-path="clustering.html"><a href="clustering.html#attaching-package-gplots"><i class="fa fa-check"></i><b>10.50</b> Attaching package: ‘gplots’</a></li>
<li class="chapter" data-level="10.51" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packagestats-1"><i class="fa fa-check"></i><b>10.51</b> The following object is masked from ‘package:stats’:</a></li>
<li class="chapter" data-level="10.52" data-path="clustering.html"><a href="clustering.html#section-13"><i class="fa fa-check"></i><b>10.52</b> </a></li>
<li class="chapter" data-level="10.53" data-path="clustering.html"><a href="clustering.html#lowess"><i class="fa fa-check"></i><b>10.53</b> lowess</a></li>
<li class="chapter" data-level="10.54" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.54</b> K-means</a><ul>
<li class="chapter" data-level="10.54.1" data-path="clustering.html"><a href="clustering.html#algorithm-1"><i class="fa fa-check"></i><b>10.54.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.54.2" data-path="clustering.html"><a href="clustering.html#choosing-initial-cluster-centres"><i class="fa fa-check"></i><b>10.54.2</b> Choosing initial cluster centres</a></li>
<li class="chapter" data-level="10.54.3" data-path="clustering.html"><a href="clustering.html#choosingK"><i class="fa fa-check"></i><b>10.54.3</b> Choosing k</a></li>
<li class="chapter" data-level="10.54.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-1"><i class="fa fa-check"></i><b>10.54.4</b> Example: clustering synthetic data sets</a></li>
</ul></li>
<li class="chapter" data-level="10.55" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations"><i class="fa fa-check"></i><b>10.55</b> Warning: did not converge in 10 iterations</a></li>
<li class="chapter" data-level="10.56" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations-1"><i class="fa fa-check"></i><b>10.56</b> Warning: did not converge in 10 iterations</a></li>
<li class="chapter" data-level="10.57" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations-2"><i class="fa fa-check"></i><b>10.57</b> Warning: did not converge in 10 iterations</a></li>
<li class="chapter" data-level="10.58" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations-3"><i class="fa fa-check"></i><b>10.58</b> Warning: did not converge in 10 iterations</a><ul>
<li class="chapter" data-level="10.58.1" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-1"><i class="fa fa-check"></i><b>10.58.1</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.59" data-path="clustering.html"><a href="clustering.html#tissue-1"><i class="fa fa-check"></i><b>10.59</b> tissue</a></li>
<li class="chapter" data-level="10.60" data-path="clustering.html"><a href="clustering.html#cerebellum-colon-endometrium-hippocampus-kidney-liver-1"><i class="fa fa-check"></i><b>10.60</b> cerebellum colon endometrium hippocampus kidney liver</a></li>
<li class="chapter" data-level="10.61" data-path="clustering.html"><a href="clustering.html#section-14"><i class="fa fa-check"></i><b>10.61</b> 38 34 15 31 39 26</a></li>
<li class="chapter" data-level="10.62" data-path="clustering.html"><a href="clustering.html#placenta-1"><i class="fa fa-check"></i><b>10.62</b> placenta</a></li>
<li class="chapter" data-level="10.63" data-path="clustering.html"><a href="clustering.html#section-15"><i class="fa fa-check"></i><b>10.63</b> 6</a></li>
<li class="chapter" data-level="10.64" data-path="clustering.html"><a href="clustering.html#section-16"><i class="fa fa-check"></i><b>10.64</b> <a href="clustering.html#section-39">1</a> 22215 189</a></li>
<li class="chapter" data-level="10.65" data-path="clustering.html"><a href="clustering.html#loading-required-package-foreach"><i class="fa fa-check"></i><b>10.65</b> Loading required package: foreach</a></li>
<li class="chapter" data-level="10.66" data-path="clustering.html"><a href="clustering.html#loading-required-package-iterators"><i class="fa fa-check"></i><b>10.66</b> Loading required package: iterators</a></li>
<li class="chapter" data-level="10.67" data-path="clustering.html"><a href="clustering.html#loading-required-package-parallel"><i class="fa fa-check"></i><b>10.67</b> Loading required package: parallel</a></li>
<li class="chapter" data-level="10.68" data-path="clustering.html"><a href="clustering.html#section-17"><i class="fa fa-check"></i><b>10.68</b> <a href="clustering.html#section-39">1</a> 2</a></li>
<li class="chapter" data-level="10.69" data-path="clustering.html"><a href="clustering.html#section-18"><i class="fa fa-check"></i><b>10.69</b> </a></li>
<li class="chapter" data-level="10.70" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6-7"><i class="fa fa-check"></i><b>10.70</b> tissue 1 2 3 4 5 6 7</a></li>
<li class="chapter" data-level="10.71" data-path="clustering.html"><a href="clustering.html#cerebellum-0-0-0-33-0-0-5"><i class="fa fa-check"></i><b>10.71</b> cerebellum 0 0 0 33 0 0 5</a></li>
<li class="chapter" data-level="10.72" data-path="clustering.html"><a href="clustering.html#colon-0-0-0-0-0-34-0"><i class="fa fa-check"></i><b>10.72</b> colon 0 0 0 0 0 34 0</a></li>
<li class="chapter" data-level="10.73" data-path="clustering.html"><a href="clustering.html#endometrium-0-0-0-0-15-0-0"><i class="fa fa-check"></i><b>10.73</b> endometrium 0 0 0 0 15 0 0</a></li>
<li class="chapter" data-level="10.74" data-path="clustering.html"><a href="clustering.html#hippocampus-0-0-0-0-0-0-31"><i class="fa fa-check"></i><b>10.74</b> hippocampus 0 0 0 0 0 0 31</a></li>
<li class="chapter" data-level="10.75" data-path="clustering.html"><a href="clustering.html#kidney-0-0-39-0-0-0-0"><i class="fa fa-check"></i><b>10.75</b> kidney 0 0 39 0 0 0 0</a></li>
<li class="chapter" data-level="10.76" data-path="clustering.html"><a href="clustering.html#liver-26-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.76</b> liver 26 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.77" data-path="clustering.html"><a href="clustering.html#placenta-0-6-0-0-0-0-0"><i class="fa fa-check"></i><b>10.77</b> placenta 0 6 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.78" data-path="clustering.html"><a href="clustering.html#section-19"><i class="fa fa-check"></i><b>10.78</b> </a></li>
<li class="chapter" data-level="10.79" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6-7-1"><i class="fa fa-check"></i><b>10.79</b> tissue 1 2 3 4 5 6 7</a></li>
<li class="chapter" data-level="10.80" data-path="clustering.html"><a href="clustering.html#cerebellum-0-0-0-0-0-5-33"><i class="fa fa-check"></i><b>10.80</b> cerebellum 0 0 0 0 0 5 33</a></li>
<li class="chapter" data-level="10.81" data-path="clustering.html"><a href="clustering.html#colon-0-0-34-0-0-0-0"><i class="fa fa-check"></i><b>10.81</b> colon 0 0 34 0 0 0 0</a></li>
<li class="chapter" data-level="10.82" data-path="clustering.html"><a href="clustering.html#endometrium-15-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.82</b> endometrium 15 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.83" data-path="clustering.html"><a href="clustering.html#hippocampus-0-0-0-0-31-0-0"><i class="fa fa-check"></i><b>10.83</b> hippocampus 0 0 0 0 31 0 0</a></li>
<li class="chapter" data-level="10.84" data-path="clustering.html"><a href="clustering.html#kidney-37-2-0-0-0-0-0"><i class="fa fa-check"></i><b>10.84</b> kidney 37 2 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.85" data-path="clustering.html"><a href="clustering.html#liver-0-26-0-0-0-0-0"><i class="fa fa-check"></i><b>10.85</b> liver 0 26 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.86" data-path="clustering.html"><a href="clustering.html#placenta-0-0-0-6-0-0-0"><i class="fa fa-check"></i><b>10.86</b> placenta 0 0 0 6 0 0 0</a></li>
<li class="chapter" data-level="10.87" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>10.87</b> DBSCAN</a><ul>
<li class="chapter" data-level="10.87.1" data-path="clustering.html"><a href="clustering.html#algorithm-2"><i class="fa fa-check"></i><b>10.87.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.87.2" data-path="clustering.html"><a href="clustering.html#implementation-in-r"><i class="fa fa-check"></i><b>10.87.2</b> Implementation in R</a></li>
<li class="chapter" data-level="10.87.3" data-path="clustering.html"><a href="clustering.html#choosing-parameters"><i class="fa fa-check"></i><b>10.87.3</b> Choosing parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.88" data-path="clustering.html"><a href="clustering.html#section-20"><i class="fa fa-check"></i><b>10.88</b> </a></li>
<li class="chapter" data-level="10.89" data-path="clustering.html"><a href="clustering.html#section-21"><i class="fa fa-check"></i><b>10.89</b> 0 1 2 3</a></li>
<li class="chapter" data-level="10.90" data-path="clustering.html"><a href="clustering.html#section-22"><i class="fa fa-check"></i><b>10.90</b> 43 484 486 487</a><ul>
<li class="chapter" data-level="10.90.1" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-2"><i class="fa fa-check"></i><b>10.90.1</b> Example: clustering synthetic data sets</a></li>
</ul></li>
<li class="chapter" data-level="10.91" data-path="clustering.html"><a href="clustering.html#section-23"><i class="fa fa-check"></i><b>10.91</b> </a></li>
<li class="chapter" data-level="10.92" data-path="clustering.html"><a href="clustering.html#section-24"><i class="fa fa-check"></i><b>10.92</b> 0 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.93" data-path="clustering.html"><a href="clustering.html#section-25"><i class="fa fa-check"></i><b>10.93</b> 2 168 307 105 127 45 34</a></li>
<li class="chapter" data-level="10.94" data-path="clustering.html"><a href="clustering.html#section-26"><i class="fa fa-check"></i><b>10.94</b> </a></li>
<li class="chapter" data-level="10.95" data-path="clustering.html"><a href="clustering.html#section-27"><i class="fa fa-check"></i><b>10.95</b> 0 1 2</a></li>
<li class="chapter" data-level="10.96" data-path="clustering.html"><a href="clustering.html#section-28"><i class="fa fa-check"></i><b>10.96</b> 8 748 744</a></li>
<li class="chapter" data-level="10.97" data-path="clustering.html"><a href="clustering.html#section-29"><i class="fa fa-check"></i><b>10.97</b> </a></li>
<li class="chapter" data-level="10.98" data-path="clustering.html"><a href="clustering.html#section-30"><i class="fa fa-check"></i><b>10.98</b> 0 1</a></li>
<li class="chapter" data-level="10.99" data-path="clustering.html"><a href="clustering.html#section-31"><i class="fa fa-check"></i><b>10.99</b> 40 1460</a></li>
<li class="chapter" data-level="10.100" data-path="clustering.html"><a href="clustering.html#section-32"><i class="fa fa-check"></i><b>10.100</b> </a></li>
<li class="chapter" data-level="10.101" data-path="clustering.html"><a href="clustering.html#section-33"><i class="fa fa-check"></i><b>10.101</b> 0 1 2</a></li>
<li class="chapter" data-level="10.102" data-path="clustering.html"><a href="clustering.html#section-34"><i class="fa fa-check"></i><b>10.102</b> 109 399 992</a></li>
<li class="chapter" data-level="10.103" data-path="clustering.html"><a href="clustering.html#section-35"><i class="fa fa-check"></i><b>10.103</b> </a></li>
<li class="chapter" data-level="10.104" data-path="clustering.html"><a href="clustering.html#section-36"><i class="fa fa-check"></i><b>10.104</b> 0 1 2 3</a></li>
<li class="chapter" data-level="10.105" data-path="clustering.html"><a href="clustering.html#section-37"><i class="fa fa-check"></i><b>10.105</b> 29 489 488 494</a></li>
<li class="chapter" data-level="10.106" data-path="clustering.html"><a href="clustering.html#section-38"><i class="fa fa-check"></i><b>10.106</b> </a></li>
<li class="chapter" data-level="10.107" data-path="clustering.html"><a href="clustering.html#section-39"><i class="fa fa-check"></i><b>10.107</b> 1</a></li>
<li class="chapter" data-level="10.108" data-path="clustering.html"><a href="clustering.html#section-40"><i class="fa fa-check"></i><b>10.108</b> 1500</a><ul>
<li class="chapter" data-level="10.108.1" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-2"><i class="fa fa-check"></i><b>10.108.1</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.109" data-path="clustering.html"><a href="clustering.html#tissue-2"><i class="fa fa-check"></i><b>10.109</b> tissue</a></li>
<li class="chapter" data-level="10.110" data-path="clustering.html"><a href="clustering.html#cerebellum-colon-endometrium-hippocampus-kidney-liver-2"><i class="fa fa-check"></i><b>10.110</b> cerebellum colon endometrium hippocampus kidney liver</a></li>
<li class="chapter" data-level="10.111" data-path="clustering.html"><a href="clustering.html#section-41"><i class="fa fa-check"></i><b>10.111</b> 38 34 15 31 39 26</a></li>
<li class="chapter" data-level="10.112" data-path="clustering.html"><a href="clustering.html#placenta-2"><i class="fa fa-check"></i><b>10.112</b> placenta</a></li>
<li class="chapter" data-level="10.113" data-path="clustering.html"><a href="clustering.html#section-42"><i class="fa fa-check"></i><b>10.113</b> 6</a></li>
<li class="chapter" data-level="10.114" data-path="clustering.html"><a href="clustering.html#section-43"><i class="fa fa-check"></i><b>10.114</b> </a></li>
<li class="chapter" data-level="10.115" data-path="clustering.html"><a href="clustering.html#section-44"><i class="fa fa-check"></i><b>10.115</b> 0 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.116" data-path="clustering.html"><a href="clustering.html#section-45"><i class="fa fa-check"></i><b>10.116</b> 12 37 62 34 24 15 5</a></li>
<li class="chapter" data-level="10.117" data-path="clustering.html"><a href="clustering.html#section-46"><i class="fa fa-check"></i><b>10.117</b> </a></li>
<li class="chapter" data-level="10.118" data-path="clustering.html"><a href="clustering.html#tissue-0-1-2-3-4-5-6"><i class="fa fa-check"></i><b>10.118</b> tissue 0 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.119" data-path="clustering.html"><a href="clustering.html#cerebellum-2-0-31-0-0-0-5"><i class="fa fa-check"></i><b>10.119</b> cerebellum 2 0 31 0 0 0 5</a></li>
<li class="chapter" data-level="10.120" data-path="clustering.html"><a href="clustering.html#colon-0-0-0-34-0-0-0"><i class="fa fa-check"></i><b>10.120</b> colon 0 0 0 34 0 0 0</a></li>
<li class="chapter" data-level="10.121" data-path="clustering.html"><a href="clustering.html#endometrium-0-0-0-0-0-15-0"><i class="fa fa-check"></i><b>10.121</b> endometrium 0 0 0 0 0 15 0</a></li>
<li class="chapter" data-level="10.122" data-path="clustering.html"><a href="clustering.html#hippocampus-0-0-31-0-0-0-0"><i class="fa fa-check"></i><b>10.122</b> hippocampus 0 0 31 0 0 0 0</a></li>
<li class="chapter" data-level="10.123" data-path="clustering.html"><a href="clustering.html#kidney-2-37-0-0-0-0-0"><i class="fa fa-check"></i><b>10.123</b> kidney 2 37 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.124" data-path="clustering.html"><a href="clustering.html#liver-2-0-0-0-24-0-0"><i class="fa fa-check"></i><b>10.124</b> liver 2 0 0 0 24 0 0</a></li>
<li class="chapter" data-level="10.125" data-path="clustering.html"><a href="clustering.html#placenta-6-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.125</b> placenta 6 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.126" data-path="clustering.html"><a href="clustering.html#evaluating-cluster-quality"><i class="fa fa-check"></i><b>10.126</b> Evaluating cluster quality</a><ul>
<li class="chapter" data-level="10.126.1" data-path="clustering.html"><a href="clustering.html#silhouetteMethod"><i class="fa fa-check"></i><b>10.126.1</b> Silhouette method</a></li>
<li class="chapter" data-level="10.126.2" data-path="clustering.html"><a href="clustering.html#example---k-means-clustering-of-blobs-data-set"><i class="fa fa-check"></i><b>10.126.2</b> Example - k-means clustering of blobs data set</a></li>
</ul></li>
<li class="chapter" data-level="10.127" data-path="clustering.html"><a href="clustering.html#summary"><i class="fa fa-check"></i><b>10.127</b> Summary</a><ul>
<li class="chapter" data-level="10.127.1" data-path="clustering.html"><a href="clustering.html#applications"><i class="fa fa-check"></i><b>10.127.1</b> Applications</a></li>
<li class="chapter" data-level="10.127.2" data-path="clustering.html"><a href="clustering.html#strengths"><i class="fa fa-check"></i><b>10.127.2</b> Strengths</a></li>
<li class="chapter" data-level="10.127.3" data-path="clustering.html"><a href="clustering.html#limitations"><i class="fa fa-check"></i><b>10.127.3</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="10.128" data-path="clustering.html"><a href="clustering.html#exercises-7"><i class="fa fa-check"></i><b>10.128</b> Exercises</a><ul>
<li class="chapter" data-level="10.128.1" data-path="clustering.html"><a href="clustering.html#clusteringEx1"><i class="fa fa-check"></i><b>10.128.1</b> Exercise 1</a></li>
<li class="chapter" data-level="10.128.2" data-path="clustering.html"><a href="clustering.html#clusteringEx2"><i class="fa fa-check"></i><b>10.128.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="10.129" data-path="clustering.html"><a href="clustering.html#section-47"><i class="fa fa-check"></i><b>10.129</b> </a></li>
<li class="chapter" data-level="10.130" data-path="clustering.html"><a href="clustering.html#attaching-package-ebimage"><i class="fa fa-check"></i><b>10.130</b> Attaching package: ‘EBImage’</a></li>
<li class="chapter" data-level="10.131" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packagedendextend"><i class="fa fa-check"></i><b>10.131</b> The following object is masked from ‘package:dendextend’:</a></li>
<li class="chapter" data-level="10.132" data-path="clustering.html"><a href="clustering.html#section-48"><i class="fa fa-check"></i><b>10.132</b> </a></li>
<li class="chapter" data-level="10.133" data-path="clustering.html"><a href="clustering.html#rotate"><i class="fa fa-check"></i><b>10.133</b> rotate</a></li>
<li class="chapter" data-level="10.134" data-path="clustering.html"><a href="clustering.html#section-49"><i class="fa fa-check"></i><b>10.134</b> <a href="clustering.html#section-39">1</a> 528 393 3</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span><ul>
<li class="chapter" data-level="A.135" data-path="04-nearest-neighbours.html"><a href="#python"><i class="fa fa-check"></i><b>A.135</b> Python</a></li>
<li class="chapter" data-level="A.136" data-path="clustering.html"><a href="clustering.html#machine-learning-data-set-repositories"><i class="fa fa-check"></i><b>A.136</b> Machine learning data set repositories</a><ul>
<li class="chapter" data-level="A.136.1" data-path="clustering.html"><a href="clustering.html#mldata"><i class="fa fa-check"></i><b>A.136.1</b> MLDATA</a></li>
<li class="chapter" data-level="A.136.2" data-path="clustering.html"><a href="clustering.html#uci-machine-learning-repository"><i class="fa fa-check"></i><b>A.136.2</b> UCI Machine Learning Repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html"><i class="fa fa-check"></i><b>B</b> Solutions ch. 3 - Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="B.1" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#exercise-1"><i class="fa fa-check"></i><b>B.1</b> Exercise 1</a></li>
<li class="chapter" data-level="B.2" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#exercise-2"><i class="fa fa-check"></i><b>B.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html"><i class="fa fa-check"></i><b>C</b> Solutions ch. 4 - Linear and non-linear logistic regression</a><ul>
<li class="chapter" data-level="C.1" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html#exercise-1-1"><i class="fa fa-check"></i><b>C.1</b> Exercise 1</a></li>
<li class="chapter" data-level="C.2" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html#exercise-2-1"><i class="fa fa-check"></i><b>C.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html"><i class="fa fa-check"></i><b>D</b> Solutions ch. 5 - Nearest neighbours</a><ul>
<li class="chapter" data-level="D.1" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-1-2"><i class="fa fa-check"></i><b>D.1</b> Exercise 1</a></li>
<li class="chapter" data-level="D.2" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-2-2"><i class="fa fa-check"></i><b>D.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html"><i class="fa fa-check"></i><b>E</b> Solutions ch. 6 - Decision trees and random forests</a><ul>
<li class="chapter" data-level="E.1" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-1-3"><i class="fa fa-check"></i><b>E.1</b> Exercise 1</a></li>
<li class="chapter" data-level="E.2" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-2-3"><i class="fa fa-check"></i><b>E.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="solutions-svm.html"><a href="solutions-svm.html"><i class="fa fa-check"></i><b>F</b> Solutions ch. 7 - Support vector machines</a><ul>
<li class="chapter" data-level="F.1" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-1-4"><i class="fa fa-check"></i><b>F.1</b> Exercise 1</a></li>
<li class="chapter" data-level="F.2" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-2-4"><i class="fa fa-check"></i><b>F.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="solutions-ann.html"><a href="solutions-ann.html"><i class="fa fa-check"></i><b>G</b> Solutions ch. 8 - Artificial neural networks</a><ul>
<li class="chapter" data-level="G.1" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-1-5"><i class="fa fa-check"></i><b>G.1</b> Exercise 1</a></li>
<li class="chapter" data-level="G.2" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-2-5"><i class="fa fa-check"></i><b>G.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="H" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html"><i class="fa fa-check"></i><b>H</b> Solutions ch. 9 - Dimensionality reduction</a><ul>
<li class="chapter" data-level="H.1" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-1-6"><i class="fa fa-check"></i><b>H.1</b> Exercise 1</a></li>
<li class="chapter" data-level="H.2" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-2-6"><i class="fa fa-check"></i><b>H.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="I" data-path="solutions-clustering.html"><a href="solutions-clustering.html"><i class="fa fa-check"></i><b>I</b> Solutions ch. 10 - Clustering</a><ul>
<li class="chapter" data-level="I.1" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-1-7"><i class="fa fa-check"></i><b>I.1</b> Exercise 1</a></li>
<li class="chapter" data-level="I.2" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-2-7"><i class="fa fa-check"></i><b>I.2</b> Exercise 2</a></li>
<li class="chapter" data-level="I.3" data-path="solutions-clustering.html"><a href="solutions-clustering.html#loading-required-package-foreach-1"><i class="fa fa-check"></i><b>I.3</b> Loading required package: foreach</a></li>
<li class="chapter" data-level="I.4" data-path="solutions-clustering.html"><a href="solutions-clustering.html#loading-required-package-iterators-1"><i class="fa fa-check"></i><b>I.4</b> Loading required package: iterators</a></li>
<li class="chapter" data-level="I.5" data-path="solutions-clustering.html"><a href="solutions-clustering.html#loading-required-package-parallel-1"><i class="fa fa-check"></i><b>I.5</b> Loading required package: parallel</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nearest-neighbours" class="section level1">
<h1><span class="header-section-number">5</span> Nearest neighbours</h1>
<!-- Matt -->
<!-- 
Get ideas on presentation from Harvard bioinformatics website. In particular, use of dataset with two variables (crabs??), because easier to display. Performance of classifier as k increases (should initially improve and then get worse - starts to lose flexibility).

In exercises could introduce application of knn to regression.

GENERAL:
SPLOM for displaying datasets with small number of variables

FEATURE SELECTION
filter methods  /  wrapper methods / genetic algorithms

Refer to scikit learn

FEATURE SCALING

BIAS-VARIANCE TRADEOFF
In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set[citation needed].:

    The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
    The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).


-->
<div id="introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>memory based and require no model to be fit</p>
<p>classification and non-linear regression</p>
<p>bias and variance</p>
<p>computational load - finding neighbours and storing the entire training set</p>
<p>k-d tree / linear search</p>
<p>system.time k-d tree search vs linear search</p>
<p>library(class)</p>
<p>class::knn</p>
<p>importance of centering a scaling</p>
<p>increase in neighbours - increase in ties</p>
<div id="measuring-distance-between-objects" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Measuring distance between objects</h3>
<strong>Euclidean distance:</strong>
<span class="math display" id="eq:euclidean">\[\begin{equation}
  distance\left(p,q\right)=\sqrt{\sum_{i=1}^{n} (p_i-q_i)^2}
  \tag{5.1}
\end{equation}\]</span>
<div class="figure" style="text-align: center"><span id="fig:euclideanDistanceDiagram"></span>
<img src="04-nearest-neighbours_files/figure-html/euclideanDistanceDiagram-1.png" alt="Euclidean distance." width="75%" />
<p class="caption">
Figure 5.1: Euclidean distance.
</p>
</div>
</div>
</div>
<div id="classification" class="section level2">
<h2><span class="header-section-number">5.2</span> Classification</h2>
<div id="algorithm" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Algorithm</h3>
<div class="figure" style="text-align: center"><span id="fig:knnClassification"></span>
<img src="images/knn_classification.svg" alt="Illustration of _k_-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class.  By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282" width="75%" />
<p class="caption">
Figure 5.2: Illustration of <em>k</em>-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class. By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, <a href="https://commons.wikimedia.org/w/index.php?curid=2170282" class="uri">https://commons.wikimedia.org/w/index.php?curid=2170282</a>
</p>
</div>
</div>
<div id="simulated-data" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Simulated data</h3>
<p>We will use a simulated data set to demonstrate:</p>
<ul>
<li>bias-variance trade-off</li>
<li>the knn function in R</li>
<li>plotting decision boundaries</li>
<li>choosing the optimum value of <em>k</em></li>
</ul>
<p>The dataset is partitioned into training and test sets.</p>
<p>Load data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;data/example_binary_classification/bin_class_example.rda&quot;</span>)
<span class="kw">str</span>(xtrain)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  -0.223 0.944 2.36 1.846 1.732 ...
##  $ V2: num  -1.153 -0.827 -0.128 2.014 -0.574 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(xtest)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  2.09 2.3 2.07 1.65 1.18 ...
##  $ V2: num  -1.009 1.0947 0.1644 0.3243 -0.0277 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytrain))</code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytest))</code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(GGally)
<span class="kw">library</span>(RColorBrewer)
point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)
point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)
point_size =<span class="st"> </span><span class="dv">2</span>

<span class="kw">ggplot</span>(xtrain, <span class="kw">aes</span>(V1,V2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytrain<span class="dv">+1</span>], <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], 
             <span class="dt">size=</span>point_size) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(xtest, <span class="kw">aes</span>(V1,V2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytest<span class="dv">+1</span>], <span class="dt">shape=</span>point_shapes[ytest<span class="dv">+1</span>], 
             <span class="dt">size=</span>point_size) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simDataBinClassTrainTest"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-1.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-2.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" />
<p class="caption">
Figure 5.3: Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using <em>k</em>-nn
</p>
</div>
</div>
<div id="knn-function" class="section level3">
<h3><span class="header-section-number">5.2.3</span> knn function</h3>
<p>For <em>k</em>-nn classification and regression we will use the <strong>knn</strong> function in the package <strong>class</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)</code></pre></div>
<p><strong>Arguments to knn</strong></p>
<ul>
<li><code>train</code> : matrix or data frame of training set cases.</li>
<li><code>test</code> : matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.</li>
<li><code>cl</code> : factor of true classifications of training set</li>
<li><code>k</code> : number of neighbours considered.</li>
<li><code>l</code> : minimum vote for definite decision, otherwise doubt. (More precisely, less than k-l dissenting votes are allowed, even if k is increased by ties.)</li>
<li><code>prob</code> : If this is true, the proportion of the votes for the winning class are returned as attribute prob.</li>
<li><code>use.all</code> : controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours.</li>
</ul>
<p>Perform <em>k</em>-nn on the training set with <em>k</em>=1</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1train &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtrain, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
<span class="kw">table</span>(ytrain,knn1train)</code></pre></div>
<pre><code>##       knn1train
## ytrain   0   1
##      0 200   0
##      1   0 200</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">&quot;KNN prediction error for training set: &quot;</span>, <span class="dv">1</span>-<span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1train))==ytrain), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<pre><code>## KNN prediction error for training set:  0</code></pre>
<p>Test data set</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1test &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtest, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
<span class="kw">table</span>(ytest, knn1test)</code></pre></div>
<pre><code>##      knn1test
## ytest   0   1
##     0 131  69
##     1  81 119</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">cat</span>(<span class="st">&quot;KNN prediction error for test set: &quot;</span>, <span class="dv">1</span>-<span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1test))==ytest), <span class="st">&quot;</span><span class="ch">\n</span><span class="st">&quot;</span>)</code></pre></div>
<pre><code>## KNN prediction error for test set:  0.375</code></pre>
</div>
<div id="plotting-decision-boundaries" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Plotting decision boundaries</h3>
<p>Since we have just two dimensions we can visualize the decision boundary generated by the <em>k</em>-nn classifier in a 2D scatterplot. Situations where your original data set contains only two variables will be rare, but it is not unusual to reduce a high-dimensional data set to just two dimensions using the methods that will be discussed in chapter <a href="dimensionality-reduction.html#dimensionality-reduction">9</a>. Therefore, knowing how to plot decision boundaries will potentially be helpful for many different datasets and classifiers.</p>
<p>Create a grid so we can predict across the full range of our variables V1 and V2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gridSize &lt;-<span class="st"> </span><span class="dv">150</span> 
v1limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])))
tmpV1 &lt;-<span class="st"> </span><span class="kw">seq</span>(v1limits[<span class="dv">1</span>],v1limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
v2limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])))
tmpV2 &lt;-<span class="st"> </span><span class="kw">seq</span>(v2limits[<span class="dv">1</span>],v2limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(tmpV1,tmpV2)
<span class="kw">names</span>(xgrid) &lt;-<span class="st"> </span><span class="kw">names</span>(xtrain)</code></pre></div>
<p>Predict values of all elements of grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1grid &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xgrid, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
V3 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1grid))
xgrid &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgrid, V3)</code></pre></div>
<p>Plot</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)
point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)
point_size =<span class="st"> </span><span class="dv">2</span>
<span class="co"># grid point 16</span>
<span class="co"># grid point size =0.2</span>

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtrain, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytrain<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtest, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytest<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simDataBinClassDecisionBoundaryK1"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-1.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-2.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" />
<p class="caption">
Figure 5.4: Binary classification of the simulated training and test sets with <em>k</em>=1.
</p>
</div>
</div>
<div id="bias-variance-tradeoff" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Bias-variance tradeoff</h3>
<p>The bias–variance tradeoff is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:</p>
<ul>
<li>The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li>
<li>The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).</li>
</ul>
<p>Logarithmic spaced sequence function from <a href="https://cran.r-project.org/package=emdbook">emdbook</a> package.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lseq &lt;-<span class="st"> </span>function(from, to, length.out) {
  <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="kw">log</span>(from), <span class="kw">log</span>(to), <span class="dt">length.out =</span> length.out))
}</code></pre></div>
<p>Get log spaced sequence of length 20, round and then remove any duplicates resulting from rounding.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">round</span>(<span class="kw">lseq</span>(<span class="dv">1</span>,<span class="dv">200</span>,<span class="dv">20</span>)))</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_error &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, function(i){
  yhat &lt;-<span class="st"> </span><span class="kw">knn</span>(xtrain, xtrain, ytrain, i)
  <span class="kw">return</span>(<span class="dv">1</span>-<span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(yhat))==ytrain))
})

test_error &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, function(i){
  yhat &lt;-<span class="st"> </span><span class="kw">knn</span>(xtrain, xtest, ytrain, i)
  <span class="kw">return</span>(<span class="dv">1</span>-<span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(yhat))==ytest))
})

k &lt;-<span class="st"> </span><span class="kw">rep</span>(s, <span class="dv">2</span>)
set &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;train&quot;</span>, <span class="kw">length</span>(s)), <span class="kw">rep</span>(<span class="st">&quot;test&quot;</span>, <span class="kw">length</span>(s)))
error &lt;-<span class="st"> </span><span class="kw">c</span>(train_error, test_error)
misclass_errors &lt;-<span class="st"> </span><span class="kw">data.frame</span>(k, set, error)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(misclass_errors, <span class="kw">aes</span>(<span class="dt">x=</span>k, <span class="dt">y=</span>error, <span class="dt">group=</span>set)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">colour=</span>set, <span class="dt">linetype=</span>set), <span class="dt">size=</span><span class="fl">1.5</span>) +
<span class="st">  </span><span class="kw">scale_x_log10</span>() +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Misclassification Errors&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>), <span class="dt">legend.title=</span><span class="kw">element_blank</span>(),
        <span class="dt">legend.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">12</span>), 
        <span class="dt">axis.title.x=</span><span class="kw">element_text</span>(<span class="dt">face=</span><span class="st">&quot;italic&quot;</span>, <span class="dt">size=</span><span class="dv">12</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:misclassErrorsFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/misclassErrorsFunK-1.png" alt="Misclassification errors as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.5: Misclassification errors as a function of neighbourhood size.
</p>
</div>
</div>
<div id="choosing-k" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Choosing <em>k</em></h3>
<p>We will use the caret library.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)</code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<p><a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> has parallel processing built in. To take advantage of this feature we simply need to load the <a href="http://cran.r-project.org/web/packages/doMC/index.html">doMC</a> package and register workers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(doMC)</code></pre></div>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">registerDoMC</span>()</code></pre></div>
<p>To find out how many cores we have registered we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">getDoParWorkers</span>()</code></pre></div>
<pre><code>## [1] 2</code></pre>
<strong>Cohen’s Kappa:</strong>
<span class="math display" id="eq:kappa">\[\begin{equation}
  Kappa = \frac{O-E}{1-E}
  \tag{5.2}
\end{equation}\]</span>
<p>where <em>O</em> is the observed accuracy and <em>E</em> is the expected accuracy based on the marginal totals of the confusion matrix. Cohen’s Kappa takes values between -1 and 1; a value of zero indicates no agreement between the observed and predicted classes, while a value of one shows perfect concordance of the model prediction and the observed classes. If the prediction is in the opposite direction of the truth, a negative value will be obtained, but large negative values are rare in practice <span class="citation">(Kuhn and Johnson <a href="#ref-Kuhn2013">2013</a>)</span>.</p>
</div>
<div id="feature-selection" class="section level3">
<h3><span class="header-section-number">5.2.7</span> Feature selection</h3>
<p>Error training vs cv vs test</p>
</div>
</div>
<div id="regression" class="section level2">
<h2><span class="header-section-number">5.3</span> Regression</h2>
<!--
## Caret

pre-processing
identification of correlated predictors


Parallel processing with doMC
registerDoMC()
getDoParWorkers()

## Curse of dimensionality
Pre-processing data using dimensionality reduction.

transformation functionality in caret

## Examples

centre1 <- read.csv("data/serum_proteomics/male_centre1.csv")
centre2 <- read.csv("data/serum_proteomics/male_centre2.csv")

c1sub <- centre1[,c(1,5,6,9,10)]
c2sub <- centre2[,c(1,5,6,9,10)]

res <- FNN::knn(c1sub[,2:5], c1sub[,2:5], cl=c1sub$Diagnostic_group, k=1)
table(c1sub$Diagnostic_group, res)

res <- FNN::knn(c1sub[,2:5], c2sub[,2:5], cl=c1sub$Diagnostic_group, k=1)
table(c2sub$Diagnostic_group, res)

bias / variance trade-off

include:
division into training and test set
preprocessing - illustrate with diagram

-->
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">5.4</span> Exercises</h2>
<div id="knnEx1" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Exercise 1</h3>
<p>Classification</p>
<p>Try different methods of feature selection</p>
</div>
<div id="knnEx2" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Exercise 2</h3>
<p>Regression</p>
<p>Alzheimers &amp; gene expression? MMSE and gene expression?</p>
<p>Solutions to exercises can be found in appendix <a href="solutions-nearest-neighbours.html#solutions-nearest-neighbours">D</a>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Kuhn2013">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. New York: Springer.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decision-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bioinformatics-training/intro-machine-learning/edit/master/04-nearest-neighbours.Rmd",
"text": "Edit"
},
"download": ["intro-machine-learning.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
