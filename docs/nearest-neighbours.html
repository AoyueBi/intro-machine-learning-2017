<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta name="description" content="Course materials for An Introduction to Machine Learning">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="figures/cover_image.png" />
  <meta property="og:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="github-repo" content="bioinformatics-training/intro-machine-learning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="twitter:image" content="figures/cover_image.png" />

<meta name="author" content="Sudhakaran Prabakaran, Matt Wayland and Chris Penfold">


<meta name="date" content="2017-09-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="logistic-regression.html">
<link rel="next" href="decision-trees.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Introduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About the course</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#registration"><i class="fa fa-check"></i><b>1.2</b> Registration</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4</b> Github</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.5</b> License</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6</b> Contact</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.7</b> Colophon</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>3</b> Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-models.html"><a href="linear-models.html#exercises"><i class="fa fa-check"></i><b>3.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Linear and non linear logistic regression</a><ul>
<li class="chapter" data-level="4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#exercises-1"><i class="fa fa-check"></i><b>4.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html"><i class="fa fa-check"></i><b>5</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a><ul>
<li class="chapter" data-level="5.1.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#measuring-distance-between-objects"><i class="fa fa-check"></i><b>5.1.1</b> Measuring distance between objects</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#classification"><i class="fa fa-check"></i><b>5.2</b> Classification</a><ul>
<li class="chapter" data-level="5.2.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#algorithm"><i class="fa fa-check"></i><b>5.2.1</b> Algorithm</a></li>
<li class="chapter" data-level="5.2.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#simulated-data"><i class="fa fa-check"></i><b>5.2.2</b> Simulated data</a></li>
<li class="chapter" data-level="5.2.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knn-function"><i class="fa fa-check"></i><b>5.2.3</b> knn function</a></li>
<li class="chapter" data-level="5.2.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#plotting-decision-boundaries"><i class="fa fa-check"></i><b>5.2.4</b> Plotting decision boundaries</a></li>
<li class="chapter" data-level="5.2.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#bias-variance-tradeoff"><i class="fa fa-check"></i><b>5.2.5</b> Bias-variance tradeoff</a></li>
<li class="chapter" data-level="5.2.6" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#choosing-k"><i class="fa fa-check"></i><b>5.2.6</b> Choosing <em>k</em></a></li>
<li class="chapter" data-level="5.2.7" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#data-pre-processing"><i class="fa fa-check"></i><b>5.2.7</b> Data pre-processing</a></li>
<li class="chapter" data-level="5.2.8" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#feature-selection"><i class="fa fa-check"></i><b>5.2.8</b> Feature selection</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#regression"><i class="fa fa-check"></i><b>5.3</b> Regression</a></li>
<li class="chapter" data-level="5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#exercises-2"><i class="fa fa-check"></i><b>5.4</b> Exercises</a><ul>
<li class="chapter" data-level="5.4.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx1"><i class="fa fa-check"></i><b>5.4.1</b> Exercise 1</a></li>
<li class="chapter" data-level="5.4.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx2"><i class="fa fa-check"></i><b>5.4.2</b> Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>6</b> Decision trees and random forests</a><ul>
<li class="chapter" data-level="6.1" data-path="decision-trees.html"><a href="decision-trees.html#exercises-3"><i class="fa fa-check"></i><b>6.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>7</b> Support vector machines</a><ul>
<li class="chapter" data-level="7.1" data-path="svm.html"><a href="svm.html#exercises-4"><i class="fa fa-check"></i><b>7.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8</b> Artificial neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html#exercises-5"><i class="fa fa-check"></i><b>8.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#dimensionality-reduction"><i class="fa fa-check"></i><b>9</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="9.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>9.1</b> Linear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principle-component-analysis"><i class="fa fa-check"></i><b>9.1.1</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="9.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#horeshoe-effect"><i class="fa fa-check"></i><b>9.1.2</b> Horeshoe effect</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-dimensionality-reduction"><i class="fa fa-check"></i><b>9.2</b> Nonlinear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>9.2.1</b> t-SNE</a></li>
<li class="chapter" data-level="9.2.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#gaussian-process-latent-variable-models"><i class="fa fa-check"></i><b>9.2.2</b> Gaussian Process Latent Variable Models</a></li>
<li class="chapter" data-level="9.2.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#gplvms-with-informative-priors"><i class="fa fa-check"></i><b>9.2.3</b> GPLVMs with informative priors</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#exercises-6"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#distance-metrics"><i class="fa fa-check"></i><b>10.2</b> Distance metrics</a><ul>
<li class="chapter" data-level="10.2.1" data-path="clustering.html"><a href="clustering.html#image-segmentation"><i class="fa fa-check"></i><b>10.2.1</b> Image segmentation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative"><i class="fa fa-check"></i><b>10.3</b> Hierarchic agglomerative</a><ul>
<li class="chapter" data-level="10.3.1" data-path="clustering.html"><a href="clustering.html#linkage-algorithms"><i class="fa fa-check"></i><b>10.3.1</b> Linkage algorithms</a></li>
<li class="chapter" data-level="10.3.2" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets"><i class="fa fa-check"></i><b>10.3.2</b> Example: clustering synthetic data sets</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#section"><i class="fa fa-check"></i><b>10.4</b> </a></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#section-1"><i class="fa fa-check"></i><b>10.5</b> ———————</a></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#welcome-to-dendextend-version-1.5.2"><i class="fa fa-check"></i><b>10.6</b> Welcome to dendextend version 1.5.2</a></li>
<li class="chapter" data-level="10.7" data-path="clustering.html"><a href="clustering.html#type-citationdendextend-for-how-to-cite-the-package."><i class="fa fa-check"></i><b>10.7</b> Type citation(‘dendextend’) for how to cite the package.</a></li>
<li class="chapter" data-level="10.8" data-path="clustering.html"><a href="clustering.html#section-2"><i class="fa fa-check"></i><b>10.8</b> </a></li>
<li class="chapter" data-level="10.9" data-path="clustering.html"><a href="clustering.html#type-browsevignettespackage-dendextend-for-the-package-vignette."><i class="fa fa-check"></i><b>10.9</b> Type browseVignettes(package = ‘dendextend’) for the package vignette.</a></li>
<li class="chapter" data-level="10.10" data-path="clustering.html"><a href="clustering.html#the-github-page-is-httpsgithub.comtalgalilidendextend"><i class="fa fa-check"></i><b>10.10</b> The github page is: <a href="https://github.com/talgalili/dendextend/" class="uri">https://github.com/talgalili/dendextend/</a></a></li>
<li class="chapter" data-level="10.11" data-path="clustering.html"><a href="clustering.html#section-3"><i class="fa fa-check"></i><b>10.11</b> </a></li>
<li class="chapter" data-level="10.12" data-path="clustering.html"><a href="clustering.html#suggestions-and-bug-reports-can-be-submitted-at-httpsgithub.comtalgalilidendextendissues"><i class="fa fa-check"></i><b>10.12</b> Suggestions and bug-reports can be submitted at: <a href="https://github.com/talgalili/dendextend/issues" class="uri">https://github.com/talgalili/dendextend/issues</a></a></li>
<li class="chapter" data-level="10.13" data-path="clustering.html"><a href="clustering.html#or-contact-tal.galiligmail.com"><i class="fa fa-check"></i><b>10.13</b> Or contact: <a href="mailto:tal.galili@gmail.com">tal.galili@gmail.com</a></a></li>
<li class="chapter" data-level="10.14" data-path="clustering.html"><a href="clustering.html#section-4"><i class="fa fa-check"></i><b>10.14</b> </a></li>
<li class="chapter" data-level="10.15" data-path="clustering.html"><a href="clustering.html#to-suppress-this-message-use-suppresspackagestartupmessageslibrarydendextend"><i class="fa fa-check"></i><b>10.15</b> To suppress this message use: suppressPackageStartupMessages(library(dendextend))</a></li>
<li class="chapter" data-level="10.16" data-path="clustering.html"><a href="clustering.html#section-5"><i class="fa fa-check"></i><b>10.16</b> ———————</a></li>
<li class="chapter" data-level="10.17" data-path="clustering.html"><a href="clustering.html#section-6"><i class="fa fa-check"></i><b>10.17</b> </a></li>
<li class="chapter" data-level="10.18" data-path="clustering.html"><a href="clustering.html#attaching-package-dendextend"><i class="fa fa-check"></i><b>10.18</b> Attaching package: ‘dendextend’</a></li>
<li class="chapter" data-level="10.19" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packageggdendro"><i class="fa fa-check"></i><b>10.19</b> The following object is masked from ‘package:ggdendro’:</a></li>
<li class="chapter" data-level="10.20" data-path="clustering.html"><a href="clustering.html#section-7"><i class="fa fa-check"></i><b>10.20</b> </a></li>
<li class="chapter" data-level="10.21" data-path="clustering.html"><a href="clustering.html#theme_dendro"><i class="fa fa-check"></i><b>10.21</b> theme_dendro</a></li>
<li class="chapter" data-level="10.22" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packagestats"><i class="fa fa-check"></i><b>10.22</b> The following object is masked from ‘package:stats’:</a></li>
<li class="chapter" data-level="10.23" data-path="clustering.html"><a href="clustering.html#section-8"><i class="fa fa-check"></i><b>10.23</b> </a></li>
<li class="chapter" data-level="10.24" data-path="clustering.html"><a href="clustering.html#cutree"><i class="fa fa-check"></i><b>10.24</b> cutree</a><ul>
<li class="chapter" data-level="10.24.1" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues"><i class="fa fa-check"></i><b>10.24.1</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.25" data-path="clustering.html"><a href="clustering.html#tissue"><i class="fa fa-check"></i><b>10.25</b> tissue</a></li>
<li class="chapter" data-level="10.26" data-path="clustering.html"><a href="clustering.html#cerebellum-colon-endometrium-hippocampus-kidney-liver"><i class="fa fa-check"></i><b>10.26</b> cerebellum colon endometrium hippocampus kidney liver</a></li>
<li class="chapter" data-level="10.27" data-path="clustering.html"><a href="clustering.html#section-9"><i class="fa fa-check"></i><b>10.27</b> 38 34 15 31 39 26</a></li>
<li class="chapter" data-level="10.28" data-path="clustering.html"><a href="clustering.html#placenta"><i class="fa fa-check"></i><b>10.28</b> placenta</a></li>
<li class="chapter" data-level="10.29" data-path="clustering.html"><a href="clustering.html#section-10"><i class="fa fa-check"></i><b>10.29</b> 6</a></li>
<li class="chapter" data-level="10.30" data-path="clustering.html"><a href="clustering.html#section-11"><i class="fa fa-check"></i><b>10.30</b> <a href="clustering.html#section-39">1</a> 22215 189</a></li>
<li class="chapter" data-level="10.31" data-path="clustering.html"><a href="clustering.html#cluster"><i class="fa fa-check"></i><b>10.31</b> cluster</a></li>
<li class="chapter" data-level="10.32" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6"><i class="fa fa-check"></i><b>10.32</b> tissue 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.33" data-path="clustering.html"><a href="clustering.html#cerebellum-0-36-0-0-2-0"><i class="fa fa-check"></i><b>10.33</b> cerebellum 0 36 0 0 2 0</a></li>
<li class="chapter" data-level="10.34" data-path="clustering.html"><a href="clustering.html#colon-0-0-34-0-0-0"><i class="fa fa-check"></i><b>10.34</b> colon 0 0 34 0 0 0</a></li>
<li class="chapter" data-level="10.35" data-path="clustering.html"><a href="clustering.html#endometrium-15-0-0-0-0-0"><i class="fa fa-check"></i><b>10.35</b> endometrium 15 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.36" data-path="clustering.html"><a href="clustering.html#hippocampus-0-31-0-0-0-0"><i class="fa fa-check"></i><b>10.36</b> hippocampus 0 31 0 0 0 0</a></li>
<li class="chapter" data-level="10.37" data-path="clustering.html"><a href="clustering.html#kidney-37-0-0-0-2-0"><i class="fa fa-check"></i><b>10.37</b> kidney 37 0 0 0 2 0</a></li>
<li class="chapter" data-level="10.38" data-path="clustering.html"><a href="clustering.html#liver-0-0-0-24-2-0"><i class="fa fa-check"></i><b>10.38</b> liver 0 0 0 24 2 0</a></li>
<li class="chapter" data-level="10.39" data-path="clustering.html"><a href="clustering.html#placenta-0-0-0-0-0-6"><i class="fa fa-check"></i><b>10.39</b> placenta 0 0 0 0 0 6</a></li>
<li class="chapter" data-level="10.40" data-path="clustering.html"><a href="clustering.html#cluster-1"><i class="fa fa-check"></i><b>10.40</b> cluster</a></li>
<li class="chapter" data-level="10.41" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6-7-8"><i class="fa fa-check"></i><b>10.41</b> tissue 1 2 3 4 5 6 7 8</a></li>
<li class="chapter" data-level="10.42" data-path="clustering.html"><a href="clustering.html#cerebellum-0-31-0-0-2-0-5-0"><i class="fa fa-check"></i><b>10.42</b> cerebellum 0 31 0 0 2 0 5 0</a></li>
<li class="chapter" data-level="10.43" data-path="clustering.html"><a href="clustering.html#colon-0-0-34-0-0-0-0-0"><i class="fa fa-check"></i><b>10.43</b> colon 0 0 34 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.44" data-path="clustering.html"><a href="clustering.html#endometrium-0-0-0-0-0-15-0-0"><i class="fa fa-check"></i><b>10.44</b> endometrium 0 0 0 0 0 15 0 0</a></li>
<li class="chapter" data-level="10.45" data-path="clustering.html"><a href="clustering.html#hippocampus-0-31-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.45</b> hippocampus 0 31 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.46" data-path="clustering.html"><a href="clustering.html#kidney-37-0-0-0-2-0-0-0"><i class="fa fa-check"></i><b>10.46</b> kidney 37 0 0 0 2 0 0 0</a></li>
<li class="chapter" data-level="10.47" data-path="clustering.html"><a href="clustering.html#liver-0-0-0-24-2-0-0-0"><i class="fa fa-check"></i><b>10.47</b> liver 0 0 0 24 2 0 0 0</a></li>
<li class="chapter" data-level="10.48" data-path="clustering.html"><a href="clustering.html#placenta-0-0-0-0-0-0-0-6"><i class="fa fa-check"></i><b>10.48</b> placenta 0 0 0 0 0 0 0 6</a></li>
<li class="chapter" data-level="10.49" data-path="clustering.html"><a href="clustering.html#section-12"><i class="fa fa-check"></i><b>10.49</b> </a></li>
<li class="chapter" data-level="10.50" data-path="clustering.html"><a href="clustering.html#attaching-package-gplots"><i class="fa fa-check"></i><b>10.50</b> Attaching package: ‘gplots’</a></li>
<li class="chapter" data-level="10.51" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packagestats-1"><i class="fa fa-check"></i><b>10.51</b> The following object is masked from ‘package:stats’:</a></li>
<li class="chapter" data-level="10.52" data-path="clustering.html"><a href="clustering.html#section-13"><i class="fa fa-check"></i><b>10.52</b> </a></li>
<li class="chapter" data-level="10.53" data-path="clustering.html"><a href="clustering.html#lowess"><i class="fa fa-check"></i><b>10.53</b> lowess</a></li>
<li class="chapter" data-level="10.54" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.54</b> K-means</a><ul>
<li class="chapter" data-level="10.54.1" data-path="clustering.html"><a href="clustering.html#algorithm-1"><i class="fa fa-check"></i><b>10.54.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.54.2" data-path="clustering.html"><a href="clustering.html#choosing-initial-cluster-centres"><i class="fa fa-check"></i><b>10.54.2</b> Choosing initial cluster centres</a></li>
<li class="chapter" data-level="10.54.3" data-path="clustering.html"><a href="clustering.html#choosingK"><i class="fa fa-check"></i><b>10.54.3</b> Choosing k</a></li>
<li class="chapter" data-level="10.54.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-1"><i class="fa fa-check"></i><b>10.54.4</b> Example: clustering synthetic data sets</a></li>
</ul></li>
<li class="chapter" data-level="10.55" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations"><i class="fa fa-check"></i><b>10.55</b> Warning: did not converge in 10 iterations</a></li>
<li class="chapter" data-level="10.56" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations-1"><i class="fa fa-check"></i><b>10.56</b> Warning: did not converge in 10 iterations</a></li>
<li class="chapter" data-level="10.57" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations-2"><i class="fa fa-check"></i><b>10.57</b> Warning: did not converge in 10 iterations</a></li>
<li class="chapter" data-level="10.58" data-path="clustering.html"><a href="clustering.html#warning-did-not-converge-in-10-iterations-3"><i class="fa fa-check"></i><b>10.58</b> Warning: did not converge in 10 iterations</a><ul>
<li class="chapter" data-level="10.58.1" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-1"><i class="fa fa-check"></i><b>10.58.1</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.59" data-path="clustering.html"><a href="clustering.html#tissue-1"><i class="fa fa-check"></i><b>10.59</b> tissue</a></li>
<li class="chapter" data-level="10.60" data-path="clustering.html"><a href="clustering.html#cerebellum-colon-endometrium-hippocampus-kidney-liver-1"><i class="fa fa-check"></i><b>10.60</b> cerebellum colon endometrium hippocampus kidney liver</a></li>
<li class="chapter" data-level="10.61" data-path="clustering.html"><a href="clustering.html#section-14"><i class="fa fa-check"></i><b>10.61</b> 38 34 15 31 39 26</a></li>
<li class="chapter" data-level="10.62" data-path="clustering.html"><a href="clustering.html#placenta-1"><i class="fa fa-check"></i><b>10.62</b> placenta</a></li>
<li class="chapter" data-level="10.63" data-path="clustering.html"><a href="clustering.html#section-15"><i class="fa fa-check"></i><b>10.63</b> 6</a></li>
<li class="chapter" data-level="10.64" data-path="clustering.html"><a href="clustering.html#section-16"><i class="fa fa-check"></i><b>10.64</b> <a href="clustering.html#section-39">1</a> 22215 189</a></li>
<li class="chapter" data-level="10.65" data-path="clustering.html"><a href="clustering.html#loading-required-package-foreach"><i class="fa fa-check"></i><b>10.65</b> Loading required package: foreach</a></li>
<li class="chapter" data-level="10.66" data-path="clustering.html"><a href="clustering.html#loading-required-package-iterators"><i class="fa fa-check"></i><b>10.66</b> Loading required package: iterators</a></li>
<li class="chapter" data-level="10.67" data-path="clustering.html"><a href="clustering.html#loading-required-package-parallel"><i class="fa fa-check"></i><b>10.67</b> Loading required package: parallel</a></li>
<li class="chapter" data-level="10.68" data-path="clustering.html"><a href="clustering.html#section-17"><i class="fa fa-check"></i><b>10.68</b> <a href="clustering.html#section-39">1</a> 2</a></li>
<li class="chapter" data-level="10.69" data-path="clustering.html"><a href="clustering.html#section-18"><i class="fa fa-check"></i><b>10.69</b> </a></li>
<li class="chapter" data-level="10.70" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6-7"><i class="fa fa-check"></i><b>10.70</b> tissue 1 2 3 4 5 6 7</a></li>
<li class="chapter" data-level="10.71" data-path="clustering.html"><a href="clustering.html#cerebellum-0-0-0-33-0-0-5"><i class="fa fa-check"></i><b>10.71</b> cerebellum 0 0 0 33 0 0 5</a></li>
<li class="chapter" data-level="10.72" data-path="clustering.html"><a href="clustering.html#colon-0-0-0-0-0-34-0"><i class="fa fa-check"></i><b>10.72</b> colon 0 0 0 0 0 34 0</a></li>
<li class="chapter" data-level="10.73" data-path="clustering.html"><a href="clustering.html#endometrium-0-0-0-0-15-0-0"><i class="fa fa-check"></i><b>10.73</b> endometrium 0 0 0 0 15 0 0</a></li>
<li class="chapter" data-level="10.74" data-path="clustering.html"><a href="clustering.html#hippocampus-0-0-0-0-0-0-31"><i class="fa fa-check"></i><b>10.74</b> hippocampus 0 0 0 0 0 0 31</a></li>
<li class="chapter" data-level="10.75" data-path="clustering.html"><a href="clustering.html#kidney-0-0-39-0-0-0-0"><i class="fa fa-check"></i><b>10.75</b> kidney 0 0 39 0 0 0 0</a></li>
<li class="chapter" data-level="10.76" data-path="clustering.html"><a href="clustering.html#liver-26-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.76</b> liver 26 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.77" data-path="clustering.html"><a href="clustering.html#placenta-0-6-0-0-0-0-0"><i class="fa fa-check"></i><b>10.77</b> placenta 0 6 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.78" data-path="clustering.html"><a href="clustering.html#section-19"><i class="fa fa-check"></i><b>10.78</b> </a></li>
<li class="chapter" data-level="10.79" data-path="clustering.html"><a href="clustering.html#tissue-1-2-3-4-5-6-7-1"><i class="fa fa-check"></i><b>10.79</b> tissue 1 2 3 4 5 6 7</a></li>
<li class="chapter" data-level="10.80" data-path="clustering.html"><a href="clustering.html#cerebellum-0-0-0-0-0-5-33"><i class="fa fa-check"></i><b>10.80</b> cerebellum 0 0 0 0 0 5 33</a></li>
<li class="chapter" data-level="10.81" data-path="clustering.html"><a href="clustering.html#colon-0-0-34-0-0-0-0"><i class="fa fa-check"></i><b>10.81</b> colon 0 0 34 0 0 0 0</a></li>
<li class="chapter" data-level="10.82" data-path="clustering.html"><a href="clustering.html#endometrium-15-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.82</b> endometrium 15 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.83" data-path="clustering.html"><a href="clustering.html#hippocampus-0-0-0-0-31-0-0"><i class="fa fa-check"></i><b>10.83</b> hippocampus 0 0 0 0 31 0 0</a></li>
<li class="chapter" data-level="10.84" data-path="clustering.html"><a href="clustering.html#kidney-37-2-0-0-0-0-0"><i class="fa fa-check"></i><b>10.84</b> kidney 37 2 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.85" data-path="clustering.html"><a href="clustering.html#liver-0-26-0-0-0-0-0"><i class="fa fa-check"></i><b>10.85</b> liver 0 26 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.86" data-path="clustering.html"><a href="clustering.html#placenta-0-0-0-6-0-0-0"><i class="fa fa-check"></i><b>10.86</b> placenta 0 0 0 6 0 0 0</a></li>
<li class="chapter" data-level="10.87" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>10.87</b> DBSCAN</a><ul>
<li class="chapter" data-level="10.87.1" data-path="clustering.html"><a href="clustering.html#algorithm-2"><i class="fa fa-check"></i><b>10.87.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.87.2" data-path="clustering.html"><a href="clustering.html#implementation-in-r"><i class="fa fa-check"></i><b>10.87.2</b> Implementation in R</a></li>
<li class="chapter" data-level="10.87.3" data-path="clustering.html"><a href="clustering.html#choosing-parameters"><i class="fa fa-check"></i><b>10.87.3</b> Choosing parameters</a></li>
</ul></li>
<li class="chapter" data-level="10.88" data-path="clustering.html"><a href="clustering.html#section-20"><i class="fa fa-check"></i><b>10.88</b> </a></li>
<li class="chapter" data-level="10.89" data-path="clustering.html"><a href="clustering.html#section-21"><i class="fa fa-check"></i><b>10.89</b> 0 1 2 3</a></li>
<li class="chapter" data-level="10.90" data-path="clustering.html"><a href="clustering.html#section-22"><i class="fa fa-check"></i><b>10.90</b> 43 484 486 487</a><ul>
<li class="chapter" data-level="10.90.1" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-2"><i class="fa fa-check"></i><b>10.90.1</b> Example: clustering synthetic data sets</a></li>
</ul></li>
<li class="chapter" data-level="10.91" data-path="clustering.html"><a href="clustering.html#section-23"><i class="fa fa-check"></i><b>10.91</b> </a></li>
<li class="chapter" data-level="10.92" data-path="clustering.html"><a href="clustering.html#section-24"><i class="fa fa-check"></i><b>10.92</b> 0 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.93" data-path="clustering.html"><a href="clustering.html#section-25"><i class="fa fa-check"></i><b>10.93</b> 2 168 307 105 127 45 34</a></li>
<li class="chapter" data-level="10.94" data-path="clustering.html"><a href="clustering.html#section-26"><i class="fa fa-check"></i><b>10.94</b> </a></li>
<li class="chapter" data-level="10.95" data-path="clustering.html"><a href="clustering.html#section-27"><i class="fa fa-check"></i><b>10.95</b> 0 1 2</a></li>
<li class="chapter" data-level="10.96" data-path="clustering.html"><a href="clustering.html#section-28"><i class="fa fa-check"></i><b>10.96</b> 8 748 744</a></li>
<li class="chapter" data-level="10.97" data-path="clustering.html"><a href="clustering.html#section-29"><i class="fa fa-check"></i><b>10.97</b> </a></li>
<li class="chapter" data-level="10.98" data-path="clustering.html"><a href="clustering.html#section-30"><i class="fa fa-check"></i><b>10.98</b> 0 1</a></li>
<li class="chapter" data-level="10.99" data-path="clustering.html"><a href="clustering.html#section-31"><i class="fa fa-check"></i><b>10.99</b> 40 1460</a></li>
<li class="chapter" data-level="10.100" data-path="clustering.html"><a href="clustering.html#section-32"><i class="fa fa-check"></i><b>10.100</b> </a></li>
<li class="chapter" data-level="10.101" data-path="clustering.html"><a href="clustering.html#section-33"><i class="fa fa-check"></i><b>10.101</b> 0 1 2</a></li>
<li class="chapter" data-level="10.102" data-path="clustering.html"><a href="clustering.html#section-34"><i class="fa fa-check"></i><b>10.102</b> 109 399 992</a></li>
<li class="chapter" data-level="10.103" data-path="clustering.html"><a href="clustering.html#section-35"><i class="fa fa-check"></i><b>10.103</b> </a></li>
<li class="chapter" data-level="10.104" data-path="clustering.html"><a href="clustering.html#section-36"><i class="fa fa-check"></i><b>10.104</b> 0 1 2 3</a></li>
<li class="chapter" data-level="10.105" data-path="clustering.html"><a href="clustering.html#section-37"><i class="fa fa-check"></i><b>10.105</b> 29 489 488 494</a></li>
<li class="chapter" data-level="10.106" data-path="clustering.html"><a href="clustering.html#section-38"><i class="fa fa-check"></i><b>10.106</b> </a></li>
<li class="chapter" data-level="10.107" data-path="clustering.html"><a href="clustering.html#section-39"><i class="fa fa-check"></i><b>10.107</b> 1</a></li>
<li class="chapter" data-level="10.108" data-path="clustering.html"><a href="clustering.html#section-40"><i class="fa fa-check"></i><b>10.108</b> 1500</a><ul>
<li class="chapter" data-level="10.108.1" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-2"><i class="fa fa-check"></i><b>10.108.1</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.109" data-path="clustering.html"><a href="clustering.html#tissue-2"><i class="fa fa-check"></i><b>10.109</b> tissue</a></li>
<li class="chapter" data-level="10.110" data-path="clustering.html"><a href="clustering.html#cerebellum-colon-endometrium-hippocampus-kidney-liver-2"><i class="fa fa-check"></i><b>10.110</b> cerebellum colon endometrium hippocampus kidney liver</a></li>
<li class="chapter" data-level="10.111" data-path="clustering.html"><a href="clustering.html#section-41"><i class="fa fa-check"></i><b>10.111</b> 38 34 15 31 39 26</a></li>
<li class="chapter" data-level="10.112" data-path="clustering.html"><a href="clustering.html#placenta-2"><i class="fa fa-check"></i><b>10.112</b> placenta</a></li>
<li class="chapter" data-level="10.113" data-path="clustering.html"><a href="clustering.html#section-42"><i class="fa fa-check"></i><b>10.113</b> 6</a></li>
<li class="chapter" data-level="10.114" data-path="clustering.html"><a href="clustering.html#section-43"><i class="fa fa-check"></i><b>10.114</b> </a></li>
<li class="chapter" data-level="10.115" data-path="clustering.html"><a href="clustering.html#section-44"><i class="fa fa-check"></i><b>10.115</b> 0 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.116" data-path="clustering.html"><a href="clustering.html#section-45"><i class="fa fa-check"></i><b>10.116</b> 12 37 62 34 24 15 5</a></li>
<li class="chapter" data-level="10.117" data-path="clustering.html"><a href="clustering.html#section-46"><i class="fa fa-check"></i><b>10.117</b> </a></li>
<li class="chapter" data-level="10.118" data-path="clustering.html"><a href="clustering.html#tissue-0-1-2-3-4-5-6"><i class="fa fa-check"></i><b>10.118</b> tissue 0 1 2 3 4 5 6</a></li>
<li class="chapter" data-level="10.119" data-path="clustering.html"><a href="clustering.html#cerebellum-2-0-31-0-0-0-5"><i class="fa fa-check"></i><b>10.119</b> cerebellum 2 0 31 0 0 0 5</a></li>
<li class="chapter" data-level="10.120" data-path="clustering.html"><a href="clustering.html#colon-0-0-0-34-0-0-0"><i class="fa fa-check"></i><b>10.120</b> colon 0 0 0 34 0 0 0</a></li>
<li class="chapter" data-level="10.121" data-path="clustering.html"><a href="clustering.html#endometrium-0-0-0-0-0-15-0"><i class="fa fa-check"></i><b>10.121</b> endometrium 0 0 0 0 0 15 0</a></li>
<li class="chapter" data-level="10.122" data-path="clustering.html"><a href="clustering.html#hippocampus-0-0-31-0-0-0-0"><i class="fa fa-check"></i><b>10.122</b> hippocampus 0 0 31 0 0 0 0</a></li>
<li class="chapter" data-level="10.123" data-path="clustering.html"><a href="clustering.html#kidney-2-37-0-0-0-0-0"><i class="fa fa-check"></i><b>10.123</b> kidney 2 37 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.124" data-path="clustering.html"><a href="clustering.html#liver-2-0-0-0-24-0-0"><i class="fa fa-check"></i><b>10.124</b> liver 2 0 0 0 24 0 0</a></li>
<li class="chapter" data-level="10.125" data-path="clustering.html"><a href="clustering.html#placenta-6-0-0-0-0-0-0"><i class="fa fa-check"></i><b>10.125</b> placenta 6 0 0 0 0 0 0</a></li>
<li class="chapter" data-level="10.126" data-path="clustering.html"><a href="clustering.html#evaluating-cluster-quality"><i class="fa fa-check"></i><b>10.126</b> Evaluating cluster quality</a><ul>
<li class="chapter" data-level="10.126.1" data-path="clustering.html"><a href="clustering.html#silhouetteMethod"><i class="fa fa-check"></i><b>10.126.1</b> Silhouette method</a></li>
<li class="chapter" data-level="10.126.2" data-path="clustering.html"><a href="clustering.html#example---k-means-clustering-of-blobs-data-set"><i class="fa fa-check"></i><b>10.126.2</b> Example - k-means clustering of blobs data set</a></li>
</ul></li>
<li class="chapter" data-level="10.127" data-path="clustering.html"><a href="clustering.html#summary"><i class="fa fa-check"></i><b>10.127</b> Summary</a><ul>
<li class="chapter" data-level="10.127.1" data-path="clustering.html"><a href="clustering.html#applications"><i class="fa fa-check"></i><b>10.127.1</b> Applications</a></li>
<li class="chapter" data-level="10.127.2" data-path="clustering.html"><a href="clustering.html#strengths"><i class="fa fa-check"></i><b>10.127.2</b> Strengths</a></li>
<li class="chapter" data-level="10.127.3" data-path="clustering.html"><a href="clustering.html#limitations"><i class="fa fa-check"></i><b>10.127.3</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="10.128" data-path="clustering.html"><a href="clustering.html#exercises-7"><i class="fa fa-check"></i><b>10.128</b> Exercises</a><ul>
<li class="chapter" data-level="10.128.1" data-path="clustering.html"><a href="clustering.html#clusteringEx1"><i class="fa fa-check"></i><b>10.128.1</b> Exercise 1</a></li>
<li class="chapter" data-level="10.128.2" data-path="clustering.html"><a href="clustering.html#clusteringEx2"><i class="fa fa-check"></i><b>10.128.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="10.129" data-path="clustering.html"><a href="clustering.html#section-47"><i class="fa fa-check"></i><b>10.129</b> </a></li>
<li class="chapter" data-level="10.130" data-path="clustering.html"><a href="clustering.html#attaching-package-ebimage"><i class="fa fa-check"></i><b>10.130</b> Attaching package: ‘EBImage’</a></li>
<li class="chapter" data-level="10.131" data-path="clustering.html"><a href="clustering.html#the-following-object-is-masked-from-packagedendextend"><i class="fa fa-check"></i><b>10.131</b> The following object is masked from ‘package:dendextend’:</a></li>
<li class="chapter" data-level="10.132" data-path="clustering.html"><a href="clustering.html#section-48"><i class="fa fa-check"></i><b>10.132</b> </a></li>
<li class="chapter" data-level="10.133" data-path="clustering.html"><a href="clustering.html#rotate"><i class="fa fa-check"></i><b>10.133</b> rotate</a></li>
<li class="chapter" data-level="10.134" data-path="clustering.html"><a href="clustering.html#section-49"><i class="fa fa-check"></i><b>10.134</b> <a href="clustering.html#section-39">1</a> 528 393 3</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span><ul>
<li class="chapter" data-level="A.135" data-path="04-nearest-neighbours.html"><a href="#python"><i class="fa fa-check"></i><b>A.135</b> Python</a></li>
<li class="chapter" data-level="A.136" data-path="clustering.html"><a href="clustering.html#machine-learning-data-set-repositories"><i class="fa fa-check"></i><b>A.136</b> Machine learning data set repositories</a><ul>
<li class="chapter" data-level="A.136.1" data-path="clustering.html"><a href="clustering.html#mldata"><i class="fa fa-check"></i><b>A.136.1</b> MLDATA</a></li>
<li class="chapter" data-level="A.136.2" data-path="clustering.html"><a href="clustering.html#uci-machine-learning-repository"><i class="fa fa-check"></i><b>A.136.2</b> UCI Machine Learning Repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html"><i class="fa fa-check"></i><b>B</b> Solutions ch. 3 - Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="B.1" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#exercise-1"><i class="fa fa-check"></i><b>B.1</b> Exercise 1</a></li>
<li class="chapter" data-level="B.2" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#exercise-2"><i class="fa fa-check"></i><b>B.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html"><i class="fa fa-check"></i><b>C</b> Solutions ch. 4 - Linear and non-linear logistic regression</a><ul>
<li class="chapter" data-level="C.1" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html#exercise-1-1"><i class="fa fa-check"></i><b>C.1</b> Exercise 1</a></li>
<li class="chapter" data-level="C.2" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html#exercise-2-1"><i class="fa fa-check"></i><b>C.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html"><i class="fa fa-check"></i><b>D</b> Solutions ch. 5 - Nearest neighbours</a><ul>
<li class="chapter" data-level="D.1" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-1-2"><i class="fa fa-check"></i><b>D.1</b> Exercise 1</a></li>
<li class="chapter" data-level="D.2" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-2-2"><i class="fa fa-check"></i><b>D.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html"><i class="fa fa-check"></i><b>E</b> Solutions ch. 6 - Decision trees and random forests</a><ul>
<li class="chapter" data-level="E.1" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-1-3"><i class="fa fa-check"></i><b>E.1</b> Exercise 1</a></li>
<li class="chapter" data-level="E.2" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-2-3"><i class="fa fa-check"></i><b>E.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="solutions-svm.html"><a href="solutions-svm.html"><i class="fa fa-check"></i><b>F</b> Solutions ch. 7 - Support vector machines</a><ul>
<li class="chapter" data-level="F.1" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-1-4"><i class="fa fa-check"></i><b>F.1</b> Exercise 1</a></li>
<li class="chapter" data-level="F.2" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-2-4"><i class="fa fa-check"></i><b>F.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="solutions-ann.html"><a href="solutions-ann.html"><i class="fa fa-check"></i><b>G</b> Solutions ch. 8 - Artificial neural networks</a><ul>
<li class="chapter" data-level="G.1" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-1-5"><i class="fa fa-check"></i><b>G.1</b> Exercise 1</a></li>
<li class="chapter" data-level="G.2" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-2-5"><i class="fa fa-check"></i><b>G.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="H" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html"><i class="fa fa-check"></i><b>H</b> Solutions ch. 9 - Dimensionality reduction</a><ul>
<li class="chapter" data-level="H.1" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-1-6"><i class="fa fa-check"></i><b>H.1</b> Exercise 1</a></li>
<li class="chapter" data-level="H.2" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-2-6"><i class="fa fa-check"></i><b>H.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="I" data-path="solutions-clustering.html"><a href="solutions-clustering.html"><i class="fa fa-check"></i><b>I</b> Solutions ch. 10 - Clustering</a><ul>
<li class="chapter" data-level="I.1" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-1-7"><i class="fa fa-check"></i><b>I.1</b> Exercise 1</a></li>
<li class="chapter" data-level="I.2" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-2-7"><i class="fa fa-check"></i><b>I.2</b> Exercise 2</a></li>
<li class="chapter" data-level="I.3" data-path="solutions-clustering.html"><a href="solutions-clustering.html#loading-required-package-foreach-1"><i class="fa fa-check"></i><b>I.3</b> Loading required package: foreach</a></li>
<li class="chapter" data-level="I.4" data-path="solutions-clustering.html"><a href="solutions-clustering.html#loading-required-package-iterators-1"><i class="fa fa-check"></i><b>I.4</b> Loading required package: iterators</a></li>
<li class="chapter" data-level="I.5" data-path="solutions-clustering.html"><a href="solutions-clustering.html#loading-required-package-parallel-1"><i class="fa fa-check"></i><b>I.5</b> Loading required package: parallel</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nearest-neighbours" class="section level1">
<h1><span class="header-section-number">5</span> Nearest neighbours</h1>
<!-- Matt -->
<!-- 
Get ideas on presentation from Harvard bioinformatics website. In particular, use of dataset with two variables (crabs??), because easier to display. Performance of classifier as k increases (should initially improve and then get worse - starts to lose flexibility).

In exercises could introduce application of knn to regression.

GENERAL:
SPLOM for displaying datasets with small number of variables

FEATURE SELECTION
filter methods  /  wrapper methods / genetic algorithms

Refer to scikit learn

FEATURE SCALING

BIAS-VARIANCE TRADEOFF
In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set[citation needed].:

    The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
    The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).


-->
<div id="introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>memory based and require no model to be fit</p>
<p>classification and non-linear regression</p>
<p>bias and variance</p>
<p>computational load - finding neighbours and storing the entire training set</p>
<p>k-d tree / linear search</p>
<p>system.time k-d tree search vs linear search</p>
<p>library(class)</p>
<p>class::knn</p>
<p>importance of centering a scaling</p>
<p>increase in neighbours - increase in ties</p>
<div id="measuring-distance-between-objects" class="section level3">
<h3><span class="header-section-number">5.1.1</span> Measuring distance between objects</h3>
<strong>Euclidean distance:</strong>
<span class="math display" id="eq:euclidean">\[\begin{equation}
  distance\left(p,q\right)=\sqrt{\sum_{i=1}^{n} (p_i-q_i)^2}
  \tag{5.1}
\end{equation}\]</span>
<div class="figure" style="text-align: center"><span id="fig:euclideanDistanceDiagram"></span>
<img src="04-nearest-neighbours_files/figure-html/euclideanDistanceDiagram-1.png" alt="Euclidean distance." width="75%" />
<p class="caption">
Figure 5.1: Euclidean distance.
</p>
</div>
</div>
</div>
<div id="classification" class="section level2">
<h2><span class="header-section-number">5.2</span> Classification</h2>
<div id="algorithm" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Algorithm</h3>
<div class="figure" style="text-align: center"><span id="fig:knnClassification"></span>
<img src="images/knn_classification.svg" alt="Illustration of _k_-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class.  By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282" width="75%" />
<p class="caption">
Figure 5.2: Illustration of <em>k</em>-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class. By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, <a href="https://commons.wikimedia.org/w/index.php?curid=2170282" class="uri">https://commons.wikimedia.org/w/index.php?curid=2170282</a>
</p>
</div>
</div>
<div id="simulated-data" class="section level3">
<h3><span class="header-section-number">5.2.2</span> Simulated data</h3>
<p>We will use a simulated data set to demonstrate:</p>
<ul>
<li>bias-variance trade-off</li>
<li>the knn function in R</li>
<li>plotting decision boundaries</li>
<li>choosing the optimum value of <em>k</em></li>
</ul>
<p>The dataset is partitioned into training and test sets.</p>
<p>Load data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;data/example_binary_classification/bin_class_example.rda&quot;</span>)
<span class="kw">str</span>(xtrain)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  -0.223 0.944 2.36 1.846 1.732 ...
##  $ V2: num  -1.153 -0.827 -0.128 2.014 -0.574 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(xtest)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  2.09 2.3 2.07 1.65 1.18 ...
##  $ V2: num  -1.009 1.0947 0.1644 0.3243 -0.0277 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytrain))</code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytest))</code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(GGally)
<span class="kw">library</span>(RColorBrewer)
point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)
point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)
point_size =<span class="st"> </span><span class="dv">2</span>

<span class="kw">ggplot</span>(xtrain, <span class="kw">aes</span>(V1,V2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytrain<span class="dv">+1</span>], <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], 
             <span class="dt">size=</span>point_size) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(xtest, <span class="kw">aes</span>(V1,V2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytest<span class="dv">+1</span>], <span class="dt">shape=</span>point_shapes[ytest<span class="dv">+1</span>], 
             <span class="dt">size=</span>point_size) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simDataBinClassTrainTest"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-1.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-2.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" />
<p class="caption">
Figure 5.3: Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using <em>k</em>-nn
</p>
</div>
</div>
<div id="knn-function" class="section level3">
<h3><span class="header-section-number">5.2.3</span> knn function</h3>
<p>For <em>k</em>-nn classification and regression we will use the <strong>knn</strong> function in the package <strong>class</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)</code></pre></div>
<p><strong>Arguments to knn</strong></p>
<ul>
<li><code>train</code> : matrix or data frame of training set cases.</li>
<li><code>test</code> : matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.</li>
<li><code>cl</code> : factor of true classifications of training set</li>
<li><code>k</code> : number of neighbours considered.</li>
<li><code>l</code> : minimum vote for definite decision, otherwise doubt. (More precisely, less than k-l dissenting votes are allowed, even if k is increased by ties.)</li>
<li><code>prob</code> : If this is true, the proportion of the votes for the winning class are returned as attribute prob.</li>
<li><code>use.all</code> : controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours.</li>
</ul>
<p>Let us perform <em>k</em>-nn on the training set with <em>k</em>=1. We will use the <strong>confusionMatrix</strong> function from the <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> package to summarize performance of the classifier.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)</code></pre></div>
<pre><code>## Loading required package: lattice</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1train &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtrain, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
<span class="kw">confusionMatrix</span>(knn1train, ytrain)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 200   0
##          1   0 200
##                                      
##                Accuracy : 1          
##                  95% CI : (0.9908, 1)
##     No Information Rate : 0.5        
##     P-Value [Acc &gt; NIR] : &lt; 2.2e-16  
##                                      
##                   Kappa : 1          
##  Mcnemar&#39;s Test P-Value : NA         
##                                      
##             Sensitivity : 1.0        
##             Specificity : 1.0        
##          Pos Pred Value : 1.0        
##          Neg Pred Value : 1.0        
##              Prevalence : 0.5        
##          Detection Rate : 0.5        
##    Detection Prevalence : 0.5        
##       Balanced Accuracy : 1.0        
##                                      
##        &#39;Positive&#39; Class : 0          
## </code></pre>
<!--
table(ytrain,knn1train)
cat("KNN prediction error for training set: ", 1-mean(as.numeric(as.vector(knn1train))==ytrain), "\n")
-->
<p>Now let use the training set to predict on the test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1test &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtest, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
<span class="kw">confusionMatrix</span>(knn1test, ytest)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 131  81
##          1  69 119
##                                           
##                Accuracy : 0.625           
##                  95% CI : (0.5755, 0.6726)
##     No Information Rate : 0.5             
##     P-Value [Acc &gt; NIR] : 3.266e-07       
##                                           
##                   Kappa : 0.25            
##  Mcnemar&#39;s Test P-Value : 0.3691          
##                                           
##             Sensitivity : 0.6550          
##             Specificity : 0.5950          
##          Pos Pred Value : 0.6179          
##          Neg Pred Value : 0.6330          
##              Prevalence : 0.5000          
##          Detection Rate : 0.3275          
##    Detection Prevalence : 0.5300          
##       Balanced Accuracy : 0.6250          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<!--
table(ytest, knn1test)
cat("KNN prediction error for test set: ", 1-mean(as.numeric(as.vector(knn1test))==ytest), "\n")
-->
</div>
<div id="plotting-decision-boundaries" class="section level3">
<h3><span class="header-section-number">5.2.4</span> Plotting decision boundaries</h3>
<p>Since we have just two dimensions we can visualize the decision boundary generated by the <em>k</em>-nn classifier in a 2D scatterplot. Situations where your original data set contains only two variables will be rare, but it is not unusual to reduce a high-dimensional data set to just two dimensions using the methods that will be discussed in chapter <a href="nearest-neighbours.html#dimensionality-reduction">5.2.7.7</a>. Therefore, knowing how to plot decision boundaries will potentially be helpful for many different datasets and classifiers.</p>
<p>Create a grid so we can predict across the full range of our variables V1 and V2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gridSize &lt;-<span class="st"> </span><span class="dv">150</span> 
v1limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])))
tmpV1 &lt;-<span class="st"> </span><span class="kw">seq</span>(v1limits[<span class="dv">1</span>],v1limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
v2limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])))
tmpV2 &lt;-<span class="st"> </span><span class="kw">seq</span>(v2limits[<span class="dv">1</span>],v2limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(tmpV1,tmpV2)
<span class="kw">names</span>(xgrid) &lt;-<span class="st"> </span><span class="kw">names</span>(xtrain)</code></pre></div>
<p>Predict values of all elements of grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1grid &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xgrid, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
V3 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1grid))
xgrid &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgrid, V3)</code></pre></div>
<p>Plot</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)
point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)
point_size =<span class="st"> </span><span class="dv">2</span>

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtrain, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytrain<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtest, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytest<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simDataBinClassDecisionBoundaryK1"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-1.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-2.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" />
<p class="caption">
Figure 5.4: Binary classification of the simulated training and test sets with <em>k</em>=1.
</p>
</div>
</div>
<div id="bias-variance-tradeoff" class="section level3">
<h3><span class="header-section-number">5.2.5</span> Bias-variance tradeoff</h3>
<p>The bias–variance tradeoff is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set:</p>
<ul>
<li>The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).</li>
<li>The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).</li>
</ul>
<p>To demonstrate this phenomenon, let us look at the performance of the <em>k</em>-nn classifier over a range of values of <em>k</em>. First we will define a function to create a sequence of log spaced values. This is the <strong>lseq</strong> function from the <a href="https://cran.r-project.org/package=emdbook">emdbook</a> package:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">lseq &lt;-<span class="st"> </span>function(from, to, length.out) {
  <span class="kw">exp</span>(<span class="kw">seq</span>(<span class="kw">log</span>(from), <span class="kw">log</span>(to), <span class="dt">length.out =</span> length.out))
}</code></pre></div>
<p>Get log spaced sequence of length 20, round and then remove any duplicates resulting from rounding.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">unique</span>(<span class="kw">round</span>(<span class="kw">lseq</span>(<span class="dv">1</span>,<span class="dv">400</span>,<span class="dv">20</span>)))
<span class="kw">length</span>(s)</code></pre></div>
<pre><code>## [1] 19</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">train_error &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, function(i){
  yhat &lt;-<span class="st"> </span><span class="kw">knn</span>(xtrain, xtrain, ytrain, i)
  <span class="kw">return</span>(<span class="dv">1</span>-<span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(yhat))==ytrain))
})

test_error &lt;-<span class="st"> </span><span class="kw">sapply</span>(s, function(i){
  yhat &lt;-<span class="st"> </span><span class="kw">knn</span>(xtrain, xtest, ytrain, i)
  <span class="kw">return</span>(<span class="dv">1</span>-<span class="kw">mean</span>(<span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(yhat))==ytest))
})

k &lt;-<span class="st"> </span><span class="kw">rep</span>(s, <span class="dv">2</span>)
set &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">rep</span>(<span class="st">&quot;train&quot;</span>, <span class="kw">length</span>(s)), <span class="kw">rep</span>(<span class="st">&quot;test&quot;</span>, <span class="kw">length</span>(s)))
error &lt;-<span class="st"> </span><span class="kw">c</span>(train_error, test_error)
misclass_errors &lt;-<span class="st"> </span><span class="kw">data.frame</span>(k, set, error)</code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(misclass_errors, <span class="kw">aes</span>(<span class="dt">x=</span>k, <span class="dt">y=</span>error, <span class="dt">group=</span>set)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_line</span>(<span class="kw">aes</span>(<span class="dt">colour=</span>set, <span class="dt">linetype=</span>set), <span class="dt">size=</span><span class="fl">1.5</span>) +
<span class="st">  </span><span class="kw">scale_x_log10</span>() +
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">&quot;Misclassification Errors&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">legend.position =</span> <span class="kw">c</span>(<span class="fl">0.5</span>, <span class="fl">0.25</span>), <span class="dt">legend.title=</span><span class="kw">element_blank</span>(),
        <span class="dt">legend.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">12</span>), 
        <span class="dt">axis.title.x=</span><span class="kw">element_text</span>(<span class="dt">face=</span><span class="st">&quot;italic&quot;</span>, <span class="dt">size=</span><span class="dv">12</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:misclassErrorsFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/misclassErrorsFunK-1.png" alt="Misclassification errors as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.5: Misclassification errors as a function of neighbourhood size.
</p>
</div>
</div>
<div id="choosing-k" class="section level3">
<h3><span class="header-section-number">5.2.6</span> Choosing <em>k</em></h3>
<p>We will use the caret library.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(caret)</code></pre></div>
<p><a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> has automatic parallel processing built in. To take advantage of this feature we simply need to load the <a href="http://cran.r-project.org/web/packages/doMC/index.html">doMC</a> package and register workers:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(doMC)</code></pre></div>
<pre><code>## Loading required package: foreach</code></pre>
<pre><code>## Loading required package: iterators</code></pre>
<pre><code>## Loading required package: parallel</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">registerDoMC</span>()</code></pre></div>
<p>To find out how many cores we have registered we can use:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">getDoParWorkers</span>()</code></pre></div>
<pre><code>## [1] 2</code></pre>
<p>The <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> function <strong>train</strong> is used to fit predictive models over different values of <em>k</em>. The function <strong>trainControl</strong> is used to specify a list of computational and resampling options, which will be passed to <strong>train</strong>. We will start by configuring our cross-validation procedure using <strong>trainControl</strong>.</p>
<p>We would like to make this demonstration reproducible and because we will be running the models in parallel, using the <strong>set.seed</strong> function alone is not sufficient. In addition to using <strong>set.seed</strong> we have to make use of the optional <strong>seeds</strong> argument to <strong>trainControl</strong>. We need to supply <strong>seeds</strong> with a list of integers that will be used to set the seed at each sampling iteration. The list is required to have a length of B+1, where B is the number of resamples. We will be repeating 10-fold cross-validation a total of ten times and so our list must have a length of 101. The first B elements of the list are required to be vectors of integers of length M, where M is the number of models being evaluated (in this case 19). The last element of the list only needs to be a single integer, which will be used for the final model.</p>
<p>First we generate our list of seeds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">101</span>)
for(i in <span class="dv">1</span>:<span class="dv">100</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">19</span>)
seeds[[<span class="dv">101</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</code></pre></div>
<p>We can now use <strong>trainControl</strong> to create a list of computational options for resampling.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tc &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,
                   <span class="dt">number =</span> <span class="dv">10</span>,
                   <span class="dt">repeats =</span> <span class="dv">10</span>,
                   <span class="dt">seeds =</span> seeds)</code></pre></div>
<p>There are two options for choosing the values of <em>k</em> to be evaluated by the <strong>train</strong> function:</p>
<ol style="list-style-type: decimal">
<li>Pass a data.frame of values of <em>k</em> to the <strong>tuneGrid</strong> argument of <strong>train</strong>.</li>
<li>Specify the number of different levels of <em>k</em> using the <strong>tuneLength</strong> function and allow <strong>train</strong> to pick the actual values.</li>
</ol>
<p>We will use the first option, so that we can try the values of <em>k</em> we examined earlier. We need to convert the vector of values of k we created earlier and convert it into a data.frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">s &lt;-<span class="st"> </span><span class="kw">data.frame</span>(s)
<span class="kw">names</span>(s) &lt;-<span class="st"> &quot;k&quot;</span></code></pre></div>
<p>We are now ready to run the cross-validation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knnFit &lt;-<span class="st"> </span><span class="kw">train</span>(xtrain, <span class="kw">as.factor</span>(ytrain), 
                <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>,
                <span class="dt">tuneGrid=</span>s,
                <span class="dt">trControl=</span>tc)

knnFit</code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 400 samples
##   2 predictor
##   2 classes: &#39;0&#39;, &#39;1&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 360, 360, 360, 360, 360, 360, ... 
## Resampling results across tuning parameters:
## 
##   k    Accuracy  Kappa 
##     1  0.63375   0.2675
##     2  0.64125   0.2825
##     3  0.67925   0.3585
##     4  0.67200   0.3440
##     5  0.69675   0.3935
##     7  0.71100   0.4220
##     9  0.71650   0.4330
##    12  0.71450   0.4290
##    17  0.72650   0.4530
##    23  0.73175   0.4635
##    32  0.73775   0.4755
##    44  0.74075   0.4815
##    60  0.74675   0.4935
##    83  0.75475   0.5095
##   113  0.73600   0.4720
##   155  0.72500   0.4500
##   213  0.70950   0.4190
##   292  0.69300   0.3860
##   400  0.51300   0.0260
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 83.</code></pre>
<strong>Cohen’s Kappa:</strong>
<span class="math display" id="eq:kappa">\[\begin{equation}
  Kappa = \frac{O-E}{1-E}
  \tag{5.2}
\end{equation}\]</span>
<p>where <em>O</em> is the observed accuracy and <em>E</em> is the expected accuracy based on the marginal totals of the confusion matrix. Cohen’s Kappa takes values between -1 and 1; a value of zero indicates no agreement between the observed and predicted classes, while a value of one shows perfect concordance of the model prediction and the observed classes. If the prediction is in the opposite direction of the truth, a negative value will be obtained, but large negative values are rare in practice <span class="citation">(Kuhn and Johnson <a href="#ref-Kuhn2013">2013</a>)</span>.</p>
<p>We can plot accuracy (determined from repeated cross-validation) as a function of neighbourhood size.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(knnFit)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:cvAccuracyFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/cvAccuracyFunK-1.png" alt="Accuracy (repeated cross-validation) as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.6: Accuracy (repeated cross-validation) as a function of neighbourhood size.
</p>
</div>
<p>We can also plot other performance metrics, such as Cohen’s Kappa, using the <strong>metric</strong> argument.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(knnFit, <span class="dt">metric=</span><span class="st">&quot;Kappa&quot;</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:cvKappaFunK"></span>
<img src="04-nearest-neighbours_files/figure-html/cvKappaFunK-1.png" alt="Cohen's Kappa (repeated cross-validation) as a function of neighbourhood size." width="100%" />
<p class="caption">
Figure 5.7: Cohen’s Kappa (repeated cross-validation) as a function of neighbourhood size.
</p>
</div>
<p>Let us now evaluate how our classifier performs on the test set.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">test_pred &lt;-<span class="st"> </span><span class="kw">predict</span>(knnFit, xtest)
<span class="kw">confusionMatrix</span>(test_pred, ytest)</code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction   0   1
##          0 154  68
##          1  46 132
##                                          
##                Accuracy : 0.715          
##                  95% CI : (0.668, 0.7588)
##     No Information Rate : 0.5            
##     P-Value [Acc &gt; NIR] : &lt;2e-16         
##                                          
##                   Kappa : 0.43           
##  Mcnemar&#39;s Test P-Value : 0.0492         
##                                          
##             Sensitivity : 0.7700         
##             Specificity : 0.6600         
##          Pos Pred Value : 0.6937         
##          Neg Pred Value : 0.7416         
##              Prevalence : 0.5000         
##          Detection Rate : 0.3850         
##    Detection Prevalence : 0.5550         
##       Balanced Accuracy : 0.7150         
##                                          
##        &#39;Positive&#39; Class : 0              
## </code></pre>
<p>Scatterplots with decision boundaries can be plotted using the methods described earlier. First create a grid so we can predict across the full range of our variables V1 and V2:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gridSize &lt;-<span class="st"> </span><span class="dv">150</span> 
v1limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])))
tmpV1 &lt;-<span class="st"> </span><span class="kw">seq</span>(v1limits[<span class="dv">1</span>],v1limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
v2limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])))
tmpV2 &lt;-<span class="st"> </span><span class="kw">seq</span>(v2limits[<span class="dv">1</span>],v2limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(tmpV1,tmpV2)
<span class="kw">names</span>(xgrid) &lt;-<span class="st"> </span><span class="kw">names</span>(xtrain)</code></pre></div>
<p>Predict values of all elements of grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1grid &lt;-<span class="st"> </span><span class="kw">predict</span>(knnFit, xgrid)
V3 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1grid))
xgrid &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgrid, V3)</code></pre></div>
<p>Plot</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)
point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)
point_size =<span class="st"> </span><span class="dv">2</span>

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtrain, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytrain<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtest, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytest<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="fl">0.5</span>, <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simDataBinClassDecisionBoundaryK83"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK83-1.png" alt="Binary classification of the simulated training and test sets with _k_=83." width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK83-2.png" alt="Binary classification of the simulated training and test sets with _k_=83." width="50%" />
<p class="caption">
Figure 5.8: Binary classification of the simulated training and test sets with <em>k</em>=83.
</p>
</div>
</div>
<div id="data-pre-processing" class="section level3">
<h3><span class="header-section-number">5.2.7</span> Data pre-processing</h3>
<div id="cell-segmentation-data-set" class="section level4">
<h4><span class="header-section-number">5.2.7.1</span> Cell segmentation data set</h4>
<p>Pre-processing will be demonstrated using the cell segmentation data of <span class="citation">(Hill et al. <a href="#ref-Hill2007">2007</a>)</span></p>
<div class="figure" style="text-align: center"><span id="fig:imageSegmentationHCS"></span>
<img src="images/Hill_2007_cell_segmentation.jpg" alt="Image segmentation in high content screening. Images **b** and **c** are examples of well-segmented cells; **d** and **e** show poor-segmentation. Source: Hill(2007) https://doi.org/10.1186/1471-2105-8-340" width="75%" />
<p class="caption">
Figure 5.9: Image segmentation in high content screening. Images <strong>b</strong> and <strong>c</strong> are examples of well-segmented cells; <strong>d</strong> and <strong>e</strong> show poor-segmentation. Source: Hill(2007) <a href="https://doi.org/10.1186/1471-2105-8-340" class="uri">https://doi.org/10.1186/1471-2105-8-340</a>
</p>
</div>
<p>This data set is one of several included in <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">data</span>(segmentationData)
<span class="kw">str</span>(segmentationData)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    2019 obs. of  61 variables:
##  $ Cell                   : int  207827637 207932307 207932463 207932470 207932455 207827656 207827659 207827661 207932479 207932480 ...
##  $ Case                   : Factor w/ 2 levels &quot;Test&quot;,&quot;Train&quot;: 1 2 2 2 1 1 1 1 1 1 ...
##  $ Class                  : Factor w/ 2 levels &quot;PS&quot;,&quot;WS&quot;: 1 1 2 1 1 2 2 1 2 2 ...
##  $ AngleCh1               : num  143.25 133.75 106.65 69.15 2.89 ...
##  $ AreaCh1                : int  185 819 431 298 285 172 177 251 495 384 ...
##  $ AvgIntenCh1            : num  15.7 31.9 28 19.5 24.3 ...
##  $ AvgIntenCh2            : num  4.95 206.88 116.32 102.29 112.42 ...
##  $ AvgIntenCh3            : num  9.55 69.92 63.94 28.22 20.47 ...
##  $ AvgIntenCh4            : num  2.21 164.15 106.7 31.03 40.58 ...
##  $ ConvexHullAreaRatioCh1 : num  1.12 1.26 1.05 1.2 1.11 ...
##  $ ConvexHullPerimRatioCh1: num  0.92 0.797 0.935 0.866 0.957 ...
##  $ DiffIntenDensityCh1    : num  29.5 31.9 32.5 26.7 31.6 ...
##  $ DiffIntenDensityCh3    : num  13.8 43.1 36 22.9 21.7 ...
##  $ DiffIntenDensityCh4    : num  6.83 79.31 51.36 26.39 25.03 ...
##  $ EntropyIntenCh1        : num  4.97 6.09 5.88 5.42 5.66 ...
##  $ EntropyIntenCh3        : num  4.37 6.64 6.68 5.44 5.29 ...
##  $ EntropyIntenCh4        : num  2.72 7.88 7.14 5.78 5.24 ...
##  $ EqCircDiamCh1          : num  15.4 32.3 23.4 19.5 19.1 ...
##  $ EqEllipseLWRCh1        : num  3.06 1.56 1.38 3.39 2.74 ...
##  $ EqEllipseOblateVolCh1  : num  337 2233 802 725 608 ...
##  $ EqEllipseProlateVolCh1 : num  110 1433 583 214 222 ...
##  $ EqSphereAreaCh1        : num  742 3279 1727 1195 1140 ...
##  $ EqSphereVolCh1         : num  1901 17654 6751 3884 3621 ...
##  $ FiberAlign2Ch3         : num  1 1.49 1.3 1.22 1.49 ...
##  $ FiberAlign2Ch4         : num  1 1.35 1.52 1.73 1.38 ...
##  $ FiberLengthCh1         : num  27 64.3 21.1 43.1 34.7 ...
##  $ FiberWidthCh1          : num  7.41 13.17 21.14 7.4 8.48 ...
##  $ IntenCoocASMCh3        : num  0.01118 0.02805 0.00686 0.03096 0.02277 ...
##  $ IntenCoocASMCh4        : num  0.05045 0.01259 0.00614 0.01103 0.07969 ...
##  $ IntenCoocContrastCh3   : num  40.75 8.23 14.45 7.3 15.85 ...
##  $ IntenCoocContrastCh4   : num  13.9 6.98 16.7 13.39 3.54 ...
##  $ IntenCoocEntropyCh3    : num  7.2 6.82 7.58 6.31 6.78 ...
##  $ IntenCoocEntropyCh4    : num  5.25 7.1 7.67 7.2 5.5 ...
##  $ IntenCoocMaxCh3        : num  0.0774 0.1532 0.0284 0.1628 0.1274 ...
##  $ IntenCoocMaxCh4        : num  0.172 0.0739 0.0232 0.0775 0.2785 ...
##  $ KurtIntenCh1           : num  -0.6567 -0.2488 -0.2935 0.6259 0.0421 ...
##  $ KurtIntenCh3           : num  -0.608 -0.331 1.051 0.128 0.952 ...
##  $ KurtIntenCh4           : num  0.726 -0.265 0.151 -0.347 -0.195 ...
##  $ LengthCh1              : num  26.2 47.2 28.1 37.9 36 ...
##  $ NeighborAvgDistCh1     : num  370 174 158 206 205 ...
##  $ NeighborMinDistCh1     : num  99.1 30.1 34.9 33.1 27 ...
##  $ NeighborVarDistCh1     : num  128 81.4 90.4 116.9 111 ...
##  $ PerimCh1               : num  68.8 154.9 84.6 101.1 86.5 ...
##  $ ShapeBFRCh1            : num  0.665 0.54 0.724 0.589 0.6 ...
##  $ ShapeLWRCh1            : num  2.46 1.47 1.33 2.83 2.73 ...
##  $ ShapeP2ACh1            : num  1.88 2.26 1.27 2.55 2.02 ...
##  $ SkewIntenCh1           : num  0.455 0.399 0.472 0.882 0.517 ...
##  $ SkewIntenCh3           : num  0.46 0.62 0.971 1 1.177 ...
##  $ SkewIntenCh4           : num  1.233 0.527 0.325 0.604 0.926 ...
##  $ SpotFiberCountCh3      : int  1 4 2 4 1 1 0 2 1 1 ...
##  $ SpotFiberCountCh4      : num  5 12 7 8 8 5 5 8 12 8 ...
##  $ TotalIntenCh1          : int  2781 24964 11552 5545 6603 53779 43950 4401 7593 6512 ...
##  $ TotalIntenCh2          : num  701 160998 47511 28870 30306 ...
##  $ TotalIntenCh3          : int  1690 54675 26344 8042 5569 21234 20929 4136 6488 7503 ...
##  $ TotalIntenCh4          : int  392 128368 43959 8843 11037 57231 46187 373 24325 23162 ...
##  $ VarIntenCh1            : num  12.5 18.8 17.3 13.8 15.4 ...
##  $ VarIntenCh3            : num  7.61 56.72 37.67 30.01 20.5 ...
##  $ VarIntenCh4            : num  2.71 118.39 49.47 24.75 45.45 ...
##  $ WidthCh1               : num  10.6 32.2 21.2 13.4 13.2 ...
##  $ XCentroid              : int  42 215 371 487 283 191 180 373 236 303 ...
##  $ YCentroid              : int  14 347 252 295 159 127 138 181 467 468 ...</code></pre>
<p>The first column of <strong>segmentationData</strong> is a unique identifier for each cell and the second column is a factor indicating how the observations were characterized into training and test sets in the original study; these two variables are irrelevant for the purposes of this demonstration and so can be discarded.</p>
<p>The third column <em>Case</em> contains the class labels: <em>PS</em> (poorly-segmented) and <em>WS</em> (well-segmented). Columns 4-61 are the 58 measurements available to be used as predictors. Let’s put the class labels in a vector and the predictors in their own data.frame.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">segClass &lt;-<span class="st"> </span>segmentationData$Class
segData &lt;-<span class="st"> </span>segmentationData[,<span class="dv">4</span>:<span class="dv">61</span>]</code></pre></div>
</div>
<div id="data-splitting" class="section level4">
<h4><span class="header-section-number">5.2.7.2</span> Data splitting</h4>
<p>The first step in the analysis is to partition the data into training and test sets, using the <strong>createDataPartition</strong> function in <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
trainIndex &lt;-<span class="st"> </span><span class="kw">createDataPartition</span>(<span class="dt">y=</span>segClass, <span class="dt">times=</span><span class="dv">1</span>, <span class="dt">p=</span><span class="fl">0.5</span>, <span class="dt">list=</span>F)
segDataTrain &lt;-<span class="st"> </span>segData[trainIndex,]
segDataTest &lt;-<span class="st"> </span>segData[-trainIndex,]
segClassTrain &lt;-<span class="st"> </span>segClass[trainIndex]
segClassTest &lt;-<span class="st"> </span>segClass[-trainIndex]</code></pre></div>
<p>This results in balanced class distributions within the splits:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(segClassTrain)</code></pre></div>
<pre><code>##  PS  WS 
## 650 360</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(segClassTest)</code></pre></div>
<pre><code>##  PS  WS 
## 650 359</code></pre>
<p><em><strong>N.B. The test set is set aside for now. It will be used only ONCE, to test the final model.</strong></em></p>
</div>
<div id="removal-of-zero-and-near-zero-variance-predictors" class="section level4">
<h4><span class="header-section-number">5.2.7.3</span> Removal of zero and near zero-variance predictors</h4>
<p>The function <strong>nearZeroVar</strong> identifies predictors that have one unique value. It also diagnoses predictors having both of the following characteristics:</p>
<ul>
<li>very few unique values relative to the number of samples</li>
<li>the ratio of the frequency of the most common value to the frequency of the 2nd most common value is large.</li>
</ul>
<p>Such <em>zero and near zero-variance predictors</em> have a deleterious impact on modelling and may lead to unstable fits.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">nzv &lt;-<span class="st"> </span><span class="kw">nearZeroVar</span>(segDataTrain, <span class="dt">saveMetrics=</span>T)
nzv</code></pre></div>
<pre><code>##                         freqRatio percentUnique zeroVar   nzv
## AngleCh1                 1.000000    100.000000   FALSE FALSE
## AreaCh1                  1.083333     37.326733   FALSE FALSE
## AvgIntenCh1              1.000000    100.000000   FALSE FALSE
## AvgIntenCh2              3.000000     99.801980   FALSE FALSE
## AvgIntenCh3              1.000000    100.000000   FALSE FALSE
## AvgIntenCh4              2.000000     99.900990   FALSE FALSE
## ConvexHullAreaRatioCh1   1.000000     98.910891   FALSE FALSE
## ConvexHullPerimRatioCh1  1.000000    100.000000   FALSE FALSE
## DiffIntenDensityCh1      1.000000    100.000000   FALSE FALSE
## DiffIntenDensityCh3      1.000000    100.000000   FALSE FALSE
## DiffIntenDensityCh4      1.000000    100.000000   FALSE FALSE
## EntropyIntenCh1          1.000000    100.000000   FALSE FALSE
## EntropyIntenCh3          1.000000    100.000000   FALSE FALSE
## EntropyIntenCh4          1.000000    100.000000   FALSE FALSE
## EqCircDiamCh1            1.083333     37.326733   FALSE FALSE
## EqEllipseLWRCh1          1.000000    100.000000   FALSE FALSE
## EqEllipseOblateVolCh1    1.000000    100.000000   FALSE FALSE
## EqEllipseProlateVolCh1   1.000000    100.000000   FALSE FALSE
## EqSphereAreaCh1          1.083333     37.326733   FALSE FALSE
## EqSphereVolCh1           1.083333     37.326733   FALSE FALSE
## FiberAlign2Ch3           1.304348     94.950495   FALSE FALSE
## FiberAlign2Ch4           7.285714     94.455446   FALSE FALSE
## FiberLengthCh1           1.000000     95.841584   FALSE FALSE
## FiberWidthCh1            1.000000     95.841584   FALSE FALSE
## IntenCoocASMCh3          1.000000    100.000000   FALSE FALSE
## IntenCoocASMCh4          1.000000    100.000000   FALSE FALSE
## IntenCoocContrastCh3     1.000000    100.000000   FALSE FALSE
## IntenCoocContrastCh4     1.000000    100.000000   FALSE FALSE
## IntenCoocEntropyCh3      1.000000    100.000000   FALSE FALSE
## IntenCoocEntropyCh4      1.000000    100.000000   FALSE FALSE
## IntenCoocMaxCh3          1.250000     94.158416   FALSE FALSE
## IntenCoocMaxCh4          1.250000     94.356436   FALSE FALSE
## KurtIntenCh1             1.000000    100.000000   FALSE FALSE
## KurtIntenCh3             1.000000    100.000000   FALSE FALSE
## KurtIntenCh4             1.000000    100.000000   FALSE FALSE
## LengthCh1                1.000000    100.000000   FALSE FALSE
## NeighborAvgDistCh1       1.000000    100.000000   FALSE FALSE
## NeighborMinDistCh1       1.166667     41.089109   FALSE FALSE
## NeighborVarDistCh1       1.000000    100.000000   FALSE FALSE
## PerimCh1                 1.000000     63.762376   FALSE FALSE
## ShapeBFRCh1              1.000000    100.000000   FALSE FALSE
## ShapeLWRCh1              1.000000    100.000000   FALSE FALSE
## ShapeP2ACh1              1.000000     99.801980   FALSE FALSE
## SkewIntenCh1             1.000000    100.000000   FALSE FALSE
## SkewIntenCh3             1.000000    100.000000   FALSE FALSE
## SkewIntenCh4             1.000000    100.000000   FALSE FALSE
## SpotFiberCountCh3        1.212000      1.287129   FALSE FALSE
## SpotFiberCountCh4        1.152778      3.267327   FALSE FALSE
## TotalIntenCh1            1.000000     98.712871   FALSE FALSE
## TotalIntenCh2            1.500000     99.009901   FALSE FALSE
## TotalIntenCh3            1.000000     99.108911   FALSE FALSE
## TotalIntenCh4            1.000000     99.603960   FALSE FALSE
## VarIntenCh1              1.000000    100.000000   FALSE FALSE
## VarIntenCh3              1.000000    100.000000   FALSE FALSE
## VarIntenCh4              1.000000    100.000000   FALSE FALSE
## WidthCh1                 1.000000    100.000000   FALSE FALSE
## XCentroid                1.111111     41.584158   FALSE FALSE
## YCentroid                1.000000     35.742574   FALSE FALSE</code></pre>
</div>
<div id="centring-and-scaling" class="section level4">
<h4><span class="header-section-number">5.2.7.4</span> Centring and scaling</h4>
<p>The variables in this data set are on different scales, for example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(segDataTrain$IntenCoocASMCh4)</code></pre></div>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## 0.004874 0.017250 0.049460 0.101600 0.121200 0.867800</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(segDataTrain$TotalIntenCh2)</code></pre></div>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       1   15850   49650   53140   72300  362500</code></pre>
<p>In this situation it is important to centre and scale each predictor. A predictor variable is centered by subtracting the mean of the predictor from each value. To scale a predictor variable, each value is divided by its standard deviation. After centring and scaling the predictor variable has a mean of 0 and a standard deviation of 1. Centring and scaling will be peformed within the cross-validation process.</p>
</div>
<div id="resolving-skewness" class="section level4">
<h4><span class="header-section-number">5.2.7.5</span> Resolving skewness</h4>
<p>Many of the predictors in the segmentation data set exhibit skewness, <em>i.e.</em> the distribution of their values is asymmetric, for example:</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qplot</span>(segDataTrain$IntenCoocASMCh3, <span class="dt">binwidth=</span><span class="fl">0.1</span>) +<span class="st"> </span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">&quot;IntenCoocASMCh3&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>()</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:segDataSkewness"></span>
<img src="04-nearest-neighbours_files/figure-html/segDataSkewness-1.png" alt="Example of a predictor from the segmentation data set showing skewness." width="75%" />
<p class="caption">
Figure 5.10: Example of a predictor from the segmentation data set showing skewness.
</p>
</div>
<p><a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> provides various methods for transforming skewed variables to normality, including the Box-Cox (<span class="citation">Box and Cox (<a href="#ref-BoxCox">1964</a>)</span>) and Yeo-Johnson (<span class="citation">Yeo and Johnson (<a href="#ref-YeoJohnson">2000</a>)</span>) transformations.</p>
</div>
<div id="removal-of-correlated-predictors" class="section level4">
<h4><span class="header-section-number">5.2.7.6</span> Removal of correlated predictors</h4>
<p>Many of the variables in the segmentation data set are highly correlated.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrplot)
corMat &lt;-<span class="st"> </span><span class="kw">cor</span>(segDataTrain)
<span class="kw">corrplot</span>(corMat, <span class="dt">order=</span><span class="st">&quot;hclust&quot;</span>, <span class="dt">tl.cex=</span><span class="fl">0.4</span>)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:segDataCorrelogram"></span>
<img src="04-nearest-neighbours_files/figure-html/segDataCorrelogram-1.png" alt="Correlogram of the segmentation data set." width="75%" />
<p class="caption">
Figure 5.11: Correlogram of the segmentation data set.
</p>
</div>
<p>The <strong>preProcess</strong> function in <a href="http://cran.r-project.org/web/packages/caret/index.html">caret</a> has an option, <strong>corr</strong> to remove highly correlated variables. It considers the absolute values of pair-wise correlations. If two variables are highly correlated, <strong>preProcess</strong> looks at the mean absolute correlation of each variable and removes the variable with the largest mean absolute correlation.</p>
<!--

```r
highCorr <- findCorrelation(corMat, cutoff=0.75)
length(highCorr)
```

```
## [1] 31
```

```r
segDataTrain <- segDataTrain[,-highCorr]
```
-->
</div>
<div id="dimensionality-reduction" class="section level4">
<h4><span class="header-section-number">5.2.7.7</span> Dimensionality reduction</h4>
<p>In the case of data-sets comprised of many highly correlated variables, an alternative to removing correlated predictors is the transformation of the entire data set to a lower dimensional space, using a technique such as principal component analysis (PCA). Methods for dimensionality reduction will be explored in chapter <a href="nearest-neighbours.html#dimensionality-reduction">5.2.7.7</a>.</p>
</div>
</div>
<div id="feature-selection" class="section level3">
<h3><span class="header-section-number">5.2.8</span> Feature selection</h3>
<div id="cross-validated-performance-without-feature-selection" class="section level4">
<h4><span class="header-section-number">5.2.8.1</span> Cross-validated performance without feature selection</h4>
<p>Generate a list of seeds.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">42</span>)
seeds &lt;-<span class="st"> </span><span class="kw">vector</span>(<span class="dt">mode =</span> <span class="st">&quot;list&quot;</span>, <span class="dt">length =</span> <span class="dv">101</span>)
for(i in <span class="dv">1</span>:<span class="dv">100</span>) seeds[[i]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>, <span class="dv">50</span>)
seeds[[<span class="dv">101</span>]] &lt;-<span class="st"> </span><span class="kw">sample.int</span>(<span class="dv">1000</span>,<span class="dv">1</span>)</code></pre></div>
<p>Create a list of computational options for resampling.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tc &lt;-<span class="st"> </span><span class="kw">trainControl</span>(<span class="dt">method=</span><span class="st">&quot;repeatedcv&quot;</span>,
                   <span class="dt">number =</span> <span class="dv">10</span>,
                   <span class="dt">repeats =</span> <span class="dv">10</span>,
                   <span class="co">#preProcOptions=list(cutoff=0.75),</span>
                   <span class="dt">seeds =</span> seeds)</code></pre></div>
<p>Create a grid of values of <em>k</em> for evaluation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">tuneParam &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">k=</span><span class="kw">seq</span>(<span class="dv">5</span>,<span class="dv">500</span>,<span class="dv">10</span>))</code></pre></div>
<p>To deal with the issues of scaling, skewness and highly correlated predictors identified earlier, we need to pre-process the data. We will use the Yeo-Johnson transformation to reduce skewness, because it can deal with the zero values present in some of the predictors. Ideally the pre-processing procedures would be performed within each cross-validation loop, using the following command:</p>
<pre><code>knnFit &lt;- train(segDataTrain, segClassTrain, 
                method=&quot;knn&quot;,
                preProcess = c(&quot;YeoJohnson&quot;, &quot;center&quot;, &quot;scale&quot;, &quot;corr&quot;),
                tuneGrid=tuneParam,
                trControl=tc)</code></pre>
<p>However, this is time-consuming, so for the purposes of this demonstration we will pre-process the entire training data-set before proceeding with training and cross-validation.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">transformations &lt;-<span class="st"> </span><span class="kw">preProcess</span>(segDataTrain, 
                              <span class="dt">method=</span><span class="kw">c</span>(<span class="st">&quot;YeoJohnson&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>, <span class="st">&quot;corr&quot;</span>),
                              <span class="dt">cutoff=</span><span class="fl">0.75</span>)</code></pre></div>
<pre><code>## Warning in preProcess.default(segDataTrain, method = c(&quot;YeoJohnson&quot;, &quot;center&quot;, : correlation matrix could not be computed:
##  1correlation matrix could not be computed:
##  0.0399656535611045correlation matrix could not be computed:
##  -0.01401782480921correlation matrix could not be computed:
##  -0.0187837923574354correlation matrix could not be computed:
##  -0.0469667994444209correlation matrix could not be computed:
##  0.0664826691953978correlation matrix could not be computed:
##  0.0223565721289271correlation matrix could not be computed:
##  -0.00247772318389345correlation matrix could not be computed:
##  0.0158846522500652correlation matrix could not be computed:
##  0.00830273006378069correlation matrix could not be computed:
##  -0.0319505487556941correlation matrix could not be computed:
##  -0.00277246165172196correlation matrix could not be computed:
##  -0.0322190133174334correlation matrix could not be computed:
##  0.0468199666151097correlation matrix could not be computed:
##  0.0299932287943623correlation matrix could not be computed:
##  0.0918419175907328correlation matrix could not be computed:
##  0.0558685742029219correlation matrix could not be computed:
##  0.0133466796175919correlation matrix could not be computed:
##  -0.0213670544225047correlation matrix could not be computed:
##  -0.0232262680891569correlation matrix could not be computed:
##  0.0301963741169544correlation matrix could not be computed:
##  -0.0280243705254885correlation matrix could not be computed:
##  0.0244036442525391correlation matrix could not be computed:
##  0.024780655197781correlation matrix could not be computed:
##  -0.0407219843256941correlation matrix could not be computed:
##  -0.0533602991100316correlation matrix could not be computed:
##  -0.0124087897538399correlation matrix could not be computed:
##  0.0399656535611045correlation matrix could not be computed:
##  1correlation matrix could not be computed:
##  0.318091534789045correlation matrix could not be computed:
##  -0.470231685498673correlation matrix could not be computed:
##  0.055382594927069correlation matrix could not be computed:
##  0.035598844977434correlation matrix could not be computed:
##  0.460443788379738correlation matrix could not be computed:
##  -0.0558076756121481correlation matrix could not be computed:
##  -0.277694139449617correlation matrix could not be computed:
##  -0.0293761742332339correlation matrix could not be computed:
##  0.249664372997467correlation matrix could not be computed:
##  -0.16917572282694correlation matrix could not be computed:
##  0.0322471147419416correlation matrix could not be computed:
##  -0.101256549239793correlation matrix could not be computed:
##  0.0638580820509881correlation matrix could not be computed:
##  -0.177214629644544correlation matrix could not be computed:
##  0.686157099237658correlation matrix could not be computed:
##  -0.0546898613425807correlation matrix could not be computed:
##  -0.407902478526775correlation matrix could not be computed:
##  -0.352556540885133correlation matrix could not be computed:
##  0.162203563532878correlation matrix could not be computed:
##  0.258689286813188correlation matrix could not be computed:
##  0.0627589823761482correlation matrix could not be computed:
##  0.200696051349995correlation matrix could not be computed:
##  -0.587953133695097correlation matrix could not be computed:
##  -0.0414660401973205correlation matrix could not be computed:
##  -0.0410869166878906correlation matrix could not be computed:
##  -0.01401782480921correlation matrix could not be computed:
##  0.318091534789045correlation matrix could not be computed:
##  1correlation matrix could not be computed:
##  0.0401164967247637correlation matrix could not be computed:
##  -0.0745419614292381correlation matrix could not be computed:
##  -0.0184566818541376correlation matrix could not be computed:
##  0.447124911392658correlation matrix could not be computed:
##  -0.105101207667655correlation matrix could not be computed:
##  -0.362983563583841correlation matrix could not be computed:
##  -0.172535893520489correlation matrix could not be computed:
##  0.205784228528686correlation matrix could not be computed:
##  -0.0903719189552464correlation matrix could not be computed:
##  0.113109221450124correlation matrix could not be computed:
##  -0.0819609323500842correlation matrix could not be computed:
##  -0.00178251160156081correlation matrix could not be computed:
##  -0.030074256266626correlation matrix could not be computed:
##  0.244748161385148correlation matrix could not be computed:
##  -0.107471608862106correlation matrix could not be computed:
##  -0.26334864008133correlation matrix could not be computed:
##  0.0466343302834301correlation matrix could not be computed:
##  0.598015583208681correlation matrix could not be computed:
##  0.7494069213488correlation matrix could not be computed:
##  0.417970438924964correlation matrix could not be computed:
##  0.422081633586389correlation matrix could not be computed:
##  0.085200962920973correlation matrix could not be computed:
##  -0.0738112201896686correlation matrix could not be computed:
##  -0.111061084309888correlation matrix could not be computed:
##  -0.0187837923574354correlation matrix could not be computed:
##  -0.470231685498673correlation matrix could not be computed:
##  0.0401164967247637correlation matrix could not be computed:
##  1correlation matrix could not be computed:
##  -0.157347704337933correlation matrix could not be computed:
##  -0.106522474751295correlation matrix could not be computed:
##  0.165008370707857correlation matrix could not be computed:
##  -0.0195849997833145correlation matrix could not be computed:
##  0.0951362164884144correlation matrix could not be computed:
##  0.0535153101671921correlation matrix could not be computed:
##  -0.0154835939577822correlation matrix could not be computed:
##  0.0589554920926267correlation matrix could not be computed:
##  0.0549635093457417correlation matrix could not be computed:
##  0.0670739356526949correlation matrix could not be computed:
##  -0.0180392602603926correlation matrix could not be computed:
##  0.213254628529682correlation matrix could not be computed:
##  -0.2623111448356correlation matrix could not be computed:
##  0.138260790696054correlation matrix could not be computed:
##  0.120150938779494correlation matrix could not be computed:
##  0.634679620706067correlation matrix could not be computed:
##  0.161942779298141correlation matrix could not be computed:
##  -0.00487468617059258correlation matrix could not be computed:
##  -0.0970325280281872correlation matrix could not be computed:
##  -0.0970548286549872correlation matrix could not be computed:
##  0.735251459967222correlation matrix could not be computed:
##  -0.0233788546169213correlation matrix could not be computed:
##  -0.0197966674798393correlation matrix could not be computed:
##  -0.0469667994444209correlation matrix could not be computed:
##  0.055382594927069correlation matrix could not be computed:
##  -0.0745419614292381correlation matrix could not be computed:
##  -0.157347704337933correlation matrix could not be computed:
##  1correlation matrix could not be computed:
##  0.199810214471052correlation matrix could not be computed:
##  -0.135247528988592correlation matrix could not be computed:
##  0.119795047970913correlation matrix could not be computed:
##  -0.0189278632588467correlation matrix could not be computed:
##  -0.0918743581123067correlation matrix could not be computed:
##  -9.74037259693129e-05correlation matrix could not be computed:
##  -0.0366381613541393correlation matrix could not be computed:
##  -0.00771253996116994correlation matrix could not be computed:
##  0.0639324730558175correlation matrix could not be computed:
##  -0.0194537384769147correlation matrix could not be computed:
##  -0.0662474651007067correlation matrix could not be computed:
##  -0.0609953763465428correlation matrix could not be computed:
##  0.0578353480403023correlation matrix could not be computed:
##  -0.012630413972657correlation matrix could not be computed:
##  -0.0426034016046167correlation matrix could not be computed:
##  -0.0901980312669627correlation matrix could not be computed:
##  -0.0332098741188583correlation matrix could not be computed:
##  -0.00689801411436046correlation matrix could not be computed:
##  0.00721523296110747correlation matrix could not be computed:
##  -0.162519164311792correlation matrix could not be computed:
##  0.0294872457484293correlation matrix could not be computed:
##  -0.00655744260658358correlation matrix c</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">segDataTrain &lt;-<span class="st"> </span><span class="kw">predict</span>(transformations, segDataTrain)

<span class="kw">str</span>(segDataTrain)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    1010 obs. of  27 variables:
##  $ AngleCh1               : num  1.045 0.873 -0.376 -0.994 1.586 ...
##  $ ConvexHullPerimRatioCh1: num  0.12 -1.34 -0.68 1.81 1.56 ...
##  $ EntropyIntenCh1        : num  -2.443 -0.671 -1.688 0.554 0.425 ...
##  $ EqEllipseOblateVolCh1  : num  -0.414 1.693 0.711 -1.817 -1.667 ...
##  $ FiberAlign2Ch3         : num  -1.8124 0.0933 -0.9679 -0.6188 -0.8721 ...
##  $ FiberAlign2Ch4         : num  -1.729 -0.331 1.255 -0.291 0.463 ...
##  $ FiberWidthCh1          : num  -0.776 0.878 -0.779 0.712 0.758 ...
##  $ IntenCoocASMCh4        : num  -0.181 -1.036 -1.079 -1.131 -1.155 ...
##  $ IntenCoocContrastCh3   : num  2.4777 0.0604 -0.0816 0.0634 0.6386 ...
##  $ IntenCoocContrastCh4   : num  1.101 0.127 1.046 0.602 1.445 ...
##  $ IntenCoocMaxCh3        : num  -0.815 -0.232 -0.168 -1.366 -1.37 ...
##  $ KurtIntenCh1           : num  -0.97 -0.26 0.562 -0.187 0.296 ...
##  $ KurtIntenCh3           : num  -1.506 -1.133 -0.672 -1.908 -1.491 ...
##  $ KurtIntenCh4           : num  0.68399 -0.00329 -0.09737 -0.1679 -0.79044 ...
##  $ NeighborAvgDistCh1     : num  2.5376 -1.4791 -0.5357 0.1062 0.0663 ...
##  $ NeighborMinDistCh1     : num  3.286 0.289 0.557 -1.679 -1.679 ...
##  $ ShapeBFRCh1            : num  0.648 -0.609 -0.141 0.89 1.593 ...
##  $ ShapeLWRCh1            : num  1.23 -0.351 1.525 -1.832 -1.717 ...
##  $ SkewIntenCh1           : num  -0.213 -0.297 0.384 -2.41 -2.678 ...
##  $ SpotFiberCountCh3      : num  -0.366 1.275 1.275 -0.366 -1.573 ...
##  $ TotalIntenCh2          : num  -1.701 1.682 -0.233 1.107 1.019 ...
##  $ VarIntenCh1            : num  -2.118 -1.346 -1.917 1.062 0.856 ...
##  $ VarIntenCh3            : num  -1.9155 -0.1836 -0.8001 0.0478 -0.3659 ...
##  $ VarIntenCh4            : num  -2.304 0.332 -1.092 0.843 0.387 ...
##  $ WidthCh1               : num  -1.626 1.845 -0.718 -0.188 -0.333 ...
##  $ XCentroid              : num  -1.647 -0.241 1.484 -0.412 -0.492 ...
##  $ YCentroid              : num  -2.098 1.447 1.118 -0.251 -0.138 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knnFit &lt;-<span class="st"> </span><span class="kw">train</span>(segDataTrain, segClassTrain, 
                <span class="dt">method=</span><span class="st">&quot;knn&quot;</span>,
                <span class="dt">tuneGrid=</span>tuneParam,
                <span class="dt">trControl=</span>tc)
knnFit</code></pre></div>
<pre><code>## k-Nearest Neighbors 
## 
## 1010 samples
##   27 predictor
##    2 classes: &#39;PS&#39;, &#39;WS&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold, repeated 10 times) 
## Summary of sample sizes: 909, 909, 909, 909, 909, 909, ... 
## Resampling results across tuning parameters:
## 
##   k    Accuracy   Kappa    
##     5  0.7844554  0.5355021
##    15  0.8063366  0.5858762
##    25  0.8071287  0.5860760
##    35  0.8083168  0.5885382
##    45  0.8043564  0.5777211
##    55  0.8035644  0.5750911
##    65  0.8024752  0.5728325
##    75  0.7998020  0.5672303
##    85  0.7979208  0.5622155
##    95  0.7993069  0.5651879
##   105  0.7975248  0.5609526
##   115  0.8006931  0.5682550
##   125  0.8002970  0.5666480
##   135  0.8020792  0.5695054
##   145  0.8040594  0.5738518
##   155  0.8011881  0.5663541
##   165  0.8005941  0.5640214
##   175  0.7985149  0.5589314
##   185  0.7967327  0.5546004
##   195  0.7971287  0.5555960
##   205  0.7982178  0.5573946
##   215  0.7984158  0.5568846
##   225  0.7962376  0.5516424
##   235  0.7990099  0.5564413
##   245  0.7976238  0.5525981
##   255  0.7985149  0.5540280
##   265  0.7976238  0.5513190
##   275  0.7967327  0.5486973
##   285  0.7979208  0.5508728
##   295  0.8001980  0.5546946
##   305  0.8003960  0.5539966
##   315  0.8000990  0.5522283
##   325  0.7980198  0.5464552
##   335  0.7978218  0.5453697
##   345  0.7966337  0.5415395
##   355  0.7957426  0.5383734
##   365  0.7937624  0.5328159
##   375  0.7913861  0.5258660
##   385  0.7892079  0.5194483
##   395  0.7888119  0.5171381
##   405  0.7872277  0.5120041
##   415  0.7858416  0.5072424
##   425  0.7817822  0.4949439
##   435  0.7792079  0.4870700
##   445  0.7742574  0.4725344
##   455  0.7642574  0.4426034
##   465  0.7605941  0.4275338
##   475  0.7552475  0.4081788
##   485  0.7481188  0.3841416
##   495  0.7381188  0.3521300
## 
## Accuracy was used to select the optimal model using  the largest value.
## The final value used for the model was k = 35.</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(knnFit)</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:cvAccuracySegDataHighCorRem"></span>
<img src="04-nearest-neighbours_files/figure-html/cvAccuracySegDataHighCorRem-1.png" alt="Accuracy (repeated cross-validation) as a function of neighbourhood size for the segmentation training data with highly correlated predictors removed." width="100%" />
<p class="caption">
Figure 5.12: Accuracy (repeated cross-validation) as a function of neighbourhood size for the segmentation training data with highly correlated predictors removed.
</p>
</div>
</div>
<div id="methods" class="section level4">
<h4><span class="header-section-number">5.2.8.2</span> Methods</h4>
</div>
<div id="univariate-t-test-filter" class="section level4">
<h4><span class="header-section-number">5.2.8.3</span> Univariate (<em>t</em>-test) filter</h4>
</div>
<div id="recursive-feature-elimination" class="section level4">
<h4><span class="header-section-number">5.2.8.4</span> Recursive feature elimination</h4>
</div>
</div>
</div>
<div id="regression" class="section level2">
<h2><span class="header-section-number">5.3</span> Regression</h2>
<!--
## Caret

pre-processing
identification of correlated predictors


Parallel processing with doMC
registerDoMC()
getDoParWorkers()

## Curse of dimensionality
Pre-processing data using dimensionality reduction.

transformation functionality in caret

## Examples

centre1 <- read.csv("data/serum_proteomics/male_centre1.csv")
centre2 <- read.csv("data/serum_proteomics/male_centre2.csv")

c1sub <- centre1[,c(1,5,6,9,10)]
c2sub <- centre2[,c(1,5,6,9,10)]

res <- FNN::knn(c1sub[,2:5], c1sub[,2:5], cl=c1sub$Diagnostic_group, k=1)
table(c1sub$Diagnostic_group, res)

res <- FNN::knn(c1sub[,2:5], c2sub[,2:5], cl=c1sub$Diagnostic_group, k=1)
table(c2sub$Diagnostic_group, res)

bias / variance trade-off

include:
division into training and test set
preprocessing - illustrate with diagram

-->
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">5.4</span> Exercises</h2>
<div id="knnEx1" class="section level3">
<h3><span class="header-section-number">5.4.1</span> Exercise 1</h3>
<p>Classification</p>
<p>Try different methods of feature selection</p>
</div>
<div id="knnEx2" class="section level3">
<h3><span class="header-section-number">5.4.2</span> Exercise 2</h3>
<p>Regression</p>
<p>Alzheimers &amp; gene expression? MMSE and gene expression?</p>
<p>Solutions to exercises can be found in appendix <a href="solutions-nearest-neighbours.html#solutions-nearest-neighbours">D</a>.</p>

</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references">
<div id="ref-Kuhn2013">
<p>Kuhn, Max, and Kjell Johnson. 2013. <em>Applied Predictive Modeling</em>. New York: Springer.</p>
</div>
<div id="ref-Hill2007">
<p>Hill, Andrew A., Peter LaPan, Yizheng Li, and Steve Haney. 2007. “Impact of Image Segmentation on High-Content Screening Data Quality for Sk-Br-3 Cells.” <em>BMC Bioinformatics</em> 8 (1): 340. doi:<a href="https://doi.org/10.1186/1471-2105-8-340">10.1186/1471-2105-8-340</a>.</p>
</div>
<div id="ref-BoxCox">
<p>Box, G. E. P., and D. R. Cox. 1964. “An analysis of transformations (with discussion).” <em>Journal of the Royal Statistical Society B</em> 26: 211–52.</p>
</div>
<div id="ref-YeoJohnson">
<p>Yeo, I. K., and R. Johnson. 2000. “A new family of power transformations to improve normality or symmetry.” <em>Biometrika</em> 87: 954–59.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decision-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bioinformatics-training/intro-machine-learning/edit/master/04-nearest-neighbours.Rmd",
"text": "Edit"
},
"download": ["intro-machine-learning.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
