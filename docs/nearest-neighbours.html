<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>An Introduction to Machine Learning</title>
  <meta name="description" content="Course materials for An Introduction to Machine Learning">
  <meta name="generator" content="bookdown 0.5 and GitBook 2.6.7">

  <meta property="og:title" content="An Introduction to Machine Learning" />
  <meta property="og:type" content="book" />
  
  <meta property="og:image" content="figures/cover_image.png" />
  <meta property="og:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="github-repo" content="bioinformatics-training/intro-machine-learning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="An Introduction to Machine Learning" />
  
  <meta name="twitter:description" content="Course materials for An Introduction to Machine Learning" />
  <meta name="twitter:image" content="figures/cover_image.png" />

<meta name="author" content="Sudhakaran Prabakaran, Matt Wayland and Chris Penfold">


<meta name="date" content="2017-09-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="logistic-regression.html">
<link rel="next" href="decision-trees.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
div.sourceCode { overflow-x: auto; }
table.sourceCode, tr.sourceCode, td.lineNumbers, td.sourceCode {
  margin: 0; padding: 0; vertical-align: baseline; border: none; }
table.sourceCode { width: 100%; line-height: 100%; }
td.lineNumbers { text-align: right; padding-right: 4px; padding-left: 4px; color: #aaaaaa; border-right: 1px solid #aaaaaa; }
td.sourceCode { padding-left: 5px; }
code > span.kw { color: #007020; font-weight: bold; } /* Keyword */
code > span.dt { color: #902000; } /* DataType */
code > span.dv { color: #40a070; } /* DecVal */
code > span.bn { color: #40a070; } /* BaseN */
code > span.fl { color: #40a070; } /* Float */
code > span.ch { color: #4070a0; } /* Char */
code > span.st { color: #4070a0; } /* String */
code > span.co { color: #60a0b0; font-style: italic; } /* Comment */
code > span.ot { color: #007020; } /* Other */
code > span.al { color: #ff0000; font-weight: bold; } /* Alert */
code > span.fu { color: #06287e; } /* Function */
code > span.er { color: #ff0000; font-weight: bold; } /* Error */
code > span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
code > span.cn { color: #880000; } /* Constant */
code > span.sc { color: #4070a0; } /* SpecialChar */
code > span.vs { color: #4070a0; } /* VerbatimString */
code > span.ss { color: #bb6688; } /* SpecialString */
code > span.im { } /* Import */
code > span.va { color: #19177c; } /* Variable */
code > span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code > span.op { color: #666666; } /* Operator */
code > span.bu { } /* BuiltIn */
code > span.ex { } /* Extension */
code > span.pp { color: #bc7a00; } /* Preprocessor */
code > span.at { color: #7d9029; } /* Attribute */
code > span.do { color: #ba2121; font-style: italic; } /* Documentation */
code > span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code > span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code > span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">An Introduction to Machine Learning</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> About the course</a><ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#overview"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#registration"><i class="fa fa-check"></i><b>1.2</b> Registration</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#prerequisites"><i class="fa fa-check"></i><b>1.3</b> Prerequisites</a></li>
<li class="chapter" data-level="1.4" data-path="index.html"><a href="index.html#github"><i class="fa fa-check"></i><b>1.4</b> Github</a></li>
<li class="chapter" data-level="1.5" data-path="index.html"><a href="index.html#license"><i class="fa fa-check"></i><b>1.5</b> License</a></li>
<li class="chapter" data-level="1.6" data-path="index.html"><a href="index.html#contact"><i class="fa fa-check"></i><b>1.6</b> Contact</a></li>
<li class="chapter" data-level="1.7" data-path="index.html"><a href="index.html#colophon"><i class="fa fa-check"></i><b>1.7</b> Colophon</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>2</b> Introduction</a></li>
<li class="chapter" data-level="3" data-path="linear-models.html"><a href="linear-models.html"><i class="fa fa-check"></i><b>3</b> Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="3.1" data-path="linear-models.html"><a href="linear-models.html#exercises"><i class="fa fa-check"></i><b>3.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="logistic-regression.html"><a href="logistic-regression.html"><i class="fa fa-check"></i><b>4</b> Linear and non linear logistic regression</a><ul>
<li class="chapter" data-level="4.1" data-path="logistic-regression.html"><a href="logistic-regression.html#exercises-1"><i class="fa fa-check"></i><b>4.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html"><i class="fa fa-check"></i><b>5</b> Nearest neighbours</a><ul>
<li class="chapter" data-level="5.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#introduction"><i class="fa fa-check"></i><b>5.1</b> Introduction</a></li>
<li class="chapter" data-level="5.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#algorithm"><i class="fa fa-check"></i><b>5.2</b> Algorithm</a><ul>
<li class="chapter" data-level="5.2.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#defining-nearest"><i class="fa fa-check"></i><b>5.2.1</b> Defining nearest</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#classification"><i class="fa fa-check"></i><b>5.3</b> Classification</a></li>
<li class="chapter" data-level="5.4" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#regression"><i class="fa fa-check"></i><b>5.4</b> Regression</a></li>
<li class="chapter" data-level="5.5" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#caret"><i class="fa fa-check"></i><b>5.5</b> Caret</a></li>
<li class="chapter" data-level="5.6" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#example-one"><i class="fa fa-check"></i><b>5.6</b> Example one</a></li>
<li class="chapter" data-level="5.7" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#example-two"><i class="fa fa-check"></i><b>5.7</b> Example two</a></li>
<li class="chapter" data-level="5.8" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#curse-of-dimensionality"><i class="fa fa-check"></i><b>5.8</b> Curse of dimensionality</a></li>
<li class="chapter" data-level="5.9" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#examples"><i class="fa fa-check"></i><b>5.9</b> Examples</a></li>
<li class="chapter" data-level="5.10" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#exercises-2"><i class="fa fa-check"></i><b>5.10</b> Exercises</a><ul>
<li class="chapter" data-level="5.10.1" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx1"><i class="fa fa-check"></i><b>5.10.1</b> Exercise 1</a></li>
<li class="chapter" data-level="5.10.2" data-path="nearest-neighbours.html"><a href="nearest-neighbours.html#knnEx2"><i class="fa fa-check"></i><b>5.10.2</b> Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="decision-trees.html"><a href="decision-trees.html"><i class="fa fa-check"></i><b>6</b> Decision trees and random forests</a><ul>
<li class="chapter" data-level="6.1" data-path="decision-trees.html"><a href="decision-trees.html#exercises-3"><i class="fa fa-check"></i><b>6.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>7</b> Support vector machines</a><ul>
<li class="chapter" data-level="7.1" data-path="svm.html"><a href="svm.html#exercises-4"><i class="fa fa-check"></i><b>7.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="ann.html"><a href="ann.html"><i class="fa fa-check"></i><b>8</b> Artificial neural networks</a><ul>
<li class="chapter" data-level="8.1" data-path="ann.html"><a href="ann.html#exercises-5"><i class="fa fa-check"></i><b>8.1</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html"><i class="fa fa-check"></i><b>9</b> Dimensionality reduction</a><ul>
<li class="chapter" data-level="9.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#linear-dimensionality-reduction"><i class="fa fa-check"></i><b>9.1</b> Linear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.1.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#principle-component-analysis"><i class="fa fa-check"></i><b>9.1.1</b> Principle Component Analysis</a></li>
<li class="chapter" data-level="9.1.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#horeshoe-effect"><i class="fa fa-check"></i><b>9.1.2</b> Horeshoe effect</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#nonlinear-dimensionality-reduction"><i class="fa fa-check"></i><b>9.2</b> Nonlinear Dimensionality Reduction</a><ul>
<li class="chapter" data-level="9.2.1" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#t-sne"><i class="fa fa-check"></i><b>9.2.1</b> t-SNE</a></li>
<li class="chapter" data-level="9.2.2" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#gaussian-process-latent-variable-models"><i class="fa fa-check"></i><b>9.2.2</b> Gaussian Process Latent Variable Models</a></li>
<li class="chapter" data-level="9.2.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#gplvms-with-informative-priors"><i class="fa fa-check"></i><b>9.2.3</b> GPLVMs with informative priors</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="dimensionality-reduction.html"><a href="dimensionality-reduction.html#exercises-6"><i class="fa fa-check"></i><b>9.3</b> Exercises</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="clustering.html"><a href="clustering.html"><i class="fa fa-check"></i><b>10</b> Clustering</a><ul>
<li class="chapter" data-level="10.1" data-path="clustering.html"><a href="clustering.html#introduction-1"><i class="fa fa-check"></i><b>10.1</b> Introduction</a></li>
<li class="chapter" data-level="10.2" data-path="clustering.html"><a href="clustering.html#distance-metrics"><i class="fa fa-check"></i><b>10.2</b> Distance metrics</a><ul>
<li class="chapter" data-level="10.2.1" data-path="clustering.html"><a href="clustering.html#image-segmentation"><i class="fa fa-check"></i><b>10.2.1</b> Image segmentation</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="clustering.html"><a href="clustering.html#hierarchic-agglomerative"><i class="fa fa-check"></i><b>10.3</b> Hierarchic agglomerative</a><ul>
<li class="chapter" data-level="10.3.1" data-path="clustering.html"><a href="clustering.html#linkage-algorithms"><i class="fa fa-check"></i><b>10.3.1</b> Linkage algorithms</a></li>
<li class="chapter" data-level="10.3.2" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets"><i class="fa fa-check"></i><b>10.3.2</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="10.3.3" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues"><i class="fa fa-check"></i><b>10.3.3</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="clustering.html"><a href="clustering.html#k-means"><i class="fa fa-check"></i><b>10.4</b> K-means</a><ul>
<li class="chapter" data-level="10.4.1" data-path="clustering.html"><a href="clustering.html#algorithm-1"><i class="fa fa-check"></i><b>10.4.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.4.2" data-path="clustering.html"><a href="clustering.html#choosing-initial-cluster-centres"><i class="fa fa-check"></i><b>10.4.2</b> Choosing initial cluster centres</a></li>
<li class="chapter" data-level="10.4.3" data-path="clustering.html"><a href="clustering.html#choosingK"><i class="fa fa-check"></i><b>10.4.3</b> Choosing k</a></li>
<li class="chapter" data-level="10.4.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-1"><i class="fa fa-check"></i><b>10.4.4</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="10.4.5" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-1"><i class="fa fa-check"></i><b>10.4.5</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.5" data-path="clustering.html"><a href="clustering.html#dbscan"><i class="fa fa-check"></i><b>10.5</b> DBSCAN</a><ul>
<li class="chapter" data-level="10.5.1" data-path="clustering.html"><a href="clustering.html#algorithm-2"><i class="fa fa-check"></i><b>10.5.1</b> Algorithm</a></li>
<li class="chapter" data-level="10.5.2" data-path="clustering.html"><a href="clustering.html#implementation-in-r"><i class="fa fa-check"></i><b>10.5.2</b> Implementation in R</a></li>
<li class="chapter" data-level="10.5.3" data-path="clustering.html"><a href="clustering.html#choosing-parameters"><i class="fa fa-check"></i><b>10.5.3</b> Choosing parameters</a></li>
<li class="chapter" data-level="10.5.4" data-path="clustering.html"><a href="clustering.html#example-clustering-synthetic-data-sets-2"><i class="fa fa-check"></i><b>10.5.4</b> Example: clustering synthetic data sets</a></li>
<li class="chapter" data-level="10.5.5" data-path="clustering.html"><a href="clustering.html#example-gene-expression-profiling-of-human-tissues-2"><i class="fa fa-check"></i><b>10.5.5</b> Example: gene expression profiling of human tissues</a></li>
</ul></li>
<li class="chapter" data-level="10.6" data-path="clustering.html"><a href="clustering.html#evaluating-cluster-quality"><i class="fa fa-check"></i><b>10.6</b> Evaluating cluster quality</a><ul>
<li class="chapter" data-level="10.6.1" data-path="clustering.html"><a href="clustering.html#silhouetteMethod"><i class="fa fa-check"></i><b>10.6.1</b> Silhouette method</a></li>
<li class="chapter" data-level="10.6.2" data-path="clustering.html"><a href="clustering.html#example---k-means-clustering-of-blobs-data-set"><i class="fa fa-check"></i><b>10.6.2</b> Example - k-means clustering of blobs data set</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="clustering.html"><a href="clustering.html#summary"><i class="fa fa-check"></i><b>10.7</b> Summary</a><ul>
<li class="chapter" data-level="10.7.1" data-path="clustering.html"><a href="clustering.html#applications"><i class="fa fa-check"></i><b>10.7.1</b> Applications</a></li>
<li class="chapter" data-level="10.7.2" data-path="clustering.html"><a href="clustering.html#strengths"><i class="fa fa-check"></i><b>10.7.2</b> Strengths</a></li>
<li class="chapter" data-level="10.7.3" data-path="clustering.html"><a href="clustering.html#limitations"><i class="fa fa-check"></i><b>10.7.3</b> Limitations</a></li>
</ul></li>
<li class="chapter" data-level="10.8" data-path="clustering.html"><a href="clustering.html#exercises-7"><i class="fa fa-check"></i><b>10.8</b> Exercises</a><ul>
<li class="chapter" data-level="10.8.1" data-path="clustering.html"><a href="clustering.html#clusteringEx1"><i class="fa fa-check"></i><b>10.8.1</b> Exercise 1</a></li>
<li class="chapter" data-level="10.8.2" data-path="clustering.html"><a href="clustering.html#clusteringEx2"><i class="fa fa-check"></i><b>10.8.2</b> Exercise 2</a></li>
</ul></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="resources.html"><a href="resources.html"><i class="fa fa-check"></i><b>A</b> Resources</a><ul>
<li class="chapter" data-level="A.1" data-path="resources.html"><a href="resources.html#python"><i class="fa fa-check"></i><b>A.1</b> Python</a></li>
<li class="chapter" data-level="A.2" data-path="resources.html"><a href="resources.html#machine-learning-data-set-repositories"><i class="fa fa-check"></i><b>A.2</b> Machine learning data set repositories</a><ul>
<li class="chapter" data-level="A.2.1" data-path="resources.html"><a href="resources.html#mldata"><i class="fa fa-check"></i><b>A.2.1</b> MLDATA</a></li>
<li class="chapter" data-level="A.2.2" data-path="resources.html"><a href="resources.html#uci-machine-learning-repository"><i class="fa fa-check"></i><b>A.2.2</b> UCI Machine Learning Repository</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="B" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html"><i class="fa fa-check"></i><b>B</b> Solutions ch. 3 - Linear models and matrix algebra</a><ul>
<li class="chapter" data-level="B.1" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#exercise-1"><i class="fa fa-check"></i><b>B.1</b> Exercise 1</a></li>
<li class="chapter" data-level="B.2" data-path="solutions-linear-models.html"><a href="solutions-linear-models.html#exercise-2"><i class="fa fa-check"></i><b>B.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="C" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html"><i class="fa fa-check"></i><b>C</b> Solutions ch. 4 - Linear and non-linear logistic regression</a><ul>
<li class="chapter" data-level="C.1" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html#exercise-1-1"><i class="fa fa-check"></i><b>C.1</b> Exercise 1</a></li>
<li class="chapter" data-level="C.2" data-path="solutions-logistic-regression.html"><a href="solutions-logistic-regression.html#exercise-2-1"><i class="fa fa-check"></i><b>C.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="D" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html"><i class="fa fa-check"></i><b>D</b> Solutions ch. 5 - Nearest neighbours</a><ul>
<li class="chapter" data-level="D.1" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-1-2"><i class="fa fa-check"></i><b>D.1</b> Exercise 1</a></li>
<li class="chapter" data-level="D.2" data-path="solutions-nearest-neighbours.html"><a href="solutions-nearest-neighbours.html#exercise-2-2"><i class="fa fa-check"></i><b>D.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="E" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html"><i class="fa fa-check"></i><b>E</b> Solutions ch. 6 - Decision trees and random forests</a><ul>
<li class="chapter" data-level="E.1" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-1-3"><i class="fa fa-check"></i><b>E.1</b> Exercise 1</a></li>
<li class="chapter" data-level="E.2" data-path="solutions-decision-trees.html"><a href="solutions-decision-trees.html#exercise-2-3"><i class="fa fa-check"></i><b>E.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="F" data-path="solutions-svm.html"><a href="solutions-svm.html"><i class="fa fa-check"></i><b>F</b> Solutions ch. 7 - Support vector machines</a><ul>
<li class="chapter" data-level="F.1" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-1-4"><i class="fa fa-check"></i><b>F.1</b> Exercise 1</a></li>
<li class="chapter" data-level="F.2" data-path="solutions-svm.html"><a href="solutions-svm.html#exercise-2-4"><i class="fa fa-check"></i><b>F.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="G" data-path="solutions-ann.html"><a href="solutions-ann.html"><i class="fa fa-check"></i><b>G</b> Solutions ch. 8 - Artificial neural networks</a><ul>
<li class="chapter" data-level="G.1" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-1-5"><i class="fa fa-check"></i><b>G.1</b> Exercise 1</a></li>
<li class="chapter" data-level="G.2" data-path="solutions-ann.html"><a href="solutions-ann.html#exercise-2-5"><i class="fa fa-check"></i><b>G.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="H" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html"><i class="fa fa-check"></i><b>H</b> Solutions ch. 9 - Dimensionality reduction</a><ul>
<li class="chapter" data-level="H.1" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-1-6"><i class="fa fa-check"></i><b>H.1</b> Exercise 1</a></li>
<li class="chapter" data-level="H.2" data-path="solutions-dimensionality-reduction.html"><a href="solutions-dimensionality-reduction.html#exercise-2-6"><i class="fa fa-check"></i><b>H.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="I" data-path="solutions-clustering.html"><a href="solutions-clustering.html"><i class="fa fa-check"></i><b>I</b> Solutions ch. 10 - Clustering</a><ul>
<li class="chapter" data-level="I.1" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-1-7"><i class="fa fa-check"></i><b>I.1</b> Exercise 1</a></li>
<li class="chapter" data-level="I.2" data-path="solutions-clustering.html"><a href="solutions-clustering.html#exercise-2-7"><i class="fa fa-check"></i><b>I.2</b> Exercise 2</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">An Introduction to Machine Learning</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="nearest-neighbours" class="section level1">
<h1><span class="header-section-number">5</span> Nearest neighbours</h1>
<!-- Matt -->
<!-- 
Get ideas on presentation from Harvard bioinformatics website. In particular, use of dataset with two variables (crabs??), because easier to display. Performance of classifier as k increases (should initially improve and then get worse - starts to lose flexibility).

In exercises could introduce application of knn to regression.

GENERAL:
SPLOM for displaying datasets with small number of variables

FEATURE SELECTION
filter methods  /  wrapper methods / genetic algorithms

Refer to scikit learn

FEATURE SCALING

BIAS-VARIANCE TRADEOFF
In statistics and machine learning, the bias–variance tradeoff (or dilemma) is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set[citation needed].:

    The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting).
    The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting).


-->
<div id="introduction" class="section level2">
<h2><span class="header-section-number">5.1</span> Introduction</h2>
<p>memory based and require no model to be fit</p>
<p>bias and variance</p>
<p>computational load - finding neighbours and storing the entire training set</p>
<p>k-d tree / linear search</p>
<p>system.time k-d tree search vs linear search</p>
<p>library(class)</p>
<p>class::knn</p>
<p>importance of centering a scaling</p>
<p>increase in neighbours - increase in ties</p>
</div>
<div id="algorithm" class="section level2">
<h2><span class="header-section-number">5.2</span> Algorithm</h2>
<div id="defining-nearest" class="section level3">
<h3><span class="header-section-number">5.2.1</span> Defining nearest</h3>
<strong>Euclidean distance:</strong>
<span class="math display" id="eq:euclidean">\[\begin{equation}
  distance\left(p,q\right)=\sqrt{\sum_{i=1}^{n} (p_i-q_i)^2}
  \tag{5.1}
\end{equation}\]</span>
<div class="figure" style="text-align: center"><span id="fig:euclideanDistanceDiagram"></span>
<img src="04-nearest-neighbours_files/figure-html/euclideanDistanceDiagram-1.png" alt="Euclidean distance." width="75%" />
<p class="caption">
Figure 5.1: Euclidean distance.
</p>
</div>
<div class="figure" style="text-align: center"><span id="fig:knnClassification"></span>
<img src="images/knn_classification.svg" alt="Illustration of _k_-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class.  By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282" width="75%" />
<p class="caption">
Figure 5.2: Illustration of <em>k</em>-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class. By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, <a href="https://commons.wikimedia.org/w/index.php?curid=2170282" class="uri">https://commons.wikimedia.org/w/index.php?curid=2170282</a>
</p>
</div>
</div>
</div>
<div id="classification" class="section level2">
<h2><span class="header-section-number">5.3</span> Classification</h2>
<p>Error training vs cv vs test</p>
</div>
<div id="regression" class="section level2">
<h2><span class="header-section-number">5.4</span> Regression</h2>
</div>
<div id="caret" class="section level2">
<h2><span class="header-section-number">5.5</span> Caret</h2>
<p>pre-processing identification of correlated predictors</p>
<p>Parallel processing with doMC registerDoMC() getDoParWorkers()</p>
</div>
<div id="example-one" class="section level2">
<h2><span class="header-section-number">5.6</span> Example one</h2>
<p>Load data</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">load</span>(<span class="st">&quot;data/example_binary_classification/bin_class_example.rda&quot;</span>)
<span class="kw">str</span>(xtrain)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  -0.223 0.944 2.36 1.846 1.732 ...
##  $ V2: num  -1.153 -0.827 -0.128 2.014 -0.574 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">str</span>(xtest)</code></pre></div>
<pre><code>## &#39;data.frame&#39;:    400 obs. of  2 variables:
##  $ V1: num  2.09 2.3 2.07 1.65 1.18 ...
##  $ V2: num  -1.009 1.0947 0.1644 0.3243 -0.0277 ...</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytrain))</code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">summary</span>(<span class="kw">as.factor</span>(ytest))</code></pre></div>
<pre><code>##   0   1 
## 200 200</code></pre>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">library</span>(GGally)
<span class="kw">library</span>(RColorBrewer)
point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)
point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)
point_size =<span class="st"> </span><span class="dv">2</span>

<span class="kw">ggplot</span>(xtrain, <span class="kw">aes</span>(V1,V2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytrain<span class="dv">+1</span>], <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], 
             <span class="dt">size=</span>point_size) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(xtest, <span class="kw">aes</span>(V1,V2)) +<span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[ytest<span class="dv">+1</span>], <span class="dt">shape=</span>point_shapes[ytest<span class="dv">+1</span>], 
             <span class="dt">size=</span>point_size) +<span class="st"> </span>
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simDataBinClassTrainTest"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-1.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassTrainTest-2.png" alt="Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using _k_-nn" width="50%" />
<p class="caption">
Figure 5.3: Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using <em>k</em>-nn
</p>
</div>
<p>For <em>k</em>-nn classification and regression we will use the <strong>knn</strong> function in the package <strong>class</strong>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(class)</code></pre></div>
<p><strong>Arguments to knn</strong></p>
<p><code>train</code> : matrix or data frame of training set cases.</p>
<p><code>test</code> : matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case.</p>
<p><code>cl</code> : factor of true classifications of training set</p>
<p><code>k</code> : number of neighbours considered.</p>
<p><code>l</code> : minimum vote for definite decision, otherwise doubt. (More precisely, less than k-l dissenting votes are allowed, even if k is increased by ties.)</p>
<p><code>prob</code> : If this is true, the proportion of the votes for the winning class are returned as attribute prob.</p>
<p><code>use.all</code> : controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours.</p>
<p>Training data set</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1train &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtrain, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
<span class="kw">table</span>(ytrain,knn1train)</code></pre></div>
<pre><code>##       knn1train
## ytrain   0   1
##      0 200   0
##      1   0 200</code></pre>
<p>Test data set</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1test &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xtest, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
<span class="kw">table</span>(ytest, knn1test)</code></pre></div>
<pre><code>##      knn1test
## ytest   0   1
##     0 131  69
##     1  81 119</code></pre>
<p>When we have just two dimensions it is easy to visualize the decision boundary generated by the <em>k</em>-nn classifier. Obviously quite unusual to be dealing with just two variables, but we may have reduced a high dimensional dataset to just two dimensions using the techniques that will be discussed in chapter <a href="dimensionality-reduction.html#dimensionality-reduction">9</a>.</p>
<p>Create a grid so we can predict across the full range of our variables V1 and V2.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">gridSize &lt;-<span class="st"> </span><span class="dv">150</span> 
v1limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">1</span>],xtest[,<span class="dv">1</span>])))
tmpV1 &lt;-<span class="st"> </span><span class="kw">seq</span>(v1limits[<span class="dv">1</span>],v1limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
v2limits &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="kw">min</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])),<span class="kw">max</span>(<span class="kw">c</span>(xtrain[,<span class="dv">2</span>],xtest[,<span class="dv">2</span>])))
tmpV2 &lt;-<span class="st"> </span><span class="kw">seq</span>(v2limits[<span class="dv">1</span>],v2limits[<span class="dv">2</span>],<span class="dt">len=</span>gridSize)
xgrid &lt;-<span class="st"> </span><span class="kw">expand.grid</span>(tmpV1,tmpV2)
<span class="kw">names</span>(xgrid) &lt;-<span class="st"> </span><span class="kw">names</span>(xtrain)</code></pre></div>
<p>Predict values of all elements of grid.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">knn1grid &lt;-<span class="st"> </span>class::<span class="kw">knn</span>(<span class="dt">train=</span>xtrain, <span class="dt">test=</span>xgrid, <span class="dt">cl=</span>ytrain, <span class="dt">k=</span><span class="dv">1</span>)
V3 &lt;-<span class="st"> </span><span class="kw">as.numeric</span>(<span class="kw">as.vector</span>(knn1grid))
xgrid2 &lt;-<span class="st"> </span><span class="kw">cbind</span>(xgrid, V3)</code></pre></div>
<p>Plot</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">point_shapes &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">15</span>,<span class="dv">17</span>)
point_colours &lt;-<span class="st"> </span><span class="kw">brewer.pal</span>(<span class="dv">3</span>,<span class="st">&quot;Dark2&quot;</span>)
point_size =<span class="st"> </span><span class="dv">2</span>
<span class="co"># grid point 16</span>
<span class="co"># grid point size =0.2</span>

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtrain, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytrain<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid2, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>), <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;train&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))

<span class="kw">ggplot</span>(xgrid, <span class="kw">aes</span>(V1,V2)) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">col=</span>point_colours[knn1grid], <span class="dt">shape=</span><span class="dv">16</span>, <span class="dt">size=</span><span class="fl">0.3</span>) +
<span class="st">  </span><span class="kw">geom_point</span>(<span class="dt">data=</span>xtest, <span class="kw">aes</span>(V1,V2), <span class="dt">col=</span>point_colours[ytest<span class="dv">+1</span>],
             <span class="dt">shape=</span>point_shapes[ytrain<span class="dv">+1</span>], <span class="dt">size=</span>point_size) +
<span class="st">  </span><span class="kw">geom_contour</span>(<span class="dt">data=</span>xgrid2, <span class="kw">aes</span>(<span class="dt">x=</span>V1, <span class="dt">y=</span>V2, <span class="dt">z=</span>V3), <span class="dt">breaks=</span><span class="kw">c</span>(<span class="dv">0</span>,.<span class="dv">5</span>), <span class="dt">col=</span><span class="st">&quot;grey30&quot;</span>) +
<span class="st">  </span><span class="kw">ggtitle</span>(<span class="st">&quot;test&quot;</span>) +
<span class="st">  </span><span class="kw">theme_bw</span>() +
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">plot.title =</span> <span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">25</span>, <span class="dt">face=</span><span class="st">&quot;bold&quot;</span>), <span class="dt">axis.text=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">15</span>),
        <span class="dt">axis.title=</span><span class="kw">element_text</span>(<span class="dt">size=</span><span class="dv">20</span>,<span class="dt">face=</span><span class="st">&quot;bold&quot;</span>))</code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:simDataBinClassDecisionBoundaryK1"></span>
<img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-1.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" /><img src="04-nearest-neighbours_files/figure-html/simDataBinClassDecisionBoundaryK1-2.png" alt="Binary classification of the simulated training and test sets with _k_=1." width="50%" />
<p class="caption">
Figure 5.4: Binary classification of the simulated training and test sets with <em>k</em>=1.
</p>
</div>
</div>
<div id="example-two" class="section level2">
<h2><span class="header-section-number">5.7</span> Example two</h2>
</div>
<div id="curse-of-dimensionality" class="section level2">
<h2><span class="header-section-number">5.8</span> Curse of dimensionality</h2>
<p>Pre-processing data using dimensionality reduction.</p>
<p>transformation functionality in caret</p>
</div>
<div id="examples" class="section level2">
<h2><span class="header-section-number">5.9</span> Examples</h2>
<p>centre1 &lt;- read.csv(“data/serum_proteomics/male_centre1.csv”) centre2 &lt;- read.csv(“data/serum_proteomics/male_centre2.csv”)</p>
<p>c1sub &lt;- centre1[,c(1,5,6,9,10)] c2sub &lt;- centre2[,c(1,5,6,9,10)]</p>
<p>res &lt;- FNN::knn(c1sub[,2:5], c1sub[,2:5], cl=c1sub<span class="math inline">\(Diagnostic_group, k=1) table(c1sub\)</span>Diagnostic_group, res)</p>
<p>res &lt;- FNN::knn(c1sub[,2:5], c2sub[,2:5], cl=c1sub<span class="math inline">\(Diagnostic_group, k=1) table(c2sub\)</span>Diagnostic_group, res)</p>
<p>bias / variance trade-off</p>
<p>include: division into training and test set preprocessing - illustrate with diagram</p>
</div>
<div id="exercises-2" class="section level2">
<h2><span class="header-section-number">5.10</span> Exercises</h2>
<div id="knnEx1" class="section level3">
<h3><span class="header-section-number">5.10.1</span> Exercise 1</h3>
<p>Classification</p>
<p>Try different methods of feature selection</p>
</div>
<div id="knnEx2" class="section level3">
<h3><span class="header-section-number">5.10.2</span> Exercise 2</h3>
<p>Regression</p>
<p>Alzheimers &amp; gene expression? MMSE and gene expression?</p>
<p>Solutions to exercises can be found in appendix <a href="solutions-nearest-neighbours.html#solutions-nearest-neighbours">D</a>.</p>

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="logistic-regression.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="decision-trees.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/bioinformatics-training/intro-machine-learning/edit/master/04-nearest-neighbours.Rmd",
"text": "Edit"
},
"download": ["intro-machine-learning.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(script.src))
      script.src  = script.src.replace(/^https?:/, '');
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
