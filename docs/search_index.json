[
["index.html", "An Introduction to Machine Learning 1 About the course 1.1 Overview 1.2 Registration 1.3 Prerequisites 1.4 Github 1.5 License 1.6 Contact 1.7 Colophon", " An Introduction to Machine Learning Sudhakaran Prabakaran, Matt Wayland and Chris Penfold 2017-08-29 1 About the course 1.1 Overview Machine learning gives computers the ability to learn without being explicitly programmed. It encompasses a broad range of approaches to data analysis with applicability across the biological sciences. Lectures will introduce commonly used algorithms and provide insight into their theoretical underpinnings. In the practicals students will apply these algorithms to real biological data-sets using the R language and environment. During this course you will learn about: Some of the core mathematical concepts underpinning machine learning algorithms: matrices and linear algebra; Bayesâ€™ theorem. Classification (supervised learning): partitioning data into training and test sets; feature selection; logistic regression; support vector machines; artificial neural networks; decision trees; nearest neighbours, cross-validation. Exploratory data analysis (unsupervised learning): dimensionality reduction, anomaly detection, clustering. After this course you should be able to: Understand the concepts of machine learning. Understand the strengths and limitations of the various machine learning algorithms presented in this course. Select appropriate machine learning methods for your data. Perform machine learning in R. 1.2 Registration Bioinformatics Training: An Introduction to Machine Learning 1.3 Prerequisites Some familiarity with R would be helpful. For an introduction to R see An Introduction to Solving Biological Problems with R course. 1.4 Github bioinformatics-training/intro-machine-learning 1.5 License GPL-3 1.6 Contact If you have any comments, questions or suggestions about the material, please contact the authors: Sudhakaran Prabakaran, Matt Wayland and Chris Penfold. 1.7 Colophon This book was produced using the bookdown package (Xie 2017), which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "2 Introduction", " 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa "],
["linear-models-and-matrix-algebra.html", "3 Linear models and matrix algebra", " 3 Linear models and matrix algebra "],
["linear-and-non-linear-logistic-regression.html", "4 Linear and non linear logistic regression", " 4 Linear and non linear logistic regression "],
["nearest-neighbours.html", "5 Nearest neighbours 5.1 Example one 5.2 Example two", " 5 Nearest neighbours 5.1 Example one 5.2 Example two "],
["decision-trees-and-random-forests.html", "6 Decision trees and random forests", " 6 Decision trees and random forests "],
["support-vector-machines.html", "7 Support vector machines", " 7 Support vector machines "],
["artificial-neural-networks.html", "8 Artificial neural networks", " 8 Artificial neural networks "],
["dimensionality-reduction.html", "9 Dimensionality reduction 9.1 Linear Dimensionality Reduction 9.2 Nonlinear Dimensionality Reduction", " 9 Dimensionality reduction 9.1 Linear Dimensionality Reduction 9.1.1 Principle Component Analysis 9.1.2 Horeshoe effect 9.2 Nonlinear Dimensionality Reduction 9.2.1 t-SNE 9.2.2 Gaussian Process Latent Variable Models 9.2.3 GPLVMs with informative priors "],
["clustering.html", "10 Clustering 10.1 Introduction 10.2 Distance metrics 10.3 Hierarchic methods 10.4 Partitioning methods 10.5 Summary 10.6 Exercises", " 10 Clustering 10.1 Introduction Hierarchic (produce dendrogram) vs partitioning methods Figure 10.1: Example clusters. A, blobs; B, aggregation (Gionis, Mannila, and Tsaparas 2007); C, noisy moons; D, noisy circles; E, D31 (Veenman, Reinders, and Backer 2002); F, no structure. 10.2 Distance metrics Minkowski distance: \\[\\begin{equation} distance\\left(x,y,p\\right)=\\left(\\sum_{i=1}^{n} abs(x_i-y_i)^p\\right)^{1/p} \\tag{10.1} \\end{equation}\\] Graphical explanation of euclidean, manhattan and max (Chebyshev?) 10.2.1 Image segmentation 10.3 Hierarchic methods Table 10.1: Example distance matrix A B C D B 2 C 6 5 D 10 10 5 E 9 8 3 4 10.3.1 Linkage algorithms Make one section panel of three dendrograms one table Single linkage - nearest neighbours linkage Complete linkage - furthest neighbours linkage Average linkage - UPGMA (Unweighted Pair Group Method with Arithmetic Mean) Table 10.2: Merge distances for objects in the example distance matrix using three different linkage methods. Groups Single Complete Average A,B,C,D,E 0 0 0 (A,B),C,D,E 2 2 2 (A,B),(C,E),D 3 3 3 (A,B)(C,D,E) 4 5 4.5 (A,B,C,D,E) 5 10 8 Figure 10.2: Dendrograms for the example distance matrix using three different linkage methods. 10.3.2 Example: gene expression profiling of human tissues Load required libraries library(RColorBrewer) library(dendextend) ## ## --------------------- ## Welcome to dendextend version 1.5.2 ## Type citation(&#39;dendextend&#39;) for how to cite the package. ## ## Type browseVignettes(package = &#39;dendextend&#39;) for the package vignette. ## The github page is: https://github.com/talgalili/dendextend/ ## ## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues ## Or contact: &lt;tal.galili@gmail.com&gt; ## ## To suppress this message use: suppressPackageStartupMessages(library(dendextend)) ## --------------------- ## ## Attaching package: &#39;dendextend&#39; ## The following object is masked from &#39;package:ggdendro&#39;: ## ## theme_dendro ## The following object is masked from &#39;package:stats&#39;: ## ## cutree Load data load(&quot;data/tissues_gene_expression/tissuesGeneExpression.rda&quot;) Inspect data table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver ## 38 34 15 31 39 26 ## placenta ## 6 dim(e) ## [1] 22215 189 Compute distance between each sample d &lt;- dist(t(e)) perform hierarchical clustering hc &lt;- hclust(d, method=&quot;average&quot;) plot(hc, labels=tissue, cex=0.5, hang=-1, xlab=&quot;&quot;, sub=&quot;&quot;) Figure 10.3: Clustering of tissue samples based on gene expression profiles. use dendextend library to plot dendrogram with colour labels tissue_type &lt;- unique(tissue) dend &lt;- as.dendrogram(hc) dend_colours &lt;- brewer.pal(length(unique(tissue)),&quot;Dark2&quot;) names(dend_colours) &lt;- tissue_type labels(dend) &lt;- tissue[order.dendrogram(dend)] labels_colors(dend) &lt;- dend_colours[tissue][order.dendrogram(dend)] labels_cex(dend) = 0.5 plot(dend, horiz=T) Figure 10.4: Clustering of tissue samples based on gene expression profiles with labels coloured by tissue type. Define clusters by cutting tree at a specific height plot(dend, horiz=T) abline(v=125, lwd=2, lty=2, col=&quot;blue&quot;) Figure 10.5: Clusters found by cutting tree at a height of 125 hclusters &lt;- cutree(dend, h=125) table(tissue, cluster=hclusters) ## cluster ## tissue 1 2 3 4 5 6 ## cerebellum 0 36 0 0 2 0 ## colon 0 0 34 0 0 0 ## endometrium 15 0 0 0 0 0 ## hippocampus 0 31 0 0 0 0 ## kidney 37 0 0 0 2 0 ## liver 0 0 0 24 2 0 ## placenta 0 0 0 0 0 6 Select a specific number of clusters. plot(dend, horiz=T) abline(v = heights_per_k.dendrogram(dend)[&quot;8&quot;], lwd = 2, lty = 2, col = &quot;blue&quot;) Figure 10.6: Selection of eight clusters from the dendogram hclusters &lt;- cutree(dend, k=8) table(tissue, cluster=hclusters) ## cluster ## tissue 1 2 3 4 5 6 7 8 ## cerebellum 0 31 0 0 2 0 5 0 ## colon 0 0 34 0 0 0 0 0 ## endometrium 0 0 0 0 0 15 0 0 ## hippocampus 0 31 0 0 0 0 0 0 ## kidney 37 0 0 0 2 0 0 0 ## liver 0 0 0 24 2 0 0 0 ## placenta 0 0 0 0 0 0 0 6 10.4 Partitioning methods 10.4.1 K-means Pseudocode to illustrate range of different types of data that can be clustered - image segmentation Figure 10.7: Iterations of the k-means algorithm 10.4.2 DBSCAN Density-based spatial clustering of applications with noise 10.4.3 Gene expression tissue types? 10.5 Summary 10.5.1 Applications 10.5.2 Strengths 10.5.3 Limitations 10.6 Exercises Exercise solutions: B.8 References "],
["resources.html", "A Resources A.1 Python A.2 Machine learning data set repository", " A Resources A.1 Python scikit-learn A.2 Machine learning data set repository mldata.org This repository manages the following types of objects: Data Sets - Raw data as a collection of similarily structured objects. Material and Methods - Descriptions of the computational pipeline. Learning Tasks - Learning tasks defined on raw data. Challenges - Collections of tasks which have a particular theme. "],
["solutions-to-exercises.html", "B Solutions to exercises B.1 Chapter 2 - Linear models and matrix algebra B.2 Chapter 3 - Linear and non-linear logistic regression B.3 Chapter 4 - Nearest neighbours B.4 Chapter 5 - Decision trees and random forests B.5 Chapter 6 - Support vector machines B.6 Chapter 7 - Artificial neural networks B.7 Chapter 8 - Dimensionality reduction B.8 Chapter 9 - Clustering", " B Solutions to exercises B.1 Chapter 2 - Linear models and matrix algebra B.2 Chapter 3 - Linear and non-linear logistic regression B.3 Chapter 4 - Nearest neighbours B.4 Chapter 5 - Decision trees and random forests B.5 Chapter 6 - Support vector machines B.6 Chapter 7 - Artificial neural networks B.7 Chapter 8 - Dimensionality reduction B.8 Chapter 9 - Clustering "],
["references.html", "References", " References "]
]
