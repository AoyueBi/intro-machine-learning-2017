[
["nearest-neighbours.html", "5 Nearest neighbours 5.1 Introduction 5.2 Classification 5.3 Regression 5.4 Exercises", " 5 Nearest neighbours 5.1 Introduction memory based and require no model to be fit classification and non-linear regression bias and variance computational load - finding neighbours and storing the entire training set k-d tree / linear search system.time k-d tree search vs linear search library(class) class::knn importance of centering a scaling increase in neighbours - increase in ties 5.1.1 Measuring distance between objects Euclidean distance: \\[\\begin{equation} distance\\left(p,q\\right)=\\sqrt{\\sum_{i=1}^{n} (p_i-q_i)^2} \\tag{5.1} \\end{equation}\\] Figure 5.1: Euclidean distance. 5.2 Classification 5.2.1 Algorithm Figure 5.2: Illustration of k-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class. By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282 5.2.2 Simulated data We will use a simulated data set to demonstrate: bias-variance trade-off the knn function in R plotting decision boundaries choosing the optimum value of k The dataset is partitioned into training and test sets. Load data load(&quot;data/example_binary_classification/bin_class_example.rda&quot;) str(xtrain) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ V1: num -0.223 0.944 2.36 1.846 1.732 ... ## $ V2: num -1.153 -0.827 -0.128 2.014 -0.574 ... str(xtest) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ V1: num 2.09 2.3 2.07 1.65 1.18 ... ## $ V2: num -1.009 1.0947 0.1644 0.3243 -0.0277 ... summary(as.factor(ytrain)) ## 0 1 ## 200 200 summary(as.factor(ytest)) ## 0 1 ## 200 200 library(ggplot2) library(GGally) library(RColorBrewer) point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 ggplot(xtrain, aes(V1,V2)) + geom_point(col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xtest, aes(V1,V2)) + geom_point(col=point_colours[ytest+1], shape=point_shapes[ytest+1], size=point_size) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 5.3: Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using k-nn 5.2.3 knn function For k-nn classification and regression we will use the knn function in the package class. library(class) Arguments to knn train : matrix or data frame of training set cases. test : matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case. cl : factor of true classifications of training set k : number of neighbours considered. l : minimum vote for definite decision, otherwise doubt. (More precisely, less than k-l dissenting votes are allowed, even if k is increased by ties.) prob : If this is true, the proportion of the votes for the winning class are returned as attribute prob. use.all : controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours. Let us perform k-nn on the training set with k=1. We will use the confusionMatrix function from the caret package to summarize performance of the classifier. library(caret) ## Loading required package: lattice knn1train &lt;- class::knn(train=xtrain, test=xtrain, cl=ytrain, k=1) confusionMatrix(knn1train, ytrain) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 200 0 ## 1 0 200 ## ## Accuracy : 1 ## 95% CI : (0.9908, 1) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : &lt; 2.2e-16 ## ## Kappa : 1 ## Mcnemar&#39;s Test P-Value : NA ## ## Sensitivity : 1.0 ## Specificity : 1.0 ## Pos Pred Value : 1.0 ## Neg Pred Value : 1.0 ## Prevalence : 0.5 ## Detection Rate : 0.5 ## Detection Prevalence : 0.5 ## Balanced Accuracy : 1.0 ## ## &#39;Positive&#39; Class : 0 ## Now let use the training set to predict on the test set. knn1test &lt;- class::knn(train=xtrain, test=xtest, cl=ytrain, k=1) confusionMatrix(knn1test, ytest) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 131 81 ## 1 69 119 ## ## Accuracy : 0.625 ## 95% CI : (0.5755, 0.6726) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : 3.266e-07 ## ## Kappa : 0.25 ## Mcnemar&#39;s Test P-Value : 0.3691 ## ## Sensitivity : 0.6550 ## Specificity : 0.5950 ## Pos Pred Value : 0.6179 ## Neg Pred Value : 0.6330 ## Prevalence : 0.5000 ## Detection Rate : 0.3275 ## Detection Prevalence : 0.5300 ## Balanced Accuracy : 0.6250 ## ## &#39;Positive&#39; Class : 0 ## 5.2.4 Plotting decision boundaries Since we have just two dimensions we can visualize the decision boundary generated by the k-nn classifier in a 2D scatterplot. Situations where your original data set contains only two variables will be rare, but it is not unusual to reduce a high-dimensional data set to just two dimensions using the methods that will be discussed in chapter 5.2.7.7. Therefore, knowing how to plot decision boundaries will potentially be helpful for many different datasets and classifiers. Create a grid so we can predict across the full range of our variables V1 and V2. gridSize &lt;- 150 v1limits &lt;- c(min(c(xtrain[,1],xtest[,1])),max(c(xtrain[,1],xtest[,1]))) tmpV1 &lt;- seq(v1limits[1],v1limits[2],len=gridSize) v2limits &lt;- c(min(c(xtrain[,2],xtest[,2])),max(c(xtrain[,2],xtest[,2]))) tmpV2 &lt;- seq(v2limits[1],v2limits[2],len=gridSize) xgrid &lt;- expand.grid(tmpV1,tmpV2) names(xgrid) &lt;- names(xtrain) Predict values of all elements of grid. knn1grid &lt;- class::knn(train=xtrain, test=xgrid, cl=ytrain, k=1) V3 &lt;- as.numeric(as.vector(knn1grid)) xgrid &lt;- cbind(xgrid, V3) Plot point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtrain, aes(V1,V2), col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtest, aes(V1,V2), col=point_colours[ytest+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 5.4: Binary classification of the simulated training and test sets with k=1. 5.2.5 Bias-variance tradeoff The biasâ€“variance tradeoff is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). To demonstrate this phenomenon, let us look at the performance of the k-nn classifier over a range of values of k. First we will define a function to create a sequence of log spaced values. This is the lseq function from the emdbook package: lseq &lt;- function(from, to, length.out) { exp(seq(log(from), log(to), length.out = length.out)) } Get log spaced sequence of length 20, round and then remove any duplicates resulting from rounding. s &lt;- unique(round(lseq(1,400,20))) train_error &lt;- sapply(s, function(i){ yhat &lt;- knn(xtrain, xtrain, ytrain, i) return(1-mean(as.numeric(as.vector(yhat))==ytrain)) }) test_error &lt;- sapply(s, function(i){ yhat &lt;- knn(xtrain, xtest, ytrain, i) return(1-mean(as.numeric(as.vector(yhat))==ytest)) }) k &lt;- rep(s, 2) set &lt;- c(rep(&quot;train&quot;, length(s)), rep(&quot;test&quot;, length(s))) error &lt;- c(train_error, test_error) misclass_errors &lt;- data.frame(k, set, error) ggplot(misclass_errors, aes(x=k, y=error, group=set)) + geom_line(aes(colour=set, linetype=set), size=1.5) + scale_x_log10() + ylab(&quot;Misclassification Errors&quot;) + theme_bw() + theme(legend.position = c(0.5, 0.25), legend.title=element_blank(), legend.text=element_text(size=12), axis.title.x=element_text(face=&quot;italic&quot;, size=12)) Figure 5.5: Misclassification errors as a function of neighbourhood size. 5.2.6 Choosing k We will use the caret library. library(caret) caret has automatic parallel processing built in. To take advantage of this feature we simply need to load the doMC package and register workers: library(doMC) ## Loading required package: foreach ## Loading required package: iterators ## Loading required package: parallel registerDoMC() To find out how many cores we have registered we can use: getDoParWorkers() ## [1] 2 The caret function train is used to fit predictive models over different values of k. The function trainControl is used to specify a list of computational and resampling options, which will be passed to train. We will start by configuring our cross-validation procedure using trainControl. We would like to make this demonstration reproducible and because we will be running the models in parallel, using the set.seed function alone is not sufficient. In addition to using set.seed we have to make use of the optional seeds argument to trainControl. We need to supply seeds with a list of integers that will be used to set the seed at each sampling iteration. The list is required to have a length of B+1, where B is the number of resamples. We will be repeating 10-fold cross-validation a total of ten times and so our list must have a length of 101. The first B elements of the list are required to be vectors of integers of length M, where M is the number of models being evaluated (in this case 19). The last element of the list only needs to be a single integer, which will be used for the final model. First we generate our list of seeds. set.seed(42) seeds &lt;- vector(mode = &quot;list&quot;, length = 101) for(i in 1:100) seeds[[i]] &lt;- sample.int(1000, 19) seeds[[101]] &lt;- sample.int(1000,1) We can now use trainControl to create a list of computational options for resampling. tc &lt;- trainControl(method=&quot;repeatedcv&quot;, number = 10, repeats = 10, seeds = seeds) There are two options for choosing the values of k to be evaluated by the train function: Pass a data.frame of values of k to the tuneGrid argument of train. Specify the number of different levels of k using the tuneLength function and allow train to pick the actual values. We will use the first option, so that we can try the values of k we examined earlier. We need to convert the vector of values of k we created earlier and convert it into a data.frame. s &lt;- data.frame(s) names(s) &lt;- &quot;k&quot; We are now ready to run the cross-validation. knnFit &lt;- train(xtrain, as.factor(ytrain), method=&quot;knn&quot;, tuneGrid=s, trControl=tc) knnFit ## k-Nearest Neighbors ## ## 400 samples ## 2 predictor ## 2 classes: &#39;0&#39;, &#39;1&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 10 times) ## Summary of sample sizes: 360, 360, 360, 360, 360, 360, ... ## Resampling results across tuning parameters: ## ## k Accuracy Kappa ## 1 0.63375 0.2675 ## 2 0.64125 0.2825 ## 3 0.67925 0.3585 ## 4 0.67200 0.3440 ## 5 0.69675 0.3935 ## 7 0.71100 0.4220 ## 9 0.71650 0.4330 ## 12 0.71450 0.4290 ## 17 0.72650 0.4530 ## 23 0.73175 0.4635 ## 32 0.73775 0.4755 ## 44 0.74075 0.4815 ## 60 0.74675 0.4935 ## 83 0.75475 0.5095 ## 113 0.73600 0.4720 ## 155 0.72500 0.4500 ## 213 0.70950 0.4190 ## 292 0.69300 0.3860 ## 400 0.51300 0.0260 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was k = 83. Cohenâ€™s Kappa: \\[\\begin{equation} Kappa = \\frac{O-E}{1-E} \\tag{5.2} \\end{equation}\\] where O is the observed accuracy and E is the expected accuracy based on the marginal totals of the confusion matrix. Cohenâ€™s Kappa takes values between -1 and 1; a value of zero indicates no agreement between the observed and predicted classes, while a value of one shows perfect concordance of the model prediction and the observed classes. If the prediction is in the opposite direction of the truth, a negative value will be obtained, but large negative values are rare in practice (Kuhn and Johnson 2013). We can plot accuracy (determined from repeated cross-validation) as a function of neighbourhood size. plot(knnFit) Figure 5.6: Accuracy (repeated cross-validation) as a function of neighbourhood size. We can also plot other performance metrics, such as Cohenâ€™s Kappa, using the metric argument. plot(knnFit, metric=&quot;Kappa&quot;) Figure 5.7: Cohenâ€™s Kappa (repeated cross-validation) as a function of neighbourhood size. Let us now evaluate how our classifier performs on the test set. test_pred &lt;- predict(knnFit, xtest) confusionMatrix(test_pred, ytest) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 154 68 ## 1 46 132 ## ## Accuracy : 0.715 ## 95% CI : (0.668, 0.7588) ## No Information Rate : 0.5 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.43 ## Mcnemar&#39;s Test P-Value : 0.0492 ## ## Sensitivity : 0.7700 ## Specificity : 0.6600 ## Pos Pred Value : 0.6937 ## Neg Pred Value : 0.7416 ## Prevalence : 0.5000 ## Detection Rate : 0.3850 ## Detection Prevalence : 0.5550 ## Balanced Accuracy : 0.7150 ## ## &#39;Positive&#39; Class : 0 ## Scatterplots with decision boundaries can be plotted using the methods described earlier. First create a grid so we can predict across the full range of our variables V1 and V2: gridSize &lt;- 150 v1limits &lt;- c(min(c(xtrain[,1],xtest[,1])),max(c(xtrain[,1],xtest[,1]))) tmpV1 &lt;- seq(v1limits[1],v1limits[2],len=gridSize) v2limits &lt;- c(min(c(xtrain[,2],xtest[,2])),max(c(xtrain[,2],xtest[,2]))) tmpV2 &lt;- seq(v2limits[1],v2limits[2],len=gridSize) xgrid &lt;- expand.grid(tmpV1,tmpV2) names(xgrid) &lt;- names(xtrain) Predict values of all elements of grid. knn1grid &lt;- predict(knnFit, xgrid) V3 &lt;- as.numeric(as.vector(knn1grid)) xgrid &lt;- cbind(xgrid, V3) Plot point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtrain, aes(V1,V2), col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtest, aes(V1,V2), col=point_colours[ytest+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 5.8: Binary classification of the simulated training and test sets with k=83. 5.2.7 Data pre-processing 5.2.7.1 Cell segmentation data set Pre-processing will be demonstrated using the cell segmentation data of (Hill et al. 2007) Figure 5.9: Image segmentation in high content screening. Images b and c are examples of well-segmented cells; d and e show poor-segmentation. Source: Hill(2007) https://doi.org/10.1186/1471-2105-8-340 This data set is one of several included in caret. data(segmentationData) str(segmentationData) ## &#39;data.frame&#39;: 2019 obs. of 61 variables: ## $ Cell : int 207827637 207932307 207932463 207932470 207932455 207827656 207827659 207827661 207932479 207932480 ... ## $ Case : Factor w/ 2 levels &quot;Test&quot;,&quot;Train&quot;: 1 2 2 2 1 1 1 1 1 1 ... ## $ Class : Factor w/ 2 levels &quot;PS&quot;,&quot;WS&quot;: 1 1 2 1 1 2 2 1 2 2 ... ## $ AngleCh1 : num 143.25 133.75 106.65 69.15 2.89 ... ## $ AreaCh1 : int 185 819 431 298 285 172 177 251 495 384 ... ## $ AvgIntenCh1 : num 15.7 31.9 28 19.5 24.3 ... ## $ AvgIntenCh2 : num 4.95 206.88 116.32 102.29 112.42 ... ## $ AvgIntenCh3 : num 9.55 69.92 63.94 28.22 20.47 ... ## $ AvgIntenCh4 : num 2.21 164.15 106.7 31.03 40.58 ... ## $ ConvexHullAreaRatioCh1 : num 1.12 1.26 1.05 1.2 1.11 ... ## $ ConvexHullPerimRatioCh1: num 0.92 0.797 0.935 0.866 0.957 ... ## $ DiffIntenDensityCh1 : num 29.5 31.9 32.5 26.7 31.6 ... ## $ DiffIntenDensityCh3 : num 13.8 43.1 36 22.9 21.7 ... ## $ DiffIntenDensityCh4 : num 6.83 79.31 51.36 26.39 25.03 ... ## $ EntropyIntenCh1 : num 4.97 6.09 5.88 5.42 5.66 ... ## $ EntropyIntenCh3 : num 4.37 6.64 6.68 5.44 5.29 ... ## $ EntropyIntenCh4 : num 2.72 7.88 7.14 5.78 5.24 ... ## $ EqCircDiamCh1 : num 15.4 32.3 23.4 19.5 19.1 ... ## $ EqEllipseLWRCh1 : num 3.06 1.56 1.38 3.39 2.74 ... ## $ EqEllipseOblateVolCh1 : num 337 2233 802 725 608 ... ## $ EqEllipseProlateVolCh1 : num 110 1433 583 214 222 ... ## $ EqSphereAreaCh1 : num 742 3279 1727 1195 1140 ... ## $ EqSphereVolCh1 : num 1901 17654 6751 3884 3621 ... ## $ FiberAlign2Ch3 : num 1 1.49 1.3 1.22 1.49 ... ## $ FiberAlign2Ch4 : num 1 1.35 1.52 1.73 1.38 ... ## $ FiberLengthCh1 : num 27 64.3 21.1 43.1 34.7 ... ## $ FiberWidthCh1 : num 7.41 13.17 21.14 7.4 8.48 ... ## $ IntenCoocASMCh3 : num 0.01118 0.02805 0.00686 0.03096 0.02277 ... ## $ IntenCoocASMCh4 : num 0.05045 0.01259 0.00614 0.01103 0.07969 ... ## $ IntenCoocContrastCh3 : num 40.75 8.23 14.45 7.3 15.85 ... ## $ IntenCoocContrastCh4 : num 13.9 6.98 16.7 13.39 3.54 ... ## $ IntenCoocEntropyCh3 : num 7.2 6.82 7.58 6.31 6.78 ... ## $ IntenCoocEntropyCh4 : num 5.25 7.1 7.67 7.2 5.5 ... ## $ IntenCoocMaxCh3 : num 0.0774 0.1532 0.0284 0.1628 0.1274 ... ## $ IntenCoocMaxCh4 : num 0.172 0.0739 0.0232 0.0775 0.2785 ... ## $ KurtIntenCh1 : num -0.6567 -0.2488 -0.2935 0.6259 0.0421 ... ## $ KurtIntenCh3 : num -0.608 -0.331 1.051 0.128 0.952 ... ## $ KurtIntenCh4 : num 0.726 -0.265 0.151 -0.347 -0.195 ... ## $ LengthCh1 : num 26.2 47.2 28.1 37.9 36 ... ## $ NeighborAvgDistCh1 : num 370 174 158 206 205 ... ## $ NeighborMinDistCh1 : num 99.1 30.1 34.9 33.1 27 ... ## $ NeighborVarDistCh1 : num 128 81.4 90.4 116.9 111 ... ## $ PerimCh1 : num 68.8 154.9 84.6 101.1 86.5 ... ## $ ShapeBFRCh1 : num 0.665 0.54 0.724 0.589 0.6 ... ## $ ShapeLWRCh1 : num 2.46 1.47 1.33 2.83 2.73 ... ## $ ShapeP2ACh1 : num 1.88 2.26 1.27 2.55 2.02 ... ## $ SkewIntenCh1 : num 0.455 0.399 0.472 0.882 0.517 ... ## $ SkewIntenCh3 : num 0.46 0.62 0.971 1 1.177 ... ## $ SkewIntenCh4 : num 1.233 0.527 0.325 0.604 0.926 ... ## $ SpotFiberCountCh3 : int 1 4 2 4 1 1 0 2 1 1 ... ## $ SpotFiberCountCh4 : num 5 12 7 8 8 5 5 8 12 8 ... ## $ TotalIntenCh1 : int 2781 24964 11552 5545 6603 53779 43950 4401 7593 6512 ... ## $ TotalIntenCh2 : num 701 160998 47511 28870 30306 ... ## $ TotalIntenCh3 : int 1690 54675 26344 8042 5569 21234 20929 4136 6488 7503 ... ## $ TotalIntenCh4 : int 392 128368 43959 8843 11037 57231 46187 373 24325 23162 ... ## $ VarIntenCh1 : num 12.5 18.8 17.3 13.8 15.4 ... ## $ VarIntenCh3 : num 7.61 56.72 37.67 30.01 20.5 ... ## $ VarIntenCh4 : num 2.71 118.39 49.47 24.75 45.45 ... ## $ WidthCh1 : num 10.6 32.2 21.2 13.4 13.2 ... ## $ XCentroid : int 42 215 371 487 283 191 180 373 236 303 ... ## $ YCentroid : int 14 347 252 295 159 127 138 181 467 468 ... The second column of segmentationData is a factor showing how the observations were characterized into training and test sets in the original study. This column is irrelevant here, because we will perform our own partitioning of the data, and so can be deleted: segmentationData$Case &lt;- NULL Following removal of the Case column, segmentationData has 60 columns. The first column of is a unique identifier for each cell. The second column contains the class labels: PS (poorly-segmented) and WS (well-segmented). The remaining 58 columns are the features. 5.2.7.2 Data splitting The first step in the analysis is to partition the data into training and test sets, using the createDataPartition function in caret. set.seed(42) trainIndex &lt;- createDataPartition(y=segmentationData$Class, times=1, p=0.5, list=F) segmentationTrain &lt;- segmentationData[trainIndex,] segmentationTest &lt;- segmentationData[-trainIndex,] This results in balanced class distributions within the splits: summary(segmentationTrain$Class) ## PS WS ## 650 360 summary(segmentationTest$Class) ## PS WS ## 650 359 N.B. The test set is set aside for now. It will be used only ONCE, to test the final model. 5.2.7.3 Removal of zero and near zero-variance predictors The function nearZeroVar identifies predictors that have one unique value. It also diagnoses predictors having both of the following characteristics: very few unique values relative to the number of samples the ratio of the frequency of the most common value to the frequency of the 2nd most common value is large. Such zero and near zero-variance predictors have a deleterious impact on modelling and may lead to unstable fits. nzv &lt;- nearZeroVar(segmentationTrain[,2:60], saveMetrics=T) nzv ## freqRatio percentUnique zeroVar nzv ## Class 1.805556 0.1980198 FALSE FALSE ## AngleCh1 1.000000 100.0000000 FALSE FALSE ## AreaCh1 1.083333 37.3267327 FALSE FALSE ## AvgIntenCh1 1.000000 100.0000000 FALSE FALSE ## AvgIntenCh2 3.000000 99.8019802 FALSE FALSE ## AvgIntenCh3 1.000000 100.0000000 FALSE FALSE ## AvgIntenCh4 2.000000 99.9009901 FALSE FALSE ## ConvexHullAreaRatioCh1 1.000000 98.9108911 FALSE FALSE ## ConvexHullPerimRatioCh1 1.000000 100.0000000 FALSE FALSE ## DiffIntenDensityCh1 1.000000 100.0000000 FALSE FALSE ## DiffIntenDensityCh3 1.000000 100.0000000 FALSE FALSE ## DiffIntenDensityCh4 1.000000 100.0000000 FALSE FALSE ## EntropyIntenCh1 1.000000 100.0000000 FALSE FALSE ## EntropyIntenCh3 1.000000 100.0000000 FALSE FALSE ## EntropyIntenCh4 1.000000 100.0000000 FALSE FALSE ## EqCircDiamCh1 1.083333 37.3267327 FALSE FALSE ## EqEllipseLWRCh1 1.000000 99.8019802 FALSE FALSE ## EqEllipseOblateVolCh1 1.000000 100.0000000 FALSE FALSE ## EqEllipseProlateVolCh1 1.000000 100.0000000 FALSE FALSE ## EqSphereAreaCh1 1.083333 37.3267327 FALSE FALSE ## EqSphereVolCh1 1.083333 37.3267327 FALSE FALSE ## FiberAlign2Ch3 1.304348 94.8514851 FALSE FALSE ## FiberAlign2Ch4 7.285714 94.3564356 FALSE FALSE ## FiberLengthCh1 1.000000 95.8415842 FALSE FALSE ## FiberWidthCh1 1.000000 95.8415842 FALSE FALSE ## IntenCoocASMCh3 1.000000 100.0000000 FALSE FALSE ## IntenCoocASMCh4 1.000000 100.0000000 FALSE FALSE ## IntenCoocContrastCh3 1.000000 100.0000000 FALSE FALSE ## IntenCoocContrastCh4 1.000000 100.0000000 FALSE FALSE ## IntenCoocEntropyCh3 1.000000 100.0000000 FALSE FALSE ## IntenCoocEntropyCh4 1.000000 100.0000000 FALSE FALSE ## IntenCoocMaxCh3 1.250000 94.1584158 FALSE FALSE ## IntenCoocMaxCh4 1.250000 94.3564356 FALSE FALSE ## KurtIntenCh1 1.000000 100.0000000 FALSE FALSE ## KurtIntenCh3 1.000000 100.0000000 FALSE FALSE ## KurtIntenCh4 1.000000 100.0000000 FALSE FALSE ## LengthCh1 2.000000 99.9009901 FALSE FALSE ## NeighborAvgDistCh1 1.000000 100.0000000 FALSE FALSE ## NeighborMinDistCh1 1.166667 41.0891089 FALSE FALSE ## NeighborVarDistCh1 1.000000 100.0000000 FALSE FALSE ## PerimCh1 1.000000 63.7623762 FALSE FALSE ## ShapeBFRCh1 1.000000 100.0000000 FALSE FALSE ## ShapeLWRCh1 1.000000 100.0000000 FALSE FALSE ## ShapeP2ACh1 1.000000 99.7029703 FALSE FALSE ## SkewIntenCh1 1.000000 100.0000000 FALSE FALSE ## SkewIntenCh3 1.000000 100.0000000 FALSE FALSE ## SkewIntenCh4 1.000000 100.0000000 FALSE FALSE ## SpotFiberCountCh3 1.212000 1.2871287 FALSE FALSE ## SpotFiberCountCh4 1.152778 3.2673267 FALSE FALSE ## TotalIntenCh1 1.000000 98.7128713 FALSE FALSE ## TotalIntenCh2 1.500000 99.0099010 FALSE FALSE ## TotalIntenCh3 1.000000 99.1089109 FALSE FALSE ## TotalIntenCh4 1.000000 99.6039604 FALSE FALSE ## VarIntenCh1 1.000000 100.0000000 FALSE FALSE ## VarIntenCh3 1.000000 100.0000000 FALSE FALSE ## VarIntenCh4 1.000000 100.0000000 FALSE FALSE ## WidthCh1 1.000000 100.0000000 FALSE FALSE ## XCentroid 1.111111 41.5841584 FALSE FALSE ## YCentroid 1.000000 35.7425743 FALSE FALSE 5.2.7.4 Centring and scaling The variables in this data set are on different scales, for example: summary(segmentationTrain$IntenCoocASMCh4) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.004874 0.017250 0.049460 0.101600 0.121200 0.867800 summary(segmentationTrain$TotalIntenCh2) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1 15850 49650 53140 72300 362500 In this situation it is important to centre and scale each predictor. A predictor variable is centered by subtracting the mean of the predictor from each value. To scale a predictor variable, each value is divided by its standard deviation. After centring and scaling the predictor variable has a mean of 0 and a standard deviation of 1. Centring and scaling will be peformed within the cross-validation process. 5.2.7.5 Resolving skewness Many of the predictors in the segmentation data set exhibit skewness, i.e. the distribution of their values is asymmetric, for example: qplot(segmentationTrain$IntenCoocASMCh3, binwidth=0.1) + xlab(&quot;IntenCoocASMCh3&quot;) + theme_bw() Figure 5.10: Example of a predictor from the segmentation data set showing skewness. caret provides various methods for transforming skewed variables to normality, including the Box-Cox (Box and Cox (1964)) and Yeo-Johnson (Yeo and Johnson (2000)) transformations. 5.2.7.6 Removal of correlated predictors Many of the variables in the segmentation data set are highly correlated. library(corrplot) corMat &lt;- cor(segmentationTrain[,3:60]) corrplot(corMat, order=&quot;hclust&quot;, tl.cex=0.4) Figure 5.11: Correlogram of the segmentation data set. highCorr &lt;- findCorrelation(corMat, cutoff=0.75) length(highCorr) ## [1] 31 segmentationTrain &lt;- segmentationTrain[,-highCorr] 5.2.7.7 Dimensionality reduction In the case of data-sets comprised of many highly correlated variables, an alternative to removing correlated predictors is the transformation of the entire data set to a lower dimensional space, using a technique such as principal component analysis (PCA). Methods for dimensionality reduction will be explored in chapter 5.2.7.7. 5.2.8 Feature selection 5.2.8.1 Cross-validated performance without feature selection 5.2.8.2 Methods 5.2.8.3 Univariate (t-test) filter 5.2.8.4 Recursive feature elimination 5.3 Regression 5.4 Exercises 5.4.1 Exercise 1 Classification Try different methods of feature selection 5.4.2 Exercise 2 Regression Alzheimers &amp; gene expression? MMSE and gene expression? Solutions to exercises can be found in appendix D. References "]
]
