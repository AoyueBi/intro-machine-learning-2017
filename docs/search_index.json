[
["index.html", "An Introduction to Machine Learning 1 About the course 1.1 Overview 1.2 Registration 1.3 Prerequisites 1.4 Github 1.5 License 1.6 Contact 1.7 Colophon", " An Introduction to Machine Learning Sudhakaran Prabakaran, Matthew Wayland and Christopher Penfold 2017-08-23 1 About the course 1.1 Overview Machine learning gives computers the ability to learn without being explicitly programmed. It encompasses a broad range of approaches to data analysis with applicability across the biological sciences. Lectures will introduce commonly used algorithms and provide insight into their theoretical underpinnings. In the practicals students will apply these algorithms to real biological data-sets using the R language and environment. During this course you will learn about: Some of the core mathematical concepts underpinning machine learning algorithms: matrices and linear algebra; Bayesâ€™ theorem. Classification (supervised learning): partitioning data into training and test sets; feature selection; logistic regression; support vector machines; artificial neural networks; decision trees; nearest neighbours, cross-validation. Exploratory data analysis (unsupervised learning): dimensionality reduction, anomaly detection, clustering. After this course you should be able to: Understand the concepts of machine learning. Understand the strengths and limitations of the various machine learning algorithms presented in this course. Select appropriate machine learning methods for your data. Perform machine learning in R. 1.2 Registration Bioinformatics Training: An Introduction to Machine Learning 1.3 Prerequisites Some familiarity with R would be helpful. For an introduction to R see An Introduction to Solving Biological Problems with R course. 1.4 Github bioinformatics-training/intro-machine-learning 1.5 License GPL-3 1.6 Contact If you have any comments, questions or suggestions about the material, please contact the authors: Sudhakaran Prabakaran, Matt Wayland and Chris Penfold. 1.7 Colophon This book was produced using the bookdown package (Xie 2017), which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "2 Introduction", " 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa "],
["linear-models-and-matrix-algebra.html", "3 Linear models and matrix algebra", " 3 Linear models and matrix algebra "],
["linear-and-non-linear-logistic-regression.html", "4 Linear and non linear logistic regression", " 4 Linear and non linear logistic regression "],
["nearest-neighbours.html", "5 Nearest neighbours 5.1 Example one 5.2 Example two", " 5 Nearest neighbours 5.1 Example one 5.2 Example two "],
["decision-trees-and-random-forests.html", "6 Decision trees and random forests", " 6 Decision trees and random forests "],
["support-vector-machines.html", "7 Support vector machines", " 7 Support vector machines "],
["artificial-neural-networks.html", "8 Artificial neural networks", " 8 Artificial neural networks "],
["dimensionality-reduction.html", "9 Dimensionality reduction 9.1 Linear Dimensionality Reduction 9.2 Nonlinear Dimensionality Reduction", " 9 Dimensionality reduction 9.1 Linear Dimensionality Reduction 9.1.1 Principle Component Analysis 9.1.2 Horeshoe effect 9.2 Nonlinear Dimensionality Reduction 9.2.1 t-SNE 9.2.2 Gaussian Process Latent Variable Models 9.2.3 GPLVMs with informative priors "],
["clustering.html", "10 Clustering 10.1 Introduction 10.2 Types of cluster 10.3 Distance metrics 10.4 Clustering algorithms 10.5 Visualization 10.6 Examples 10.7 Summary 10.8 Exercises 10.9 Extended exercises", " 10 Clustering 10.1 Introduction 10.2 Types of cluster Figure 10.1: Example clusters 10.3 Distance metrics Minkowski distance: \\[\\begin{equation} distance\\left(x,y,p\\right)=\\left(\\sum_{i=1}^{n} abs(x_i-y_i)^p\\right)^{1/p} \\tag{10.1} \\end{equation}\\] 10.4 Clustering algorithms 10.4.1 K-means Pseudocode 10.4.2 DBSCAN Density-based spatial clustering of applications with noise 10.5 Visualization Dendrogram Heatmap 10.6 Examples 10.6.1 Image segmentation 10.6.2 Quality control 10.7 Summary 10.7.1 Applications 10.7.2 Strengths 10.7.3 Limitations 10.8 Exercises Exercise solutions: B.8 10.9 Extended exercises "],
["resources.html", "A Resources A.1 Python A.2 Machine learning data set repository", " A Resources A.1 Python scikit-learn A.2 Machine learning data set repository mldata.org This repository manages the following types of objects: Data Sets - Raw data as a collection of similarily structured objects. Material and Methods - Descriptions of the computational pipeline. Learning Tasks - Learning tasks defined on raw data. Challenges - Collections of tasks which have a particular theme. "],
["solutions-to-exercises.html", "B Solutions to exercises B.1 Chapter 2 - Linear models and matrix algebra B.2 Chapter 3 - Linear and non-linear logistic regression B.3 Chapter 4 - Nearest neighbours B.4 Chapter 5 - Decision trees and random forests B.5 Chapter 6 - Support vector machines B.6 Chapter 7 - Artificial neural networks B.7 Chapter 8 - Dimensionality reduction B.8 Chapter 9 - Clustering", " B Solutions to exercises B.1 Chapter 2 - Linear models and matrix algebra B.2 Chapter 3 - Linear and non-linear logistic regression B.3 Chapter 4 - Nearest neighbours B.4 Chapter 5 - Decision trees and random forests B.5 Chapter 6 - Support vector machines B.6 Chapter 7 - Artificial neural networks B.7 Chapter 8 - Dimensionality reduction B.8 Chapter 9 - Clustering "],
["references.html", "References", " References "]
]
