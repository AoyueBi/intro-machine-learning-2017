[
["index.html", "An Introduction to Machine Learning 1 About the course 1.1 Overview 1.2 Registration 1.3 Prerequisites 1.4 Github 1.5 License 1.6 Contact 1.7 Colophon", " An Introduction to Machine Learning Sudhakaran Prabakaran, Matt Wayland and Chris Penfold 2017-08-31 1 About the course 1.1 Overview Machine learning gives computers the ability to learn without being explicitly programmed. It encompasses a broad range of approaches to data analysis with applicability across the biological sciences. Lectures will introduce commonly used algorithms and provide insight into their theoretical underpinnings. In the practicals students will apply these algorithms to real biological data-sets using the R language and environment. During this course you will learn about: Some of the core mathematical concepts underpinning machine learning algorithms: matrices and linear algebra; Bayesâ€™ theorem. Classification (supervised learning): partitioning data into training and test sets; feature selection; logistic regression; support vector machines; artificial neural networks; decision trees; nearest neighbours, cross-validation. Exploratory data analysis (unsupervised learning): dimensionality reduction, anomaly detection, clustering. After this course you should be able to: Understand the concepts of machine learning. Understand the strengths and limitations of the various machine learning algorithms presented in this course. Select appropriate machine learning methods for your data. Perform machine learning in R. 1.2 Registration Bioinformatics Training: An Introduction to Machine Learning 1.3 Prerequisites Some familiarity with R would be helpful. For an introduction to R see An Introduction to Solving Biological Problems with R course. 1.4 Github bioinformatics-training/intro-machine-learning 1.5 License GPL-3 1.6 Contact If you have any comments, questions or suggestions about the material, please contact the authors: Sudhakaran Prabakaran, Matt Wayland and Chris Penfold. 1.7 Colophon This book was produced using the bookdown package (Xie 2017), which was built on top of R Markdown and knitr (Xie 2015). References "],
["intro.html", "2 Introduction", " 2 Introduction You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 2. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter ??. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 2.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 2.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 2.1. knitr::kable( head(iris, 20), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 2.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa 4.6 3.4 1.4 0.3 setosa 5.0 3.4 1.5 0.2 setosa 4.4 2.9 1.4 0.2 setosa 4.9 3.1 1.5 0.1 setosa 5.4 3.7 1.5 0.2 setosa 4.8 3.4 1.6 0.2 setosa 4.8 3.0 1.4 0.1 setosa 4.3 3.0 1.1 0.1 setosa 5.8 4.0 1.2 0.2 setosa 5.7 4.4 1.5 0.4 setosa 5.4 3.9 1.3 0.4 setosa 5.1 3.5 1.4 0.3 setosa 5.7 3.8 1.7 0.3 setosa 5.1 3.8 1.5 0.3 setosa "],
["linear-models.html", "3 Linear models and matrix algebra 3.1 Exercises", " 3 Linear models and matrix algebra 3.1 Exercises Solutions to exercises can be found in appendix B "],
["logistic-regression.html", "4 Linear and non linear logistic regression 4.1 Exercises", " 4 Linear and non linear logistic regression 4.1 Exercises Solutions to exercises can be found in appendix C. "],
["nearest-neighbours.html", "5 Nearest neighbours 5.1 Example one 5.2 Example two 5.3 Exercises", " 5 Nearest neighbours 5.1 Example one 5.2 Example two 5.3 Exercises Solutions to exercises can be found in appendix D. "],
["decision-trees.html", "6 Decision trees and random forests 6.1 Exercises", " 6 Decision trees and random forests 6.1 Exercises Solutions to exercises can be found in appendix E. "],
["svm.html", "7 Support vector machines 7.1 Exercises", " 7 Support vector machines 7.1 Exercises Solutions to exercises can be found in appendix F "],
["ann.html", "8 Artificial neural networks 8.1 Exercises", " 8 Artificial neural networks 8.1 Exercises Solutions to exercises can be found in appendix G. "],
["dimensionality-reduction.html", "9 Dimensionality reduction 9.1 Linear Dimensionality Reduction 9.2 Nonlinear Dimensionality Reduction 9.3 Exercises", " 9 Dimensionality reduction 9.1 Linear Dimensionality Reduction 9.1.1 Principle Component Analysis 9.1.2 Horeshoe effect 9.2 Nonlinear Dimensionality Reduction 9.2.1 t-SNE 9.2.2 Gaussian Process Latent Variable Models 9.2.3 GPLVMs with informative priors 9.3 Exercises Solutions to exercises can be found in appendix H. "],
["clustering.html", "10 Clustering 10.1 Introduction 10.2 Distance metrics 10.3 Hierarchic methods 10.4 Partitioning methods 10.5 Summary 10.6 Exercises", " 10 Clustering 10.1 Introduction Hierarchic (produce dendrogram) vs partitioning methods Figure 10.1: Example clusters. A, blobs; B, aggregation (Gionis, Mannila, and Tsaparas 2007); C, noisy moons; D, noisy circles; E, D31 (Veenman, Reinders, and Backer 2002); F, no structure. 10.2 Distance metrics Minkowski distance: \\[\\begin{equation} distance\\left(x,y,p\\right)=\\left(\\sum_{i=1}^{n} abs(x_i-y_i)^p\\right)^{1/p} \\tag{10.1} \\end{equation}\\] Graphical explanation of euclidean, manhattan and max (Chebyshev?) 10.2.1 Image segmentation 10.3 Hierarchic methods Table 10.1: Example distance matrix A B C D B 2 C 6 5 D 10 10 5 E 9 8 3 4 10.3.1 Linkage algorithms Make one section panel of three dendrograms one table Single linkage - nearest neighbours linkage Complete linkage - furthest neighbours linkage Average linkage - UPGMA (Unweighted Pair Group Method with Arithmetic Mean) Table 10.2: Merge distances for objects in the example distance matrix using three different linkage methods. Groups Single Complete Average A,B,C,D,E 0 0 0 (A,B),C,D,E 2 2 2 (A,B),(C,E),D 3 3 3 (A,B)(C,D,E) 4 5 4.5 (A,B,C,D,E) 5 10 8 Figure 10.2: Dendrograms for the example distance matrix using three different linkage methods. 10.3.2 Example: clustering toy data sets library(RColorBrewer) library(dendextend) ## ## --------------------- ## Welcome to dendextend version 1.5.2 ## Type citation(&#39;dendextend&#39;) for how to cite the package. ## ## Type browseVignettes(package = &#39;dendextend&#39;) for the package vignette. ## The github page is: https://github.com/talgalili/dendextend/ ## ## Suggestions and bug-reports can be submitted at: https://github.com/talgalili/dendextend/issues ## Or contact: &lt;tal.galili@gmail.com&gt; ## ## To suppress this message use: suppressPackageStartupMessages(library(dendextend)) ## --------------------- ## ## Attaching package: &#39;dendextend&#39; ## The following object is masked from &#39;package:ggdendro&#39;: ## ## theme_dendro ## The following object is masked from &#39;package:stats&#39;: ## ## cutree library(ggplot2) library(GGally) cluster_colours &lt;- brewer.pal(8,&quot;Dark2&quot;) blobs &lt;- read.csv(&quot;data/example_clusters/blobs.csv&quot;, header=F) aggregation &lt;- read.table(&quot;data/example_clusters/aggregation.txt&quot;) noisy_moons &lt;- read.csv(&quot;data/example_clusters/noisy_moons.csv&quot;, header=F) noisy_circles &lt;- read.csv(&quot;data/example_clusters/noisy_circles.csv&quot;, header=F) no_structure &lt;- read.csv(&quot;data/example_clusters/no_structure.csv&quot;, header=F) hclust_plots &lt;- function(data_set, n){ d &lt;- dist(data_set[,1:2]) dend &lt;- as.dendrogram(hclust(d, method=&quot;average&quot;)) clusters &lt;- cutree(dend,n,order_clusters_as_data=F) dend &lt;- color_branches(dend, clusters=clusters, col=cluster_colours[1:n]) clusters &lt;- clusters[order(as.numeric(names(clusters)))] labels(dend) &lt;- rep(&quot;&quot;, length(data_set[,1])) ggd &lt;- as.ggdend(dend) ggd$nodes &lt;- ggd$nodes[!(1:length(ggd$nodes[,1])),] plotPair &lt;- list(ggplot(ggd), ggplot(data_set, aes(V1,V2)) + geom_point(col=cluster_colours[clusters], size=0.2)) return(plotPair) } plotList &lt;- c( hclust_plots(aggregation, 7), hclust_plots(noisy_moons, 2), hclust_plots(noisy_circles, 2), hclust_plots(no_structure, 3) ) pm &lt;- ggmatrix( plotList, nrow=4, ncol=2, showXAxisPlotLabels = F, showYAxisPlotLabels = F, xAxisLabels=c(&quot;dendrogram&quot;, &quot;scatter plot&quot;), yAxisLabels=c(&quot;aggregation&quot;, &quot;noisy moons&quot;, &quot;noisy circles&quot;, &quot;no structure&quot;) ) + theme_bw() pm Figure 10.3: Hierarchical clustering of toy data-sets. 10.3.3 Example: gene expression profiling of human tissues Load required libraries library(RColorBrewer) library(dendextend) Load data load(&quot;data/tissues_gene_expression/tissuesGeneExpression.rda&quot;) Inspect data table(tissue) ## tissue ## cerebellum colon endometrium hippocampus kidney liver ## 38 34 15 31 39 26 ## placenta ## 6 dim(e) ## [1] 22215 189 Compute distance between each sample d &lt;- dist(t(e)) perform hierarchical clustering hc &lt;- hclust(d, method=&quot;average&quot;) plot(hc, labels=tissue, cex=0.5, hang=-1, xlab=&quot;&quot;, sub=&quot;&quot;) Figure 10.4: Clustering of tissue samples based on gene expression profiles. use dendextend library to plot dendrogram with colour labels tissue_type &lt;- unique(tissue) dend &lt;- as.dendrogram(hc) dend_colours &lt;- brewer.pal(length(unique(tissue)),&quot;Dark2&quot;) names(dend_colours) &lt;- tissue_type labels(dend) &lt;- tissue[order.dendrogram(dend)] labels_colors(dend) &lt;- dend_colours[tissue][order.dendrogram(dend)] labels_cex(dend) = 0.5 plot(dend, horiz=T) Figure 10.5: Clustering of tissue samples based on gene expression profiles with labels coloured by tissue type. Define clusters by cutting tree at a specific height plot(dend, horiz=T) abline(v=125, lwd=2, lty=2, col=&quot;blue&quot;) Figure 10.6: Clusters found by cutting tree at a height of 125 hclusters &lt;- cutree(dend, h=125) table(tissue, cluster=hclusters) ## cluster ## tissue 1 2 3 4 5 6 ## cerebellum 0 36 0 0 2 0 ## colon 0 0 34 0 0 0 ## endometrium 15 0 0 0 0 0 ## hippocampus 0 31 0 0 0 0 ## kidney 37 0 0 0 2 0 ## liver 0 0 0 24 2 0 ## placenta 0 0 0 0 0 6 Select a specific number of clusters. plot(dend, horiz=T) abline(v = heights_per_k.dendrogram(dend)[&quot;8&quot;], lwd = 2, lty = 2, col = &quot;blue&quot;) Figure 10.7: Selection of eight clusters from the dendogram hclusters &lt;- cutree(dend, k=8) table(tissue, cluster=hclusters) ## cluster ## tissue 1 2 3 4 5 6 7 8 ## cerebellum 0 31 0 0 2 0 5 0 ## colon 0 0 34 0 0 0 0 0 ## endometrium 0 0 0 0 0 15 0 0 ## hippocampus 0 31 0 0 0 0 0 0 ## kidney 37 0 0 0 2 0 0 0 ## liver 0 0 0 24 2 0 0 0 ## placenta 0 0 0 0 0 0 0 6 10.4 Partitioning methods 10.4.1 K-means 10.4.1.1 Algorithm Pseudocode to illustrate range of different types of data that can be clustered - image segmentation Figure 10.8: Iterations of the k-means algorithm 10.4.1.2 Choosing initial cluster centres library(RColorBrewer) point_shapes &lt;- c(15,17,19) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 1.5 center_point_size = 8 blobs &lt;- as.data.frame(read.csv(&quot;data/example_clusters/blobs.csv&quot;, header=F)) good_centres &lt;- as.data.frame(matrix(c(2,8,7,3,12,7), ncol=2, byrow=T)) bad_centres &lt;- as.data.frame(matrix(c(13,13,8,12,2,2), ncol=2, byrow=T)) good_result &lt;- kmeans(blobs[,1:2], centers=good_centres) bad_result &lt;- kmeans(blobs[,1:2], centers=bad_centres) plotList &lt;- list( ggplot(blobs, aes(V1,V2)) + geom_point(col=point_colours[good_result$cluster], shape=point_shapes[good_result$cluster], size=point_size) + geom_point(data=good_centres, aes(V1,V2), shape=3, col=&quot;black&quot;, size=center_point_size) + theme_bw(), ggplot(blobs, aes(V1,V2)) + geom_point(col=point_colours[bad_result$cluster], shape=point_shapes[bad_result$cluster], size=point_size) + geom_point(data=bad_centres, aes(V1,V2), shape=3, col=&quot;black&quot;, size=center_point_size) + theme_bw() ) pm &lt;- ggmatrix( plotList, nrow=1, ncol=2, showXAxisPlotLabels = T, showYAxisPlotLabels = T, xAxisLabels=c(&quot;A&quot;, &quot;B&quot;) ) + theme_bw() pm Figure 10.9: Initial centres determine clusters. The starting centres are shown as crosses. A, real clusters found; B, convergence to a local minimum. Convergence to a local minimum can be avoided by starting the algorithm multiple times, with different random centres. The nstart argument to the k-means function can be used to specify the number of random sets. 10.4.1.3 Choosing k nstart=50 10.4.2 DBSCAN Density-based spatial clustering of applications with noise 10.4.2.1 Algorithm 10.4.2.2 Choosing parameters 10.4.3 Gene expression tissue types? 10.5 Summary 10.5.1 Applications 10.5.2 Strengths 10.5.3 Limitations 10.6 Exercises Exercise solutions: I Solutions to exercises can be found in appendix I. References "],
["resources.html", "A Resources A.1 Python A.2 Machine learning data set repository", " A Resources A.1 Python scikit-learn A.2 Machine learning data set repository mldata.org This repository manages the following types of objects: Data Sets - Raw data as a collection of similarily structured objects. Material and Methods - Descriptions of the computational pipeline. Learning Tasks - Learning tasks defined on raw data. Challenges - Collections of tasks which have a particular theme. "],
["solutions-linear-models.html", "B Solutions ch. 3 - Linear models and matrix algebra B.1 Exercise 1 B.2 Exercise 2", " B Solutions ch. 3 - Linear models and matrix algebra Solutions to exercises of chapter 3. B.1 Exercise 1 B.2 Exercise 2 "],
["solutions-logistic-regression.html", "C Solutions ch. 4 - Linear and non-linear logistic regression C.1 Exercise 1 C.2 Exercise 2", " C Solutions ch. 4 - Linear and non-linear logistic regression Solutions to exercises of chapter 4. C.1 Exercise 1 C.2 Exercise 2 "],
["solutions-nearest-neighbours.html", "D Solutions ch. 5 - Nearest neighbours D.1 Exercise 1 D.2 Exercise 2", " D Solutions ch. 5 - Nearest neighbours Solutions to exercises of chapter 5. D.1 Exercise 1 D.2 Exercise 2 "],
["solutions-decision-trees.html", "E Solutions ch. 6 - Decision trees and random forests E.1 Exercise 1 E.2 Exercise 2", " E Solutions ch. 6 - Decision trees and random forests Solutions to exercises of chapter 6. E.1 Exercise 1 E.2 Exercise 2 "],
["solutions-svm.html", "F Solutions ch. 7 - Support vector machines F.1 Exercise 1 F.2 Exercise 2", " F Solutions ch. 7 - Support vector machines Solutions to exercises of chapter 7. F.1 Exercise 1 F.2 Exercise 2 "],
["solutions-ann.html", "G Solutions ch. 8 - Artificial neural networks G.1 Exercise 1 G.2 Exercise 2", " G Solutions ch. 8 - Artificial neural networks Solutions to exercises of chapter 8. G.1 Exercise 1 G.2 Exercise 2 "],
["solutions-dimensionality-reduction.html", "H Solutions ch. 9 - Dimensionality reduction H.1 Exercise 1 H.2 Exercise 2", " H Solutions ch. 9 - Dimensionality reduction Solutions to exercises of chapter 9. H.1 Exercise 1 H.2 Exercise 2 "],
["solutions-clustering.html", "I Solutions ch. 10 - Clustering I.1 Exercise 1 I.2 Exercise 2", " I Solutions ch. 10 - Clustering Solutions to exercises of chapter 10. I.1 Exercise 1 I.2 Exercise 2 "],
["references.html", "References", " References "]
]
