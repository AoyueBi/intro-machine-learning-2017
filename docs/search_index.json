[
["nearest-neighbours.html", "5 Nearest neighbours 5.1 Introduction 5.2 Classification 5.3 Regression 5.4 Exercises", " 5 Nearest neighbours 5.1 Introduction memory based and require no model to be fit classification and non-linear regression bias and variance computational load - finding neighbours and storing the entire training set k-d tree / linear search system.time k-d tree search vs linear search library(class) class::knn importance of centering a scaling increase in neighbours - increase in ties 5.1.1 Measuring distance between objects Euclidean distance: \\[\\begin{equation} distance\\left(p,q\\right)=\\sqrt{\\sum_{i=1}^{n} (p_i-q_i)^2} \\tag{5.1} \\end{equation}\\] Figure 5.1: Euclidean distance. 5.2 Classification 5.2.1 Algorithm Figure 5.2: Illustration of k-nn classification. In this example we have two classes: blue squares and red triangles. The green circle represents a test object. If k=3 (solid line circle) the test object is assigned to the red triangle class. If k=5 the test object is assigned to the blue square class. By Antti Ajanki AnAj - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=2170282 5.2.2 Simulated data We will use a simulated data set to demonstrate: bias-variance trade-off the knn function in R plotting decision boundaries choosing the optimum value of k The dataset is partitioned into training and test sets. Load data load(&quot;data/example_binary_classification/bin_class_example.rda&quot;) str(xtrain) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ V1: num -0.223 0.944 2.36 1.846 1.732 ... ## $ V2: num -1.153 -0.827 -0.128 2.014 -0.574 ... str(xtest) ## &#39;data.frame&#39;: 400 obs. of 2 variables: ## $ V1: num 2.09 2.3 2.07 1.65 1.18 ... ## $ V2: num -1.009 1.0947 0.1644 0.3243 -0.0277 ... summary(as.factor(ytrain)) ## 0 1 ## 200 200 summary(as.factor(ytest)) ## 0 1 ## 200 200 library(ggplot2) library(GGally) library(RColorBrewer) point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 ggplot(xtrain, aes(V1,V2)) + geom_point(col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xtest, aes(V1,V2)) + geom_point(col=point_colours[ytest+1], shape=point_shapes[ytest+1], size=point_size) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 5.3: Scatterplots of the simulated training and test data sets that will be used in the demonstration of binary classification using k-nn 5.2.3 knn function For k-nn classification and regression we will use the knn function in the package class. library(class) Arguments to knn train : matrix or data frame of training set cases. test : matrix or data frame of test set cases. A vector will be interpreted as a row vector for a single case. cl : factor of true classifications of training set k : number of neighbours considered. l : minimum vote for definite decision, otherwise doubt. (More precisely, less than k-l dissenting votes are allowed, even if k is increased by ties.) prob : If this is true, the proportion of the votes for the winning class are returned as attribute prob. use.all : controls handling of ties. If true, all distances equal to the kth largest are included. If false, a random selection of distances equal to the kth is chosen to use exactly k neighbours. Perform k-nn on the training set with k=1 knn1train &lt;- class::knn(train=xtrain, test=xtrain, cl=ytrain, k=1) table(ytrain,knn1train) ## knn1train ## ytrain 0 1 ## 0 200 0 ## 1 0 200 cat(&quot;KNN prediction error for training set: &quot;, 1-mean(as.numeric(as.vector(knn1train))==ytrain), &quot;\\n&quot;) ## KNN prediction error for training set: 0 Test data set knn1test &lt;- class::knn(train=xtrain, test=xtest, cl=ytrain, k=1) table(ytest, knn1test) ## knn1test ## ytest 0 1 ## 0 131 69 ## 1 81 119 cat(&quot;KNN prediction error for test set: &quot;, 1-mean(as.numeric(as.vector(knn1test))==ytest), &quot;\\n&quot;) ## KNN prediction error for test set: 0.375 5.2.4 Plotting decision boundaries Since we have just two dimensions we can visualize the decision boundary generated by the k-nn classifier in a 2D scatterplot. Situations where your original data set contains only two variables will be rare, but it is not unusual to reduce a high-dimensional data set to just two dimensions using the methods that will be discussed in chapter 9. Therefore, knowing how to plot decision boundaries will potentially be helpful for many different datasets and classifiers. Create a grid so we can predict across the full range of our variables V1 and V2. gridSize &lt;- 150 v1limits &lt;- c(min(c(xtrain[,1],xtest[,1])),max(c(xtrain[,1],xtest[,1]))) tmpV1 &lt;- seq(v1limits[1],v1limits[2],len=gridSize) v2limits &lt;- c(min(c(xtrain[,2],xtest[,2])),max(c(xtrain[,2],xtest[,2]))) tmpV2 &lt;- seq(v2limits[1],v2limits[2],len=gridSize) xgrid &lt;- expand.grid(tmpV1,tmpV2) names(xgrid) &lt;- names(xtrain) Predict values of all elements of grid. knn1grid &lt;- class::knn(train=xtrain, test=xgrid, cl=ytrain, k=1) V3 &lt;- as.numeric(as.vector(knn1grid)) xgrid &lt;- cbind(xgrid, V3) Plot point_shapes &lt;- c(15,17) point_colours &lt;- brewer.pal(3,&quot;Dark2&quot;) point_size = 2 # grid point 16 # grid point size =0.2 ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtrain, aes(V1,V2), col=point_colours[ytrain+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;train&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) ggplot(xgrid, aes(V1,V2)) + geom_point(col=point_colours[knn1grid], shape=16, size=0.3) + geom_point(data=xtest, aes(V1,V2), col=point_colours[ytest+1], shape=point_shapes[ytrain+1], size=point_size) + geom_contour(data=xgrid, aes(x=V1, y=V2, z=V3), breaks=0.5, col=&quot;grey30&quot;) + ggtitle(&quot;test&quot;) + theme_bw() + theme(plot.title = element_text(size=25, face=&quot;bold&quot;), axis.text=element_text(size=15), axis.title=element_text(size=20,face=&quot;bold&quot;)) Figure 5.4: Binary classification of the simulated training and test sets with k=1. 5.2.5 Bias-variance tradeoff The biasâ€“variance tradeoff is the problem of simultaneously minimizing two sources of error that prevent supervised learning algorithms from generalizing beyond their training set: The bias is error from erroneous assumptions in the learning algorithm. High bias can cause an algorithm to miss the relevant relations between features and target outputs (underfitting). The variance is error from sensitivity to small fluctuations in the training set. High variance can cause an algorithm to model the random noise in the training data, rather than the intended outputs (overfitting). Logarithmic spaced sequence function from emdbook package. lseq &lt;- function(from, to, length.out) { exp(seq(log(from), log(to), length.out = length.out)) } Get log spaced sequence of length 20, round and then remove any duplicates resulting from rounding. s &lt;- unique(round(lseq(1,200,20))) train_error &lt;- sapply(s, function(i){ yhat &lt;- knn(xtrain, xtrain, ytrain, i) return(1-mean(as.numeric(as.vector(yhat))==ytrain)) }) test_error &lt;- sapply(s, function(i){ yhat &lt;- knn(xtrain, xtest, ytrain, i) return(1-mean(as.numeric(as.vector(yhat))==ytest)) }) k &lt;- rep(s, 2) set &lt;- c(rep(&quot;train&quot;, length(s)), rep(&quot;test&quot;, length(s))) error &lt;- c(train_error, test_error) misclass_errors &lt;- data.frame(k, set, error) ggplot(misclass_errors, aes(x=k, y=error, group=set)) + geom_line(aes(colour=set, linetype=set), size=1.5) + scale_x_log10() + ylab(&quot;Misclassification Errors&quot;) + theme_bw() + theme(legend.position = c(0.5, 0.25), legend.title=element_blank(), legend.text=element_text(size=12), axis.title.x=element_text(face=&quot;italic&quot;, size=12)) Figure 5.5: Misclassification errors as a function of neighbourhood size. 5.2.6 Choosing k 5.2.7 Feature selection Error training vs cv vs test 5.3 Regression 5.4 Exercises 5.4.1 Exercise 1 Classification Try different methods of feature selection 5.4.2 Exercise 2 Regression Alzheimers &amp; gene expression? MMSE and gene expression? Solutions to exercises can be found in appendix D. "]
]
