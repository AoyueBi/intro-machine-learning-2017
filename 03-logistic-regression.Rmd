<<<<<<< HEAD
# Linear and non linear (logistic) regression
=======
# Linear and non linear regression and classification {#logistic-regression}
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62

<!-- Chris -->


<<<<<<< HEAD

A large component of machine learning involveds either regression analysis or classification analysis. Within regression analysis we wish to find how a particular continuous variable of interest, y, is influenced by another set of variables, x. Conceptually, this involves identifying a function that tells us the value of y given a observed x, for example, how the expression of a particular gene, y_i, changes over time, or as a function of the expression of itse regulators. In general, many applications in ML can be represented in terms of regression, including differential expression analysis and network inference.

Classificaiton algorithms, on the other hand, deal with discrete (categorical) valued outpus. The aim, here is to the ability to learn how different input values, x, map to a particular group (category), and ultimately to assign categories to a new set of observations for which catagories have no yet been assigned.

Within this chapter we will cover linear and nonlinear regression and classification using examples taken from plant datasets infected with pathogens. In particular we will be investigating the expression levels of the model plant Arabidopsis thaliana following infection with the necrotophic pathogen Botrytis cineara. Botrytis cinerea ...

The processed data is available on GEO under accession number GSE39597. Due to time constraints a pre-processed version of data is available from /data/Arabidopsis/Arabidopsis_Botrytis.csv. The dataset is tab delimited text file with the fist columns containig gene names for 163 marker genes. Columns 2 through 49 contain processed gene expression values (see Windram et al., for full details). The first 24 columns of the data matrix represent contain the control gene expression in Arabidopsis leaves at time points 2h through 48h at 2 hourly intervals; the second 24 columns contain the gene expression levels in Arabidopsis leaves following infection with the necrotophic fungus, Botyris cinerea. 

Excercise 1.1. Load in the data and plot the gene expresison of the genes, to visualise ...

```{r echo=F}
genenames <- colnames(D)
geneindex <- 34
D <- read.csv(file = "/Users/christopher_penfold/Desktop/MLCourse/intro-machine-learning/data/Arabidopsis/Arabidopsis_Botrytis_transpose.csv", header = TRUE, sep = ",", row.names=1)
plot(t(D[geneindex,1:24]),type="p",col="black",ylim=c(min(D[geneindex,])-0.2, max(D[geneindex,]+0.2)),main=genenames[geneindex])
points(t(D[geneindex,25:ncol(D)]),type="p",col="red")
```

install.packages("caret", dependencies = c("Depends", "Suggests"))
library(caret)
library(mlbench)
set.seed(1)

## Regression {#regression}

One of the simplest models to fit to data involves linear regression. Within linear regression we assume we have a simple model with the following form:

$y = mx + c$

For a given set of data, we will typically have a set of observations $\mathbf{x} = \{x_1,x_2,\ldots,x_n\}$ with a corresponding set of observations,  $\mathbf{y} = \{y_1,y_2,\ldots,y_n\}$, with the aim we can fit 

plot(Xs,D[geneindex,25:ncol(D)])

plot(Xs,D[geneindex,25:ncol(D)])
Xs <- seq(from = 2, to = 48, by = 2)
linmod <- train(y~., data=data.frame(x=Xs,y=D[geneindex,25:ncol(D)]), method = "lm")


targetgene = "AT2G28890"

ind=which(colnames(D)==targetgene)
Xs <- seq(from = 2, to = 48, by = 2)
Yvec <- D[25:nrow(D),ind]

lm(AT2G28890~Time, data = D)

lmFit<-train(AT2G28890~Time, data = D, method = "lm")
predictedValues<-predict(lmFit)

plot(Xs,D[25:nrow(D),ind],type="p",col="black",ylim=c(min(D[,ind])-0.2, max(D[,ind]+0.2)),main=genenames[ind])
points(D$Time,predictedValues,type="l",col="black")

summary(lmFit)

model = train(AT2G28890~Time, data = D, method="rpart")

Can do linear/polynomial regression in caret.

Maybe regress gene expression against a regulator? Possible arc into networks. Better bet would be higher dimensional regression, and then look at parameters?

More complex

$y = ax + bx^2 + c$

$y = \sum_i a_i x^t + c$

## Gaussian process regression

Overfitting.

Define nonlinear. Show with some examples. May need to bypass standard definitions due to time constraints. These can be supplementary along with the maths. Here focus on use. 

$y = f(x)$

As a nonparemtric approach, Gaussian processes essentially encode all ..

https://www.r-bloggers.com/gaussian-process-regression-with-r/

f ̄ = k⊤(K+σ2I)−1y, (2.25) ∗∗n
V[f∗] = k(x∗, x∗) − k⊤∗ (K + σn2 I)−1k∗.

logp(y|X) = −1y⊤(K+σ2I)−1y−1log|K+σ2I|−nlog2π

where $f(x)$ represents some potentially nonlinear function. Gaussian process regression represents a way to estimate a distribution over functions ... 

The most comprehensive GP toolboxes are Matlab and Python, however, we can illustrate GPs in R. First we need to load in some packages:

```{r echo=T}
require(MASS)
require(plyr)
require(reshape2)
require(ggplot2)
```

Following blog linke we can define a function to calculate the covariance function:

```{r echo=T}
calcSigma <- function(X1,X2,l=1,sigma=1) {
  Sigma <- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1))
  for (i in 1:nrow(Sigma)) {
    for (j in 1:ncol(Sigma)) {
      Sigma[i,j] <- sigma*exp(-0.5*(abs(X1[i]-X2[j])/l)^2)
    }
  }
  return(Sigma)
}
```

We ca


```{r echo=T}
x.star <- seq(-5,5,len=500) #Define a set of points at which to evaluate the functions
sigma  <- calcSigma(x.star,x.star) #Evaluate the covariance function at those locations, to give the covariance matrix.
```

We can generate samples from the prior and take a look at them:

```{r echo=T}
y1 <- mvrnorm(1, rep(0, length(x.star)), sigma)
y2 <- mvrnorm(1, rep(0, length(x.star)), sigma)
y3 <- mvrnorm(1, rep(0, length(x.star)), sigma)
```

What about inference? What is the GP doing? Essentially, the GP is representing the data in terms of the observation data and the hyperperameters. For simplicity, let's assume we have a function y = sin(x), and we have some observations. For example, we have an observation at x=-2.

```{r echo=T}
f <- data.frame(x=c(-2),
                y=sin(c(-2)))
```

```{r echo=T}
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

We can start to add more observations. Here's what the posterior fit looks like if we include 4 observations (at x in [-4,-2,0,1]):

```{r echo=F}
f <- data.frame(x=c(-4,-2,0,1),
                y=sin(c(-4,-2,0,1)))
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

And with 7 observations:

```{r echo=F}
f <- data.frame(x=c(-4,-3,-2,-1,0,1,2),
                y=sin(c(-4,-3,-2,-1,0,1,2)))
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

We can see that the posterior GP starts to resemble the true underlying function very well. Over most of the x axis the mean of the GP lies very close to the true function and, perhaps more importantly, we have an treatment for the uncertainty. With the GP we can see where the model is relatively sure of the behaviour of the function, and we can see where the model is uncertain. We might, for example want to get observations at x = 4, where the posterior errorbars are greatest.

Another key aspect of GP regression is the ability to evaluate marginal likelihood, sometimes called the "model evidence".  

L = 
```{r echo=T}
calcML <- function(f,l=1,sigma=1) {
  
  f2 <- t(f)
  yt <- f2[2,]
  y  <- f[,2]
  K <- calcSigma(f[,1],f[,1])
  ML <- -0.5*yt%*%ginv(K)%*%y-0.5*log(det(K))-(length(f[,1])/2)*log(2*pi);
  return(ML)
}
```


c(2, 4, 3, 1, 5, 7), # the data elements 
+   nrow=2,              # number of rows 
+   ncol=3,              # number of columns 
+   byrow = TRUE)



#### Tuning of hyperparameters


# Calculate the covariance matrices
# using the same x.star values as above
x <- f$x
k.xx <- calcSigma(x,x)
k.xxs <- calcSigma(x,x.star)
k.xsx <- calcSigma(x.star,x)
k.xsxs <- calcSigma(x.star,x.star)

# These matrix calculations correspond to equation (2.19)
# in the book.
f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs

If we have more datapoints, this allows us to become more certain about the behaviour of the system.

plot(f.star.bar,type = 'l')
lines(f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l')
lines(f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l')

ggplot(values,aes(x=x,y=value)) +
  geom_rect(xmin=-Inf, xmax=Inf, ymin=-2, ymax=2, fill="grey80") +
  geom_line(aes(group=variable)) +
  theme_bw() +
  scale_y_continuous(lim=c(-2.5,2.5), name="output, f(x)") +
  xlab("input, x")


# This time we'll plot more samples.  We could of course
# simply plot a +/- 2 standard deviation confidence interval
# as in the book but I want to show the samples explicitly here.
n.samples <- 50
values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values[,i] <- mvrnorm(1, f.star.bar, cov.f.star)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")

# Plot the results including the mean function
# and constraining data points
fig2b <- ggplot(values,aes(x=x,y=value)) +
  geom_line(aes(group=variable), colour="grey80") +
  geom_line(data=NULL,aes(x=x.star,y=f.star.bar),colour="red", size=1) + 
  geom_point(data=f,aes(x=x,y=y)) +
  theme_bw() +
  scale_y_continuous(lim=c(-3,3), name="output, f(x)") +
  xlab("input, x")

# 3. Now assume that each of the observed data points have some
# normally-distributed noise.

# The standard deviation of the noise
sigma.n <- 0.1

# Recalculate the mean and covariance functions
f.bar.star <- k.xsx%*%solve(k.xx + sigma.n^2*diag(1, ncol(k.xx)))%*%f$y
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx + sigma.n^2*diag(1, ncol(k.xx)))%*%k.xxs

# Recalulate the sample functions
values <- matrix(rep(0,length(x.star)*n.samples), ncol=n.samples)
for (i in 1:n.samples) {
  values[,i] <- mvrnorm(1, f.bar.star, cov.f.star)
}
values <- cbind(x=x.star,as.data.frame(values))
values <- melt(values,id="x")
  
  
  
ggplot(values, aes(x=x,y=value)) + 
   geom_point(data=f,aes(x=x,y=y)) +
  xlab("input, x")



Example GP regression using kernlab

gausspr(Xs, D[25:nrow(D),35], type="classification", kernel="rbfdot",kpar=list(sigma = 0.1),var=1, tol=0.001, cross=0, fit=TRUE, ... , subset, na.action = na.omit)

gpmodel<-gausspr(Xs, D[25:nrow(D),35], type="regression", kernel="rbfdot",kpar=list(sigma = 0.1),var=1, tol=0.001, cross=0, fit=TRUE)



## Fitting time series data using GPs and ML

As with most machine learning approaches, GPs have been incorporated into easy to use packages. For example, we could repeat our example using caret package with the code:

To illustrate example Gaussian Processes let's try to fit a function to a dataset. 


## Differential expression analysis 

Differential expression analysis is a way of determininig whether two sets of data are different. For example, if one measured the expression of a set of genes in two conditions, you could use an appropriate statistical test to determin whether the expression of those genes varied significantly in the two conditions. The most often used test are either Student's t-test or rank based test. Both tests however, are not appropriate for time series data, in which we have temporal information. 

Gaussian processes represent a useufl way of test for the differences in genes expression for time series observations. Here we are using the machine learning approaches to fit a model, first to the one time series, the to the second time series, and finally to a combination of the time series. A related example of this kind of approach was introduced by Stegle et al. (2010), with extensions introduced by Rattray et al. (2016) and Penfold, Sybirna et al. (2017).

Let's introduce 

## Gaussian Processes for time local models


Stegle et al. Rattray et al.

install.packages("devtools")
library(devtools)
install_github("ManchesterBioinference/DEtime")

import(DEtime)

library(DEtime)


res <- DEtime_infer(ControlTimes = Xs, ControlData = D[1:24,2], PerturbedTimes = Xs, PerturbedData = D[25:48,2])
print_DEtime
plot_DEtime(res)

res_rank <- DEtime_rank(ControlTimes = ControlTimes, ControlData = ControlData, PerturbedTimes, PerturbedData=PerturbedData, savefile=TRUE)
idx <- which(res_rank[,2]>1)


res <- DEtime_infer(ControlTimes = Xs, ControlData = t(D[1:24,]), PerturbedTimes = Xs, PerturbedData = t(D[25:48,]))
print_DEtime
plot_DEtime(res)

hist(as.numeric(res$result[,2]))

hist(as.numeric(res$result[,2]),breaks=20)

### inport simulated dataset
data(SimulatedData)

### go on with the perturbation time point inference
res <- DEtime_infer(ControlTimes = ControlTimes, ControlData = ControlData, PerturbedTimes = PerturbedTimes, PerturbedData = PerturbedData)

### Print a summary of the results
print_DEtime(res)
### plot results for all the genes
plot_DEtime(res)

## Classificaiton

Importance of classifiers.

Infer regulators that identify infection levels in plants.

## Logistic regression

library(caret)

In linear regression we tried to predict the value of y(i)
 for the i
‘th example x(i)
 using a linear function y=hθ(x)=θ⊤x.
. This is clearly not a great solution for predicting binary-valued labels (y(i)∈{0,1})
. In logistic regression we use a different hypothesis class to try to predict the probability that a given example belongs to the “1” class versus the probability that it belongs to the “0” class. Specifically, we will try to learn a function of the form:

## GP classification

Example GP regression using kernlab

=======
## Exercises

Solutions to exercises can be found in appendix \@ref(solutions-logistic-regression).
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62
