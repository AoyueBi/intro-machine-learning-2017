<<<<<<< HEAD
# Linear and non linear (logistic) regression
=======
# Linear and non linear regression and classification {#logistic-regression}
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62

<!-- Chris -->

<<<<<<< HEAD

A large component of machine learning involveds either regression analysis or classification. Within the regression framework we wish to find how a particular continuous variable of interest, y, is influenced by another set of variables, x. Conceptually, this involves identifying a function that tells us the value of y given a observed x, for example, how the expression of a particular gene, y_i, changes over time, or as a function of the expression of itse regulators. In general, many applications in ML can be represented in terms of regression, including differential expression analysis and network inference.

Classificaiton algorithms, on the other hand, deal with discrete (categorical) valued outpus. The aim, here is to the ability to learn how different input values, x, map to a particular group (category), and ultimately to assign categories to a new set of observations for which catagories have no yet been assigned.

Within this chapter we will cover linear and nonlinear regression and classification using examples taken from plant datasets infected with pathogens. In particular we will be investigating the expression levels of the model plant Arabidopsis thaliana following infection with the necrotophic pathogen Botrytis cineara. Botrytis cinerea ...

The processed data is available on GEO under accession number GSE39597. Due to time constraints a pre-processed version of data is available from /data/Arabidopsis/Arabidopsis_Botrytis.csv. The dataset is tab delimited text file with the fist columns containig gene names for 163 marker genes. Columns 2 through 49 contain processed gene expression values (see Windram et al. 2012, for full details). The first 24 columns of the data matrix represent contain the control gene expression in Arabidopsis leaves at time points 2h through 48h at 2 hourly intervals; the second 24 columns contain the gene expression levels in Arabidopsis leaves following infection with the necrotophic fungus, Botyris cinerea. 

Excercise 3.1. Load in the data and plot the gene expresison of the genes, to visualise.

## Regression {#regression}

One of the simplest models to fit to data is linear regression. Linear regression assumes an output, $y$, depends on the input variable, $x$, via:

$y = mx + c.$

For a typically set of data, we typically have a set of input variables $\mathbf{x} = \{x_1,x_2,\ldots,x_n\}$ with a corresponding set of observations, $\mathbf{y} = \{y_1,y_2,\ldots,y_n\}$. The aim is to infer the parameters $m$ and $c$. Exmaples?

Within R, linear regression can be implemented via the lm function. For example, we can perform linear regression for the gene AT2G28890 as a function of time:

```{r echo=F}
lm(AT2G28890~Time, data = D[25:nrow(D),])
```

Within the caret package, linear regression can be performed by calling the function lm. In the snippet of code, below, we load caret and perform linear regression:

```{r echo=F}
library(caret)
library(mlbench)
set.seed(1)

lrfit <- train(y~., data=data.frame(x=Xs,y=D[25:nrow(D),geneindex]), method = "lm")
predictedValues<-predict(lrfit)
summary(lrfit)
```

Finally, we can plot the fit to the control dataset as well and plot the outputs:

```{r echo=T}
lrfit2 <- train(y~., data=data.frame(x=Xs,y=D[1:24,geneindex]), method = "lm")
predictedValues2<-predict(lrfit2)

plot(Xs,D[25:nrow(D),geneindex],type="p",col="black",ylim=c(min(D[,geneindex])-0.2, max(D[,geneindex]+0.2)),main=genenames[geneindex])
points(Xs,D[1:24,geneindex],type="p",col="red")
points(Xs,predictedValues,type="l",col="black")
points(Xs,predictedValues2,type="l",col="red")
```

For this particular gene linear regression appears to have done a reasonable job identifying the trend for how gene expression of AT2G28890 changes over time. In general, however, linear regression will not typically provide a good fit for all genes.

Excercise 3.2. Try fitting to some other genes to identify some whose expression profile doesn't change linearly over time.

Excercise 3.3. Linear regression can generally be applied for any number of variables. A notable example, would be to regress the expression pattern of a gene against putative regulators.

```{r echo=T}
lrfit3 <- train(y~., data=data.frame(x=D[1:24,3:10],y=D[1:24,geneindex]), method = "lm")
```

In general, ... linear regression can be used to network inference (Oates and Mukherjee, 2012).

## Gaussian process regression

Overfitting. Nonlinear Bayesian approaches are often used for nonlinear regression. In particular, Gaussian processes (Rasmussen and Williams) attempt to model the output variable, y, as a nonlinear function of the input variable, x:

$y = f(x)$

$f(x) \sim \mathcal{GP}(m(x), k(x,x^\prime))$

where $f(x)$ represents some potentially nonlinear function. Gaussian process regression represents a way to estimate a distribution over functions ... A gaussian process can be used to 

Given a set of observations, $(\mathbf{x},\mathbf{y})$ GP regression can be used to infer the expression, $\mathbf{y}^*$, at a new input location $\mathbf{x}^*$.

$f_* = k_*^\top(K)^{-1} y$
$c_* = k(x_*,x_*)^{-1} - k_*^\top (K)^{-1} k_*$

Although the most complete GP toolboxes can be found in Matlab and Python, some GP toolboxes exist in R. In particular, notable resources such as [link](https://www.r-bloggers.com/gaussian-process-regression-with-r/) have implemented a variety of functions from the gpml package. To better understand GPs we first load in some required packages.

```{r echo=T}
require(MASS)
require(plyr)
require(reshape2)
require(ggplot2)
```

In the snippet of code, below, we implement a squared exponential covariance function, defined as:

$C(x,x^\prime) = \sigma^2 \exp\biggl{(}\frac{(x-x^\prime)^2}{2l^2}\biggr{)}$. We can implement this:

```{r echo=T}
covSE <- function(X1,X2,l=1,sig=1) {
  K <- matrix(rep(0, length(X1)*length(X2)), nrow=length(X1))
  for (i in 1:nrow(K)) {
    for (j in 1:ncol(K)) {
      K[i,j] <- sig^2*exp(-0.5*(abs(X1[i]-X2[j]))^2 /l^2)
    }
  }
  return(K)
}
```

... To better understand GPs, we can generate samples from the GP prior:

```{r echo=T}
x.star <- seq(-5,5,len=500) #Define a set of points at which to evaluate the functions
sigma  <- covSE(x.star,x.star) #Evaluate the covariance function at those locations, to give the covariance matrix.
y1 <- mvrnorm(1, rep(0, length(x.star)), sigma)
y2 <- mvrnorm(1, rep(0, length(x.star)), sigma)
y3 <- mvrnorm(1, rep(0, length(x.star)), sigma)
plot(y1,type = 'l',ylim=c(min(y1,y2,y3),max(y1,y2,y3)))
lines(y2)
lines(y3)
```

When we specify a GP, we are essentially encoding a whole range of functions. What that function looks like depends on the type of covariance function and the hyperparameters. To get a feel for this you can try changing the hyperparameters in the above code. The squared exponential covariannce function we have encoded specifies very smooth functions (infinitely differentiable). A variety of other covariance functions exist, and can be found, with examples found in the [Kernel Cookbook](http://www.cs.toronto.edu/~duvenaud/cookbook/).

Excercise 3.4 (optional): Try implementing another covariance function from the [Kernel Cookbook](http://www.cs.toronto.edu/~duvenaud/cookbook/) and generating samples from the GP prior.

### Inference using Gaussian process regression

We can generate samples from the GP prior, but what about inference? In liner regression we aimed to infer the parameters, $m$ and $a$. What is the GP doing? Essentially, iot's representing the function in terms of the observed data (and the hyperperameters). To demonstrate this, let's assume we have an uknown function (we will generate this via $y = sin(x)$). Although we don't know what the function is, we might have some observations e.g., we have one observation at $x=-2$:

```{r echo=T}
f <- data.frame(x=c(-2),
                y=sin(c(-2)))
```

We can infer a posterior GP (and plot this against the true underlying function in red):

```{r echo=T}
x <- f$x
k.xx <- covSE(x,x)
k.xxs <- covSE(x,x.star)
k.xsx <- covSE(x.star,x)
k.xsxs <- covSE(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

Of course the fit is not particularly good, but we only had one observation. Crucially, the GP encodes uncertainty, so although the model fit is not particularly good, we can see that it is not particularly good.  

Excercise 3.5 (optional): Try plotting some sample function from the posterior GP.

We can start to add more observations. Here's what the posterior fit looks like if we include 4 observations (at x in [-4,-2,0,1]):

```{r echo=F}
f <- data.frame(x=c(-4,-2,0,1),
                y=sin(c(-4,-2,0,1)))
x <- f$x
k.xx <- covSE(x,x)
k.xxs <- covSE(x,x.star)
k.xsx <- covSE(x.star,x)
k.xsxs <- covSE(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

And with 7 observations:

```{r echo=F}
f <- data.frame(x=c(-4,-3,-2,-1,0,1,2),
                y=sin(c(-4,-3,-2,-1,0,1,2)))
x <- f$x
k.xx <- covSE(x,x)
k.xxs <- covSE(x,x.star)
k.xsx <- covSE(x.star,x)
k.xsxs <- covSE(x.star,x.star)

f.star.bar <- k.xsx%*%solve(k.xx)%*%f$y  #Mean
cov.f.star <- k.xsxs - k.xsx%*%solve(k.xx)%*%k.xxs #Var

plot(x.star,sin(x.star),type = 'l',col="red",ylim=c(-2.2, 2.2))
points(f,type='o')
lines(x.star,f.star.bar,type = 'l')
lines(x.star,f.star.bar+2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
lines(x.star,f.star.bar-2*sqrt(diag(cov.f.star)),type = 'l',pch=22, lty=2, col="black")
```

We can see that even with only 7 observations the posterior GP has begun to resemble the true (nonlinear) underlying function very well. Over most of the $x$-axis the mean of the GP lies very close to the true function and, perhaps more importantly, we have an treatment for the uncertainty. With the GP we can see where the model is relatively sure of the behaviour of the function, and we can see where the model is uncertain. We might, for example want to get observations at $x = 4$, where the posterior errorbars are greatest.

#### Marginali Likelihood

Another key aspect of GP regression is the ability to evaluate marginal likelihood, otherwise referred to as the "model evidence". We can calculate the marginal likelihood using the snippet of code below:

$\ln p(\mathbf{y}|\mathbf{x}) = -\frac{1}{2}\mathbf{y}^\top (K)^{-1} \mathbf{y} -\frac{1}{2} \ln |K| - \frac{n}{2}\ln 2\pi$

```{r echo=T}
calcML <- function(f,l=1,sig=1) {
  f2 <- t(f)
  yt <- f2[2,]
  y  <- f[,2]
  K <- covSE(f[,1],f[,1],l,sig)
  ML <- -0.5*yt%*%ginv(K+0.1^2*diag(length(y)))%*%y -0.5*log(det(K)) -(length(f[,1])/2)*log(2*pi);
  return(ML)
}
```

#### Optimising hyperparameters

Crucially, by optimising the marginal we automatically select the hyperparameters. In the example below we increment the length-scale hyperparameter. 

```{r echo=T}
#install.packages("plot3D")
library(plot3D)

par <- seq(.1,10,by=0.1)
ML <- matrix(rep(0, length(par)^2), nrow=length(par), ncol=length(par))
for(i in 1:length(par)) {
  for(j in 1:length(par)) {
    ML[i,j] <- calcML(f,par[i],par[j])
  }
}
persp3D(z = ML,theta = 120)
ind<-which(ML==max(ML), arr.ind=TRUE)
print(c("length-scale", par[ind[1]]))
print(c("process variance", par[ind[2]]))
```

Excercise 3.1: Try fitting plotting the GP for the optimised values of the hyperparmeters. 

Within GPs we can acta

Excercise 3.1: Try fitting a Gaussian process to one of the gene expression profiles in the Botrytis dataset. Hint: identify a gene with good dynamics.

### Model Selection

The marginal likelihood can additionally be used as a basis for selecting models. For example studies by Stegle et al. (2010) used Gaussian processes to identify differential expression between two time series e.g., between a control time series and an infection time series. 

Differential expression analysis is a way of determininig whether two sets of data are different. For example, if one measured the expression of a set of genes in two conditions, you could use an appropriate statistical test to determin whether the expression of those genes varied significantly in the two conditions. The most often used test are either Student's t-test or rank based test. Both tests however, are not appropriate for time series data, in which we have temporal information. 

Gaussian processes represent a useufl way of test for the differences in genes expression for time series observations. Here we are using the machine learning approaches to fit a model, first to the one time series, the to the second time series, and finally to a combination of the time series. A related example of this kind of approach was introduced by Stegle et al. (2010), with extensions introduced by Rattray et al. (2016) and Penfold, Sybirna et al. (2017).

Excercise 3.1: Use GPs for model selection.

### Differential expression analysis as model selection

The application in the previous section. We have 

Stegle et al. Rattray et al.

```{r echo=T}
#install.packages("devtools")
library(devtools)
#install_github("ManchesterBioinference/DEtime")
#import(DEtime)
library(DEtime)
```


```{r echo=F}
res <- DEtime_infer(ControlTimes = Xs, ControlData = D[1:24,3], PerturbedTimes = Xs, PerturbedData = D[25:48,3])
print_DEtime
plot_DEtime(res)
```


```{r echo=F}
res <- DEtime_infer(ControlTimes = Xs, ControlData = D[1:24,3], PerturbedTimes = Xs, PerturbedData = D[25:48,3])
print_DEtime
plot_DEtime(res)
```

```{r echo=F}
res_rank <- DEtime_rank(ControlTimes = Xs, ControlData = D[1:24,3], PerturbedTimes = Xs, PerturbedData = D[25:48,3], savefile=TRUE)
idx <- which(res_rank[,2]>1)
```


```{r echo=F}
res <- DEtime_infer(ControlTimes = Xs, ControlData = t(D[1:24,]), PerturbedTimes = Xs, PerturbedData = t(D[25:48,]))
hist(as.numeric(res$result[,2]))
hist(as.numeric(res$result[,2]),breaks=20)
```


## Classificaiton

Importance of classifiers.

Infer regulators that identify infection levels in plants.

## Logistic regression

```{r echo=F}
library(caret)
data(GermanCredit)
Train <- createDataPartition(GermanCredit$Class, p=0.6, list=FALSE)
training <- GermanCredit[ Train, ]
testing <- GermanCredit[ -Train, ]
mod_fit <- train(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + 
                   CreditHistory.Critical,  data=training, method="glm", family="binomial")
                   exp(coef(mod_fit$finalModel))
                   
                   mod_fit_one <- glm(Class ~ Age + ForeignWorker + Property.RealEstate + Housing.Own + 
                     CreditHistory.Critical, data=training, family="binomial")

mod_fit_two <- glm(Class ~ Age + ForeignWorker, data=training, family="binomial")

library(pROC)
# Compute AUC for predicting Class with the variable CreditHistory.Critical
f1 = roc(Class ~ CreditHistory.Critical, data=training) 
plot(f1, col="red")
## 
## Call:
## roc.formula(formula = Class ~ CreditHistory.Critical, data = training)
## 
## Data: CreditHistory.Critical in 180 controls (Class Bad) < 420 cases (Class Good).
## Area under the curve: 0.5944
library(ROCR)
# Compute AUC for predicting Class with the model
prob <- predict(mod_fit_one, newdata=testing, type="response")
pred <- prediction(prob, testing$Class)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc
```




## GP classification

Example GP regression using kernlab

```{r echo=F}
t2 <- train(y~., data=data.frame(x=Xs,y=D[25:nrow(D),geneindex]), "gaussprPoly")
```


[Caret examples](https://github.com/tobigithub/caret-machine-learning)
[GPML](http://www.gaussianprocess.org/gpml/code/matlab/doc/)
[GPy](https://github.com/SheffieldML/GPy)
[GPflow](http://gpflow.readthedocs.io/en/latest/intro.html)
[GPflowopt](https://github.com/GPflow/GPflowOpt)
[Tensorflow](https://www.tensorflow.org)
[Theano](http://deeplearning.net/software/theano/)

=======
## Exercises

Solutions to exercises can be found in appendix \@ref(solutions-logistic-regression).
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62
