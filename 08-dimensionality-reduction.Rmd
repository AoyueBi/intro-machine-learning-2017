<<<<<<< HEAD
<!-- Chris -->

# Dimensionality reduction
=======
# Dimensionality reduction {#dimensionality-reduction}
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62

Systems level measurments of gene expression from collections of microarrays and RNA-sequencing experiments are increasingly used to understand complex biologial systems. In the past, studies based on bulk measurments have tended to provide observations for many variables (expression of many genes) with relatively few samples e.g., time points or conditions. This imbalance between the number of variables and the number of observatins is referred to as large p, small n. Further to this, the presence of noise in the system means that statistical analyses can be difficult.

With the rise of single cell RNA-sequencing (scRNA-seq), the scales of these datasets have shifted, providing measurments of many variables, with many samples, albeit from potentially heterogeneous population. The first scRNA-sequencing was done in 200? (Tang et al., ...) generating only one observation whereas in 2017, it is now routine to generate datasets with tens to hundreds of thousands of cells (Svensson, 2017). Indeed, the recent 10x Genomics million cell experiment (https://community.10xgenomics.com/t5/10x-Blog/Our-1-3-million-single-cell-dataset-is-ready-to-download/ba-p/276) sequenced over 1.3 million cells taken from the cortex, hippocampus and ventricular zone of embryonic mice. A key goal when dealing with datasets of these magnitude is the exploratory analysis to identify key subpopulations that may have been undetected using previous bulk measurements; another key goal lies in the identification the progression inherent to the dataset, by putting the cells into a particular ordering.

Datasets that are large, either in terms of the number of variables, or the number of observations, are inherently difficult for humans to develop an intuition for. A useuful array of techniques have therefore been developed to deal with such large datasets, aimed at dimensionality reduction. This aims to summarise the system which may contain tens of thousands of genes into a handful (typically two or three) informative variables, which are more intuitive space for a human to form a hypothesis. 

Whilst dimensionality reduction allows humans to inspect the datasets, particularly when the data can be represented in two or three dimensions, it should be noted that humans are exceptionally good at identifying patterns in two or three dimensional data, even when no real structure exists. For example, seeing an image of the UK in a cloud (Figure X). In these cases, it is often useful to employ other statistical machine learning approaches to search for patterns over the reduced dimensional space. In this sense, dimensionality reduction forms an important component of a moddern statistical analyses that will typically combined a variety of machine learning techniques, such as classification, regression and clustering. 

In this chapter we will explore two forms of dimensionality reduction, principle component analysis (PCA) and t-distributed stochastic neighbor embedding (tSNE), highlighting the advantages and potential pitfalls of both methods. As an illustrative example, we will use these approaches to analyse some single cell RNA-sequencing data of early human development, ...

In section X we discuss how dimensionality reduction tecniques can be combiend with other ML algorithms to perform pseudotime analyses in collections of scRNA-seq data, so-called pseudotime approaches. Finally, we will explore more recent developments in pseudotime analysis by running ... 
## Linear Dimensionality Reduction

The most widely used form of dimensionality reduction is based on principle component analysis (PCA), which was introduced by Pearson in the early 1900's (cite), and indepedently discovered by Hotelling (1930). PCA has a long history of use in the biological sciences with early uses in the analysis of gene expression data (Vohradsky et al. 1997, Craig et al. 1997, Hilsenbeck et al. 1999).

PCA is not a dimensionality reduction technique per se, but an alternative way of representing the data that more naturally captures the variance in the system. Specifically, it finds a new co-ordinate system, so that the new x-axis (the first principle component) is aligned along the direcion of greatest variance, with the y-axis aligned along the direction of second greatest variance (the second principle component), and so forth (Figure Figure \@ref(fig:PCAfig). At this stage there has been no inherent reduction in the dimensionality of the system, we have simply found a more natural way to represent the data.

```{r PCAfig, fig.cap='Illustration of PCA', out.width='80%', fig.asp=.75, fig.align='center'}
par(mar = c(4, 4, .1, .1))
plot(pressure, type = 'b', pch = 19)
```

So why is this useuful? In many biological systems, we find that most of the variance in the system can be captured by relatively few principle components. That is, later principle components tend to be uninformative, and can be discarded. Typically, we might visualise on the first two or three principle components, allowing us to visualise a high-dimensional system in two or three dimensions.

#### PCA in R

PCA can be applied in R using the prcomp function, using the following syntax:

pcaresults <- prcomp(X)

A number of outputs will be returned:

B$x - Positions of the datapoints in the new co-ordiante system.
pcaresults$sdev - the variance explained by each PCA axes.
B$rotation - projection of original axes (in our case genes) in the new PCA co-ordiante system. For visualising what genes are associated with particular PCs.

### Horeshoe effect

Principle component analysis is a linear (dimensionality reduction) technique, and is not always appropriate when dealing with nonlinearities in datasets. To illustrate this, let's consider an simulated expression set containig 8 genes, with N timepoints. We can again represnt this in terms of a matrix (below): 

> X <- matrix( c(2,4,2,0,0,0,0,0,0,0,
                 0,2,4,2,0,0,0,0,0,0,
                 0,0,2,4,2,0,0,0,0,0,  
                 0,0,0,2,4,2,0,0,0,0,   
                 0,0,0,0,2,4,2,0,0,0,    
                 0,0,0,0,0,2,4,2,0,0,   
                 0,0,0,0,0,0,2,4,2,0,  
                 0,0,0,0,0,0,0,2,4,2), nrow=8,  ncol=10, byrow = TRUE)  

> X

plot(1:10,X[1,],type="l",col="red",xlim=c(0, 14))
points(1:10,X[2,],type="l",col="blue")
points(1:10,X[5,],type="l",col="black")
legend(8, 4, legend=c("gene 1", "gene 2", "gene 5"), col=c("red", "blue", "black"),lty=1, cex=0.8)

Not that, from the matrix, we can see a simple pattern in the data, with the expression pattern of later genes representing a simple time shifted version of the previous gene. Intuitively, then, this data can be represented by a single dimension e.g., an ordering that runs from time point 1 to time point 10. Let's run PCA and visualise what we get out:

B<-prcomp(t(X))
condnames = c('TP1','TP2','TP3','TP4','TP5','TP6','TP7','TP8','TP9','TP10')
plot(B$x[,1:2],type="p",col="red",xlim=c(-5, 5),ylim=c(-5, 5))
text(B$x[,1:2]+0.5, condnames, , cex = 0.7)

If we visualise the data in two dimensions, we see that the PCA plot has, in fact, placed time point 1 very close to time point 10. But why? From the earlier plot of gene expression profiles we can see that the relationships between the various genes are not enirely straightforward. For example, gene 1 is initially correlated with gene 2, then negatively correlated, and finally uncorrelated, whilst no correlation exists between gene 1 and genes 5 - 8. In general, PCA attemtps to preserve large pairwise distances, to do so for all genes, we observe the well know horsehoe effect (Novembre & Stephens 2008, Reich et al. 2008). These types of artifacts may be problematic when trying to draw intepretations on the data, and we must take care when we see these types of effects.  

## PCA analysis of mammalian development

Now that we have a feel for PCA and understand how the basic commands work we can take a look at some real data. To illustrate this we will make use of preprocessed data taken from Yan et al. (2015; GEO ) and Guo et al. (2015; GEO ). The data from Yan et al. represents single cell RNA-seq data of from human embryos from the zygote stage (a single cell produced following fertilisation of an egg) through to the blastocyst stage (consisting of around 64 cells), as well as human embryonic stem cells (hESC), cells extracted from an early blsatocyst stage embryo and maintained in vitro. The dataset of Guo et al. (2015) contains scRNA-seq data from human primordial germ cells (hPGCs), precursors of sperm or eggs that are specified early in the developing human embryo soon after implantation (around week 2-3 in humans), and somatic cells. Together, these datasets therefore provide useful insights into the ...

The preprocessed data contains log_2 normalised counts in around 400 cells for 300X marker genes, and can be found in the file XXX.csv. Note that the first line of data in the file is an indicator denoting cell type (-1 = ESC, 0 = pre-implantation, 1 = PGC, 2 = somatic cell). The second row indicates the sex of the cell (0 = unknown/unlabelled, 1 = female, 2 = male), with the third row indicating capture time (-1 = ESC, 0 - 7 denotes various developmental stages from zygote to blastocyst, 8 - X indicates various times of embryo development from week 4 through to week 19).

Excercise 8.1. First load in the expression data into R and plot some example expression patterns.

Excercise 8.2. Use perform prcomp to perfom PCA on the data.

Excercise 8.3. Try plotting the loadings for the genes. Can we identify any genes of interest that may be particularlry important for PGCs?

Excercise 8.4. Does the data sepearate well? Perform k-means cluster analysis on the data.

Excercise 8.5. Perform a differential expression analysis between blastocyst cells and the PGCs.

## Nonlinear Dimensionality Reduction

Whilst PCA is extremely useuful for exploratory analysis, it is not always appropriate, particularly for datasets with nonlinearites in the data. A large number of nonlinear dimensinoality reduction techniques exist, inlcuding t-distributed stochastic neighbour emedding (tSNE). The tSNE has become one of the most widely used algorithms for exploration of high dimensional (nonlinear) datasets, and particularly single cell transcriptomics.

The tSNE algorithm aims to take points in a high-dimensional space and find a faithful representation of those points in a lower-dimensional space.

#### tSNE in R

The tSNE algorithm can be run in R using the Rtsne function. Unlike the earlier call to PCA, tSNE 


pcaresults <- prcomp(X)

A number of outputs will be returned:

B$x - Positions of the datapoints in the new co-ordiante system.
pcaresults$sdev - the variance explained by each PCA axes.
B$rotation - projection of original axes (in our case genes) in the new PCA co-ordiante system. For visualising what genes are associated with particular PCs.

#### Perplexity parameter and stochasticity

Unlike pca, which has no real free parameters (different wasy of doing it)  tSNE has a variety of parameters that will need to be set. First, we have the perplexity parameter which, in essence, balances local and global aspects of the data. For low values of perplexity, the algorithm will tend to entirely focus on keeping datapoints locally together, whilst high perplexity will tend to focus on conserving

“perplexity,” which says (loosely) how to balance attention between local and global aspects of your data.

#### Stochasticity

(https://distill.pub/2016/misread-tsne/)


#### Example on single cell datasets

In our previous sections we used PCA to analyse single cell datasets, and noted that it seemed adept at picking out difference cell types and idetifying putative regulators. We will now use tSNE to analyse the same data.

Excercise 8.6. Load in the single cell dataset from section ??? and run tSNE. Note: try a variety of perplexity values.

D <- read.csv(file = "/Users/christopher_penfold/Desktop/MLCourse/intro-machine-learning/data/PGC_transcriptomics/PGC_transcriptomics.csv", header = TRUE, sep = ",", row.names=1)

y1 <- B$x[which(D[1,]==-1),1:2]
y2 <- B$x[which(D[1,]==0),1:2]
y3 <- B$x[which(D[1,]==1),1:2]
y4 <- B$x[which(D[1,]==2),1:2]

plot(y1,type="p",col="red",xlim=c(-45, 45),ylim=c(-45, 45))
points(y2,type="p",col="black")
points(y3,type="p",col="blue")
points(y4,type="p",col="green", legend=c("Line 1"))
legend(-40, 40, legend=c("ESC", "preimp", "PGC", "soma"), col=c("red", "black", "blue", "green"), pch="o", bty="n", cex=0.8)


plot(B$rotation[,1:2],type="n",xlim=c(-0, 0.07),ylim=c(0.02, 0.1))
genenames <- rownames(D)
genenames <- genenames[4:nrow(D)]
text(B$rotation[,1:2], genenames, , cex = .4)



plot(B$rotation[,1:2],type="n",xlim=c(-0, 0.07),ylim=c(-0.04, -0.1))
genenames <- rownames(D)
genenames <- genenames[4:nrow(D)]
text(B$rotation[,1:2], genenames, , cex = .4)


D <- read.csv(file = "/Users/christopher_penfold/Desktop/MLCourse/intro-machine-learning/data/PGC_transcriptomics/PGC_transcriptomics.csv", header = TRUE, sep = ",", row.names=1)


tsne_model_1 = Rtsne(as.matrix(t(D)), check_duplicates=FALSE, pca=TRUE, perplexity=100, theta=0.5, dims=2)

y1 <- tsne_model_1$Y[which(D[1,]==-1),1:2]
y2 <- tsne_model_1$Y[which(D[1,]==0),1:2]
y3 <- tsne_model_1$Y[which(D[1,]==1),1:2]
y4 <- tsne_model_1$Y[which(D[1,]==2),1:2]

plot(y1,type="p",col="red",xlim=c(-45, 45),ylim=c(-45, 45))
points(y2,type="p",col="black")
points(y3,type="p",col="blue")
points(y4,type="p",col="green")
legend(-40, 40, legend=c("ESC", "preimp", "PGC", "soma"), col=c("red", "black", "blue", "green"),pch="o", bty="n", cex=0.8)

Note that for higher level of perplexity, the algorithm seems to do a good job of seperating out the different cell types. Unlike PCA analysis, tSNE appears to suggest more structure in the dataset, in particular we note that the pre-implantation observations seperate out.

Excercise 8.7. There appears to be more structure in the dataset than PCA. Take a look at the pre-implantation cells. Note that we actually have a variety of cells here, from oocytes through to blastocyst stage. The developmental stage is indicated in row three of the data, try plotting the data as a heatmap to see if this has any bearing on ...

Excercise 8.8. More formally we could try to cluster these data. 

y2_1 <- tsne_model_1$Y[which(D[1,]==0 & D[3,]==1),1:2]
y2_2 <- tsne_model_1$Y[which(D[1,]==0 & D[3,]==2),1:2]
y2_3 <- tsne_model_1$Y[which(D[1,]==0 & D[3,]==3),1:2]
y2_4 <- tsne_model_1$Y[which(D[1,]==0 & D[3,]==4),1:2]
y2_5 <- tsne_model_1$Y[which(D[1,]==0 & D[3,]==5),1:2]
y2_6 <- tsne_model_1$Y[which(D[1,]==0 & D[3,]==6),1:2]

plot(y2_1,type="p",col="tomato",xlim=c(-10, 0),ylim=c(-10, 10))
points(y2_2,type="p",col="tomato1")
points(y2_3,type="p",col="tomato1")
points(y2_4,type="p",col="tomato2")
points(y2_5,type="p",col="tomato3")
points(y2_6,type="p",col="tomato4")
legend(-10, 10, legend=c("Ooc", "Zyg", "2C", "4C","Mor","Blast"), col=c("tomato", "tomato1", "tomato1", "tomato2","tomato3","tomato4"),pch="o", bty="n", cex=0.8)

### Other dimensionality reduction techniques

A large number of alternative dimensionality reduction techniques exist with corresponding codebases in R. These include other nonliner dimensionality reduction techniques such as Gaussian Process Latent Variable Models (GPLVMs; Lawrence 2004; https://github.com/SheffieldML/vargplvm.git) and Isomap. RESOURCES.

## Dimensionality reduction and pseudotime algorithsm

Primer on pseudotime.

https://hemberg-lab.github.io/scRNA.seq.course/index.html

tSNE is a particularly useful approach when dealing with large (potentially nonliner) datasets with unknown strucutre. It is frequently used as a starting point for pseudotime algorithms. Reference Martin Hembergs course for intro to pseudotime.

However, as we have seen, there remains some intepretability issues in using these approaches for pseudotime ordering. In partciular the flexibility of the nonlinear warping means that the intepretation of the odering can be difficult. Furthermore, since most algorithms will artifically truncate the ...

An alternative apporach that is particularly useful when dealing with time series annotated datasets attempts to tie the latent dimension to a meanigful parameter. Specifically, these approaches attempt to reduce teh dimension down to a single dimension, with that dimension anchored into to time points (Figure CX). Consequently, this approach ... An example of this type of pseudotime ordering can be found in the DeLorean package of (Reid and Wernisch, 2016). Of course, ...

https://cran.r-project.org/web/packages/DeLorean/index.htmllibrary(DeLorean)

<<<<<<< HEAD

Solutions to exercises can be found in appendix \@ref(solutions-dimensionality-reduction).
>>>>>>> 634734233afdb3dfc9a40a09faa8cc06d917ce62
