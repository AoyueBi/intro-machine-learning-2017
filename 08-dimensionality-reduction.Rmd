<!-- Chris -->

# Dimensionality reduction

Systems level measurments such as those generated by collections of microarrays or RNA-sequencing experiments are increasingly used to understand complex biologial systems. Studies based on bulk RNA-sequencing have tended to provide measurments for many variables (genes) with relatively few samples e.g., time points, conditions. The statistical analysis of these datasets have tended to be difficult due to the large p, small n problem.

With the rise of single cell RNA-sequencing (scRNA-seq), the scales of these datasets have shifted, providing measurments of many variables, with many samples, albeit from potentially heterogeneous population. For example ..., .... Whilst recently the 10x Genomics million cell experiment (https://community.10xgenomics.com/t5/10x-Blog/Our-1-3-million-single-cell-dataset-is-ready-to-download/ba-p/276) sequenced over 1.3 million cells taken from the cortex, hippocampus and ventricular zone of embryonic mice. Unlike ... 

Datasets that are large, either in terms of the number of variables, or the number of observations, are inherently difficult for humans to develop an intuition for. A useuful array of techniques have therefore been developed to deal with such large datasets, aimed at dimensionality reduction. This aims to reduce the dimensionality by summarising the tens of thousands of genes into two or three informative variables, which are more intuitive space for a human to form a hypothesis. 

Whilst dimensionality reduction allows humans to inspect the datasets, particularly when the data can be represented in two or three dimensions, it should be noted that humans are exceptionally good at identifying patterns in two or three dimensional data, even when no real structure exists. For example, seeing an image of the UK in a cloud (Figure X). In these cases, it is often useful to employ other statistical machine learning approaches to search for patterns over the reduced dimensional space. In this sense, dimensionality reduction forms an important component of a moddern statistical analyses that will typically combined a variety of machine learning techniques, such as classification, regression and clustering. 

In this chapter we will explore two forms of dimensionality reductions, principle component analysis (PCA) and t-distributed stochastic neighbor embedding (tSNE), highlighting the advantages and potential pitfalls of both methods for dimensionality reduction. As an illustrative example, we will use these approaches to analyse some single cell RNA-sequencing data of early human development, ...

In section X we discuss how dimensionality reduction tecniques can be combiend with other ML algorithms to perform pseudotime analyses in collections of scRNA-seq data, so-called pseudotime approaches. Finally, we will explore more recent developments in pseudotime analysis by running DE 

## Linear Dimensionality Reduction

The most widely used form of dimensionality reduction is based on principle component analysis (PCA), which was introduced by Pearson in the early 1900's (cite), and indepedently discovered by Hotelling (1930). PCA has a long history of use in the biological sciences ... PCA remains one of the most useful techniques for analysing any large scale datasets.

PCA have a long history in ecological data, but have become increasingly used in the analysis of genetic (Cavalli-Sforza and Edwards, 1964) and genomeic data (Patterson et al., 2006).

linear combination of the original variables

 (Vohradsky et al. 1997, Craig et al. 1997)
 
 
recommended when attempting PCA on measurements that are not on a comparable scale (Everitt & Dunn 1992)

PCA is not a dimensionality reduction technique per se, but an alternative way of representing the data that more naturally captures the variance in the system. Specifically, it finds a new co-ordinate system, so that the x-axis (first principle component) is aligned along the direciont of greatest variance, with the y-axis aligned along the direction of second greatest variance, and so forth (Figure 1). Note that at this stage there is no inherent dimensionality reduction, and the new co-ordinate system retains the same number of dimensions as the original co-ordiante system.

So why is this useuful? We often find that as most of the variance in the system is captured relatively few principle components. That is, later principle components are not really all that informative, and can be discarded. Depending on our objective, we may take only the first few principle components, thereby providing a low dimensional visualisation of our dataset that can be interogated. For example allowing us to look for structure within the dataset, similarity in different cells and so forth. 

### Horeshoe effect

Principle component analysis is a linear (dimensionality reduction) technique, and is not always appropriate when dealing with nonlinearities in datasets. To illustrate this, let's consider an simulated expression set containig 8 genes, with N timepoints. We can again represnt this in terms of a matrix (below) or in terms of plots. Since the dimensionality is low, we can naturally see that there is a simple way of representing the dataset in terms of ordering: we simply use an increasing integer from gene 1 to gene 2.

However, within this simple system, we note clear nonlinearites. For example, g1 is initially positively correlated with g2, then negatively correlated, and then uncorrelated. PCA aims to place very different datapoints far apart. g1 and g5 are already uncorrelated, so PCA will attempt to place these points far from one anohter, howver, there is no more information in g1 and g6, so PCA will attempt to place g1 at a simlar distance to g5. The result is the well known horsehoe effect (cite). Lets now plot the PCA for our dataset:

## Nonlinear Dimensionality Reduction

Due to limitations in 

Introduction ot 

#### Perplexit parameter

#### Stochasticity

#### Example on single cell datasets

## PCA of biological data

Now that we have a feel for PCA and understand how the basic commands work we can take a look at some real data. To illustrate we will use preprocessed data taken from Yan et al. (2015) and Guo et al. (2015). The data from Yan et al. represents single cell RNA-seq data of from human embryos from the zygote stage (a single cell produced following fertilisation of an egg) through to the blastocyst stage (consisting of around 64 cells), as well as human embryonic stem cells (hESC), cells extracted from an early blsatocyst stage embryo and maintained in vitro. The dataset of Guo et al. (2015) contains scRNA-seq data from human primordial germ cells (hPGCs), precursors of sperm or eggs that are specified early in the developing human embryo soon after implantation (around week 2-3 in humans), and somatic cells. Together, these datasets therefore provide useful insights into the ...

The preprocessed data contains log_2 normalised counts in around 400 cells for 300X marker genes, and can be found in the file XXX.csv. Note that the first line of data in the file is an indicator denoting cell type (-1 = ESC, 0 = pre-implantation, 1 = PGC, 2 = somatic cell). The second row indicates the sex of the cell (0 = unknown/unlabelled, 1 = female, 2 = male), with the third row indicating capture time (-1 = ESC, 0 - 7 denotes various developmental stages from zygote to blastocyst, 8 - X indicates various times of embryo development from week 4 through to week 19).

Excercise 1.1. First load in the expression data into R and plot some example expression patterns. First try plotting the data using ...


D <- read.csv(file = "/Users/christopher_penfold/Desktop/MLCourse/intro-machine-learning/data/PGC_transcriptomics/PGC_transcriptomics.csv", header = TRUE, sep = ",", row.names=1)

Excercise 1.2. Use perform prcomp to perfom PCA on the data. Hint: the ...

B <- prcomp(t(D[4:nrow(D),1:ncol(D)]), center = TRUE, scale. = FALSE)

Excercise 1.3. Try plotting the loadings for the genes. Can we identify any genes of interest that may be particularlry important for PGCs?

Excercise 1.4. Does the data sepearate well? Perform k-means cluster analysis on the data. Since we have the actual labels, we can calculate how well our algorithm clusters the cells together.

Excercise 1.5. Perform a differential expression analysis between blastocyst cells and the PGCs.

### Nonlinear dimensionality reduction

Visualization of high-dimensional datasets. Load in. PCA. Cluster. DE. Not that is splits the pre-implantation into multiple values. 

y1 <- B$x[which(D[1,]==-1),1:2]
y2 <- B$x[which(D[1,]==0),1:2]
y3 <- B$x[which(D[1,]==1),1:2]
y4 <- B$x[which(D[1,]==2),1:2]

plot(y1,type="p",col="red",xlim=c(-45, 45),ylim=c(-45, 45))
points(y2,type="p",col="black")
points(y3,type="p",col="blue")
points(y4,type="p",col="green", legend=c("Line 1"))
legend(-40, 40, legend=c("ESC", "preimp", "PGC", "soma"), col=c("red", "black", "blue", "green"),pch="o", bty="n", cex=0.8)


plot(B$rotation[,1:2],type="n",xlim=c(-0, 0.07),ylim=c(0.02, 0.1))
genenames <- rownames(D)
genenames <- genenames[4:nrow(D)]
text(B$rotation[,1:2], genenames, , cex = .4)



plot(B$rotation[,1:2],type="n",xlim=c(-0, 0.07),ylim=c(-0.04, -0.1))
genenames <- rownames(D)
genenames <- genenames[4:nrow(D)]
text(B$rotation[,1:2], genenames, , cex = .4)


tsne_model_1 = Rtsne(as.matrix(t(D)), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)

y1 <- tsne_model_1$Y[which(D[1,]==-1),1:2]
y2 <- tsne_model_1$Y[which(D[1,]==0),1:2]
y3 <- tsne_model_1$Y[which(D[1,]==1),1:2]
y4 <- tsne_model_1$Y[which(D[1,]==2),1:2]

plot(y1,type="p",col="red",xlim=c(-45, 45),ylim=c(-45, 45))
points(y2,type="p",col="black")
points(y3,type="p",col="blue")
points(y4,type="p",col="green")
legend(-40, 40, legend=c("ESC", "preimp", "PGC", "soma"), col=c("red", "black", "blue", "green"),pch="o", bty="n", cex=0.8)





read in PGC/early development data
data_tsne=read.delim("data_1.txt", header = T, stringsAsFactors = F, sep = "\t")
tsne_model_1 = lib(as.matrix(data_tsne), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)

d_tsne_1 = as.data.frame(tsne_model_1$Y)
ggplot(d_tsne_1, aes(x=V1, y=V2)) +  
geom_point(size=0.25) +
guides(colour=guide_legend(override.aes=list(size=6))) +
xlab("") + ylab("") +
ggtitle("t-SNE") +
theme_light(base_size=20) +  
theme(axis.text.x=element_blank(),axis.text.y=element_blank()) +
scale_colour_brewer(palette = "Set2")
  
  
sne_model_1 = Rtsne(as.matrix(log2(data_tsne[2:1000,2:400])), check_duplicates=FALSE, pca=TRUE, perplexity=30, theta=0.5, dims=2)  
  
Demonstrate variety of perpexity

### Other dimensionality reduction techniques

A large number of alternative dimensionality reduction techniques exist with corresponding codebases in R. These include other nonliner dimensionality reduction techniques such as Gaussian Process Latent Variable Models (GPLVMs; Lawrence 2004; https://github.com/SheffieldML/vargplvm.git) and Isomap. In general ....

## Dimensionality reduction and pseudotime algorithsm

tSNE is a particularly useful approach when dealing with large (potentially nonliner) datasets with unknown strucutre. It is frequently used as a starting point for pseudotime algorithms. Reference Martin Hembergs course for intro to pseudotime.

However, as we have seen, there remains some intepretability issues in using these approaches for pseudotime ordering. In partciular the flexibility of the nonlinear warping means that the intepretation of the odering can be difficult. Furthermore, since most algorithms will artifically truncate the ...

An alternative apporach that is particularly useful when dealing with time series annotated datasets attempts to tie the latent dimension to a meanigful parameter. Specifically, these approaches attempt to reduce teh dimension down to a single dimension, with that dimension anchored into to time points (Figure CX). Consequently, this approach

An example of this type of pseudotime ordering can be found in the DeLorean package of (Reid and Wernisch, 2016). Of course, ...

Pick out a few genes, say 30 or so and run DeLorean for 10000 iterations. 


library("DeLorean")






TEXT dump

We first need to build an intuition for what is going on. A good illustrative example can be found in Ringnér (2008) where they have a simple two dimensional system, in which we have measured the expression of GATA3 and XBP1. MORE DETAILS. We can follow this experiment. First let's read in the expresion levels of from our dataset and plot the origianl data. The data is located as file XXXX and can be loaded in using the command:

> y1<- load('')
plot the data

In Figure 1(a), below, we can directly visualise the behaviour of the system, by plotting the expression of these two genes, with the the x-axis representing the expression of GATA3, and the y-axis represents the expression of XBP1.

Within R, principle component analysis can be easily applied to datasets using the inbuilt function prcomp. We can generate the principle components using the following line of code:

> Y <- prcomp(X, scale = FALSE)

explain options. Also explain the output variables.

We can now plot the data, again, this time using the principle component axes. Note that all we have done, in this case, is find a new set of coordinates in which to represent the data, the x-axis now captures the maximal amount of variation, whilst the y-axis captures the next most amount of variation. Since we started witha 2 dimensional system, we have two principle components. For a N-dimensional system, we would end up with N principle components that indicate decreasing amount of variation.

For our system we can check the amounnt of variation within the system using the comman

> ...

We can see that, in our case, principle component 1 captures the majority of the varition in the dataset, with principle component 2 capturing relatively litte. In this sense, principle component 2 is not *really* informative, and for the sake of intepretability we can choose to discard it. By doing so, we have reduced the dimension to a single one dimensional system.

At this stage it is important to understand that although we have reduced the dimension of the system, the intepretation of the new axis requires some considerations. Wheras before we had a simpler intepretation, where each axis represented the expression level of a single gene, the new principle component axis is actually represents some linear combination of various genes. It can often be informative to look at what these genes are. 